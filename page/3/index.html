<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="blog.csdn.net/quqi99">
<meta property="og:type" content="website">
<meta property="og:title" content="技术并艺术着">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="技术并艺术着">
<meta property="og:description" content="blog.csdn.net/quqi99">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="技术并艺术着">
<meta name="twitter:description" content="blog.csdn.net/quqi99">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/3/"/>





  <title>技术并艺术着</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">技术并艺术着</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">张华的技术博客 - blog.csdn.net/quqi99</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/12/RabbitMQ-Deep-Dive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/12/RabbitMQ-Deep-Dive/" itemprop="url">RabbitMQ Deep Dive</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-12-12T12:01:46+08:00">
                2020-12-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华  发表于：2015-06-03<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )<br>AMQP概念<br>通过消息机制，可以实现数据传输，非阻塞型操作，推送通知，发布/订阅，异步处理，work队列。<br>AMQP当中有四个概念非常重要：虚拟主机（virtual host），交换机（exchange），队列（queue）和绑定（binding）。</p>
<p>virutal host相当于namespace，用于不同tenant之间的exchange, queue, binding的隔离。<br> Queue队列, 每个消息都会被投入到一个或多个队列。消息就一直在里面，直到有客户端（也就是消费者，Consumer）连接到这个队列并且将其取走为止。队列是由消费者（Consumer）通过程序建立的，不是通过配置文件或者命令行工具。<br>Binding绑定, 它的作用就是把exchange和queue按照路由规则绑定起来。<br>Routing_Key路由关键字：exchange根据这个关键字进行消息投递。<br>Channele消息通道：在客户端的每个连接里，可建立多个channel，每个channel代表一个会话任务。<br>Exchange交换机，对消息进行路由，当收到Publisher传递给它的消息后，Excahnge会根据路由键routing_key决定将消息加入到哪些消息队列中。<br>消息的类型：</p>
<p>Direct Exchange – 处理路由键。需要将一个队列绑定到交换机上，要求该消息与一个特定的路由键完全匹配。一对一交换类型。<br>Topic Exchange – 将路由键和某模式进行匹配。此时队列需要绑定要一个模式上。符号“#”匹配一个或多个词，符号“*”匹配不多不少一个词。一对多主题多播交换类型。<br>Fanout Exchange – 不处理路由键。你只需要简单的将队列绑定到交换机上。一个发送到交换机的消息都会被转发到与该交换机绑定的所有队列上。一对多广播交换类型。</p>
<p>RabbitMQ简介与特点<br>RabbitMQ是一个开源的AMQP协议的实现，它具有如下特点：可靠性（Reliability）, RabbitMQ使用一些机制来保证程序的可靠性，如持久化、传输确认机制、发布确认、高可用性。灵活的路由机制（Flexible Routing）, 在消息进入队列之前，通过Exchange来路由消息的。对于典型的路由功能，RabbitMQ已经提供了一些内置的Exchange来实现。针对更复杂的路由功能，可以将多个Exchange绑定在一起，也通过插件机制实现自己的Exchange。消息集群（Clustering）多个RabbitMQ服务器可以组成一个集群，形成单个逻辑Broker。Federation, For servers that need to be more loosely and unreliably connected than clustering allows, RabbitMQ offers a federation model.队列高可用（Highly Available Queues）, 队列可以在集群中的机器上进行镜像，以确保在硬件问题下还保证消息安全。多种协议的支持（Multi-protocol）, RabbitMQ支持多种消息队列协议。</p>
<p>一个rabbitmq python例子</p>
<p>#coding:utf-8<br>import sys<br>from amqplib import client_0_8 as amqp<br>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    if (len(sys.argv) &lt;= 1):<br>        ispublisher = ‘0’<br>        print “Then pls run ‘rabbittest 1’ to sent message.”<br>    else:<br>        ispublisher = sys.argv[1]<br>    conn = amqp.Connection(host=”localhost:5672 “, userid=”guest”, password=”password”, virtual_host=”/“, insist=False)</p>
<pre><code># 每个channel都被分配了一个整数标识
chan = conn.channel()
# 创建一个队列，它是durable的（重启后会重新建立）a
# 消费者断开时不会自动删除（auto_delte=False)
chan.queue_declare(queue=&quot;queue1&quot;, durable=True, exclusive=False, auto_delete=False)
# 创建交换机，参数意思和上面的队列是一样的，还有一个type类型：fanout, direct, topic
chan.exchange_declare(exchange=&quot;switch1&quot;, type=&quot;direct&quot;,
                      durable=True, auto_delete=False,)
# 绑定交换机和队列
chan.queue_bind(queue=&quot;queue1&quot;, exchange=&quot;switch1&quot;, routing_key=&quot;key1&quot;)
if (ispublisher == &apos;1&apos;):
    # 生产者
    msg = amqp.Message(&quot;Test message!&quot;)
    msg.properties[&quot;delivery_mode&quot;] = 2
    chan.basic_publish(msg, exchange=&quot;switch1&quot;, routing_key=&quot;key1&quot;)
else:
    # 主动从队列拉消息
    msg = chan.basic_get(&quot;queue1&quot;)
    print msg.body
    chan.basic_ack(msg.delivery_tag)
    # 消息来了通知回调
    # 如果no_ack=True可以使用chan.basic_ack()人工确认，使用delivery_tag参数
    def recv_callback(msg):
        print &apos;Received: &apos; + msg.body
    chan.basic_consume(queue=&apos;queue1&apos;, no_ack=False,
                       callback=recv_callback, consumer_tag=&quot;testtag&quot;)
    # chan.basic_cancel(&quot;testtag&quot;) # 取消回调函数
    while True:
        chan.wait()  # 等待在队列上，直到下一个消息到达队列。
chan.close()
conn.close()
</code></pre><p>RabbitMQ CLI<br>安装，sudo apt-get install rabbitmq-server<br>重启，sudo service rabbitmq-server restart<br>sudo rabbitmqctl list_vhostssudo rabbitmqctl add_vhost demo<br>sudo rabbitmqctl list_users<br>sudo rabbitmqctl add_user test password<br>sudo rabbitmqctl change_password test password<br>sudo rabbitmqctl clear_password test<br>sudo rabbitmqctl list_user_permissions test<br>sudo rabbitmqctl set_permissions -p demo test “.<em>“ “.</em>“ “.*”<br>sudo rabbitmqctl clear_permissions -p demo test<br>sudo rabbitmqctl list_queues -p demo name durable auto_delete slave_pids synchronised_slave_pids<br>sudo rabbitmqadmin delete queue name=’queuename’<br>sudo rabbitmqctl list_exchanges -p demosudo rabbitmqctl list_bindings -p demo<br>sudo rabbitmqctl list_consumers -p demosudo rabbitmqctl statussudo rabbitmqctl report</p>
<h1 id="fileds-can-be-name-durable-auto-delete-arguments-policy-pid-owner-pid-exclusive-exclusive-consumer-pid-exclusive-consumer-tag-messages-ready-messages-unacknowledged-messages-messages-ready-ram-messages-unacknowledged-ram-messages-ram-messages-persistent-message-bytes-message-bytes-ready-message-bytes-unacknowledged-message-bytes-ram-message-bytes-persistent-head-message-timestamp-disk-reads-disk-writes-consumers-consumer-utilisation-memory-slave-pids-synchronised-slave-pids-state"><a href="#fileds-can-be-name-durable-auto-delete-arguments-policy-pid-owner-pid-exclusive-exclusive-consumer-pid-exclusive-consumer-tag-messages-ready-messages-unacknowledged-messages-messages-ready-ram-messages-unacknowledged-ram-messages-ram-messages-persistent-message-bytes-message-bytes-ready-message-bytes-unacknowledged-message-bytes-ram-message-bytes-persistent-head-message-timestamp-disk-reads-disk-writes-consumers-consumer-utilisation-memory-slave-pids-synchronised-slave-pids-state" class="headerlink" title="fileds can be:  [name, durable, auto_delete, arguments, policy, pid, owner_pid, exclusive, exclusive_consumer_pid, exclusive_consumer_tag, messages_ready, messages_unacknowledged, messages, messages_ready_ram, messages_unacknowledged_ram, messages_ram, messages_persistent, message_bytes, message_bytes_ready, message_bytes_unacknowledged, message_bytes_ram, message_bytes_persistent, head_message_timestamp, disk_reads, disk_writes, consumers, consumer_utilisation, memory, slave_pids, synchronised_slave_pids, state]"></a>fileds can be:  [name, durable, auto_delete, arguments, policy, pid, owner_pid, exclusive, exclusive_consumer_pid, exclusive_consumer_tag, messages_ready, messages_unacknowledged, messages, messages_ready_ram, messages_unacknowledged_ram, messages_ram, messages_persistent, message_bytes, message_bytes_ready, message_bytes_unacknowledged, message_bytes_ram, message_bytes_persistent, head_message_timestamp, disk_reads, disk_writes, consumers, consumer_utilisation, memory, slave_pids, synchronised_slave_pids, state]</h1><p>sudo rabbitmqctl list_queues name slave_pids synchronised_slave_pids durable -p openstack<br>RabbitMQ GUI</p>
<p>Enable it, sudo rabbitmq-plugins enable rabbitmq_management<br><a href="http://localhost:15672" target="_blank" rel="external">http://localhost:15672</a>  (guest/guest)<br>RabbitMQ配置文件<br><a href="http://www.rabbitmq.com/configure.html#configuration-file" target="_blank" rel="external">http://www.rabbitmq.com/configure.html#configuration-file</a><br>sudo find / -name rabbitmq.config*<br>sudo mv /usr/share/doc/rabbitmq-server/rabbitmq.config.example.gz /etc/rabbitmq/cd /etc/rabbitmq/ &amp;&amp; sudo gunzip rabbitmq.config.example.gz<br>sudo mv rabbitmq.config.example rabbitmq.config<br>RabbitMQ调优</p>
<p>1, 流控(Flow Control)机制，默认RabbitMQ在使用机器的40%以上的内存时流控机制会起作用block掉所有连接。故确保使用64位操作系统与64位Erlang VM。当RabbitMQ是集群情况下，当其中有一台机器硬盘不足的时候，所有节点的producer链接也会被阻止。</p>
<p>rabbitmqctl  set_vm_memory_high_watermark 0.4<br>rabbitmqctl set_vm_memory_high_watermark_paging_ratio 0.75<br>rabbitmqctl status<br><a href="http://www.rabbitmq.com/memory.html" target="_blank" rel="external">http://www.rabbitmq.com/memory.html</a><br>Max open files，/etc/default/rabbitmq-server<br>ulimit -n 65535<br>cat /proc/$RABBITMQ_BEAM_PROCESS_PID/limits<br>2, Erlang的Hipe优化, 可以设置hipe_compiles设置。可以看到有20-50%的性能优化。而你只需要付出1分钟左右的延迟启动。 HiPE需要你检查是否编译进入你的Erlang安装环境。Ubuntu，需要安装erlang-base-hipe.默认有些平台不支持。如果Erlang VM segfaults,请关闭这个选项。</p>
<p>[{rabbit, [{hipe_compile, true}]}].<br>RabbitMQ集群</p>
<p>跨三个节点部署 RabbitMQ 集群和镜像消息队列。可以使用 HAProxy 提供负载均衡，或者将 RabbitMQ host list 配置给 OpenStack 组件（使用 rabbit_hosts 和 rabbit_ha_queues 配置项）。</p>
<p>先看第一种方式（采用HAproxy）：</p>
<h1 id="每个节点上执行下列命令配置RabbitMQ集群"><a href="#每个节点上执行下列命令配置RabbitMQ集群" class="headerlink" title="每个节点上执行下列命令配置RabbitMQ集群"></a>每个节点上执行下列命令配置RabbitMQ集群</h1><h1 id="根据需要设置当前节点的工作模式-ram-disk-，ROOT-NODE-HOSTNAME为集群根节点的主机名，注意在此必须使用主机名"><a href="#根据需要设置当前节点的工作模式-ram-disk-，ROOT-NODE-HOSTNAME为集群根节点的主机名，注意在此必须使用主机名" class="headerlink" title="根据需要设置当前节点的工作模式(ram/disk)，ROOT_NODE_HOSTNAME为集群根节点的主机名，注意在此必须使用主机名"></a>根据需要设置当前节点的工作模式(ram/disk)，ROOT_NODE_HOSTNAME为集群根节点的主机名，注意在此必须使用主机名</h1><p>apt-get install rabbitmq-server<br>rabbitmq-server -detached                   #detached为后台运行别占据终端<br>echo ‘MYRABBITMQCLUSTERABC’ &gt; /var/lib/rabbitmq/.erlang.cookie<br>chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie<br>chmod 400 /var/lib/rabbitmq/.erlang.cookie<br>/usr/lib/rabbitmq/bin/rabbitmq-plugins enable rabbitmq_management<br>/usr/sbin/rabbitmqctl stop_app<br>/usr/sbin/rabbitmqctl reset<br>/usr/sbin/rabbitmqctl join_cluster –ram rabbit@${ROOT_NODE_HOSTNAME}<br>/usr/sbin/rabbitmqctl start_app<br>service rabbitmq-server restart</p>
<h1 id="在主节点上添加用户"><a href="#在主节点上添加用户" class="headerlink" title="在主节点上添加用户"></a>在主节点上添加用户</h1><p>/usr/sbin/rabbitmqctl add_user web_admin password<br>/usr/sbin/rabbitmqctl add_user mgmt_admin password<br>/usr/sbin/rabbitmqctl set_user_tags username monitoring<br>/usr/sbin/rabbitmqctl set_user_tags mgmt_admin administrator<br>/usr/sbin/rabbitmqctl rabbitmqctl list_users<br>/usr/sbin/rabbitmqctl set_permissions -p / mgmt_admin “.<em>“ “.</em>“ “.*”</p>
<h1 id="设置HAProxy-需要设置成镜像队列，可以访问http-192-168-64-87-8888，用户名web-admin-password进行访问"><a href="#设置HAProxy-需要设置成镜像队列，可以访问http-192-168-64-87-8888，用户名web-admin-password进行访问" class="headerlink" title="设置HAProxy, 需要设置成镜像队列，可以访问http://192.168.64.87:8888，用户名web_admin/password进行访问"></a>设置HAProxy, 需要设置成镜像队列，可以访问<a href="http://192.168.64.87:8888，用户名web_admin/password进行访问" target="_blank" rel="external">http://192.168.64.87:8888，用户名web_admin/password进行访问</a></h1><p>/usr/sbin/rabbitmqctl set_policy ha-all “^” ‘{“ha-mode”:”all”}’<br>修改文件：/etc/haproxy/haproxy.cfg<br>listen rabbitmq_cluster 0.0.0.0:5672<br>mode tcp<br>balance roundrobin<br>server   node1 192.168.1.1:5672 check inter 2000 rise 2 fall 3<br>/usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -D<br>第二种使用 rabbit_hosts 和 rabbit_ha_queues 配置项：</p>
<p>rabbit_hosts = rabbit1:5672,rabbit2:5672<br>rabbit_host = rabbit1<br>rabbit_ha_queues = true<br>如果配置了rabbit_hosts，那么nova将会按照顺序连接一个RabbitMQ服务，如果正在使用的MQ服务断开则依次尝试连接下一个，由于所有MQ的消息都是同步的，所以消息不会丢失。<br>如果配置了rabbit_host，那么需要在集群前架设haproxy，保证集群VIP服务正常。</p>
<p>confirm that actual queue is connected and can consume that queue.  </p>
<p>sudo rabbitmq-plugins enable rabbitmq_management<br>wget <a href="http://127.0.0.1:15672/cli/rabbitmqadmin" target="_blank" rel="external">http://127.0.0.1:15672/cli/rabbitmqadmin</a> &amp;&amp; chmod 777 rabbitmqadmi<br>sudo rabbitmqctl add_user test password<br>sudo rabbitmqctl set_user_tags test administrator<br>sudo rabbitmqctl set_permissions -p openstack test “.<em>“ “.</em>“ “.*”<br><a href="http://10.5.0.6:15672/#/queues/openstack/compute.juju-a09725-xenial-mitaka-7" target="_blank" rel="external">http://10.5.0.6:15672/#/queues/openstack/compute.juju-a09725-xenial-mitaka-7</a><br>./rabbitmqadmin publish -V openstack -u test -p password exchange=nova routing_key=compute.juju-a09725-xenial-mitaka-7 payload=”test”<br>tail -f /var/log/nova/nova-compute.log</p>
<p>具体地见：<a href="http://m.blog.csdn.net/blog/gtt116/21083533" target="_blank" rel="external">http://m.blog.csdn.net/blog/gtt116/21083533</a><br>Debug Hacks<br>$ tshark -r xxx.pcap |awk ‘{arr[$5]++}END{for (a in arr) print a, arr[a]}’ |sort -n -k 2 -r | head -n 3<br>10.55.74.103 62756<br>10.55.74.142 12976<br>10.55.74.139 12228<br>juju run -u rabbitmq-server/0 ‘sudo rabbitmqctl list_queues -p openstack|grep -wv 0’</p>
<p>watch -c “sudo rabbitmqctl list_queues -p openstack | grep -E ‘log|neutron|agent’”<br>Reset rabbitmq slave<br>1) On juju-3182a3-69-lxd-2, back mnesia, stop the service<br>$ sudo mv /var/lib/rabbitmq/mnesia /var/lib/rabbitmq/mnesia-back<br>$ sudo service rabbitmq-server stop<br>2) Forget the cluster nodes from the rabbit master node<br>$ sudo rabbitmqctl stop_app<br>$ sudo rabbitmqctl forget_cluster_node rabbit@juju-3182a3-69-lxd-2<br>$ sudo rabbitmqctl start_app<br>如何恢复systemd管理的native mirror rabbitmq cluster</p>
<p>如何恢复systemd管理的native mirror rabbitmq cluster<br>1, 确保在3个节点上，rabbitmq-server先由systemd启动(随后会由pacemaker接管)，这样能可能运行rabbitmqctl cluster_status命令．假设此时3个节点各自为政．<br>juju run –application=rabbitmq-server ‘sudo rabbitmqctl cluster_status’</p>
<p>2, juju status看有没有error状态，例如现在看到rabbitmq-server/1因为下列日志为error状态, rabbitmq-server/1上运行：<br><a href="https://www.jianshu.com/p/498c63e4ace1" target="_blank" rel="external">https://www.jianshu.com/p/498c63e4ace1</a><br><a href="https://ywnz.com/linuxyffq/3899.html" target="_blank" rel="external">https://ywnz.com/linuxyffq/3899.html</a><br>2020-02-21 09:24:09 DEBUG config-changed subprocess.CalledProcessError: Command ‘[‘timeout’, ‘180’, ‘/usr/sbin/rabbitmqctl’, ‘wait’, ‘/var/lib/rabbitmq/mnesia/rabbit@juju-cbd760-octavia-10.pid’]’ returned non-zero exit status 70.<br>systemctl restart rabbitmq-server<br>rabbitmqctl status |grep pid   #write pid to /var/lib/rabbitmq/mnesia/rabbit@juju-cbd760-octavia-10.pid<br>rabbitmqctl wait /var/lib/rabbitmq/mnesia/rabbit@juju-cbd760-octavia-10.pid<br>juju resolved rabbitmq-server/1 –no-retry<br>[可选]确保juju status没有rabbitmq untis相关错误之后，触发hooks<br>juju run –application ha hooks/config-changed<br>juju run –application rabbitmq-server hooks/ha-relation-joined<br>juju show-status-log neutron-openvswitch/2</p>
<p>3, 没有ceph的情况下，代码显示必须有ha-vip-only=true<br>juju deploy -n 3 rabbitmq-server<br>juju deploy hacluster ha<br>juju add-relation rabbitmq-server ha<br>juju config rabbitmq-server vip=10.5.100.20<br>juju config rabbitmq-server vip_iface=ens3<br>juju config ha corosync_bindiface=ens3<br>juju config rabbitmq-server ha-vip-only=true<br>juju config ha cluster_count=3<br>juju config rabbitmq-server min-cluster-size=3</p>
<p>4, 找到leader，假如leader是rabbitmq-server/1，并在其上找到cluster_name<br>juju run –application rabbitmq-server “is-leader”   #assue rabbitmq-server/1 is the leader<br>root@juju-cbd760-octavia-10:~# rabbitmqctl cluster_status |grep cluster<br> {cluster_name,<a href="&#109;&#97;&#x69;&#108;&#x74;&#x6f;&#58;&#60;&#34;&#114;&#97;&#x62;&#98;&#105;&#x74;&#109;&#113;&#x2d;&#x73;&#x65;&#x72;&#x76;&#x65;&#x72;&#x40;&#x6a;&#117;&#106;&#x75;&#x2d;&#99;&#98;&#x64;&#x37;&#x36;&#x30;&#45;&#x6f;&#99;&#116;&#97;&#x76;&#x69;&#x61;&#x2d;&#49;&#48;&#x22;">&#60;&#34;&#114;&#97;&#x62;&#98;&#105;&#x74;&#109;&#113;&#x2d;&#x73;&#x65;&#x72;&#x76;&#x65;&#x72;&#x40;&#x6a;&#117;&#106;&#x75;&#x2d;&#99;&#98;&#x64;&#x37;&#x36;&#x30;&#45;&#x6f;&#99;&#116;&#97;&#x76;&#x69;&#x61;&#x2d;&#49;&#48;&#x22;</a>&gt;},</p>
<p>5, rabbitmq-server/1, 把RABBITMQ_NODENAME由localhost改成juju-cbd760-octavia-10<br>root@juju-cbd760-octavia-10:~# grep -r ‘RABBITMQ_NODENAME’ /etc/rabbitmq/rabbitmq-env.conf<br>RABBITMQ_NODENAME=rabbitmq-server@juju-cbd760-octavia-10<br>root@juju-cbd760-octavia-10:~# cat /var/lib/rabbitmq/.erlang.cookie<br>CZIFVOYCELFFGUFWJBZY<br>systemctl restart rabbitmq-server<br>rabbitmqctl cluster_status</p>
<p>但代码中有这一句会在ha-vip-only=false时将RABBITMQ_NODENAME设为localhost, 所以ha-vip-only应为ha-vip-only，<br>同时，这里也说对于rabbitmq的hacluster方式的ha已经废弃了<br><a href="https://github.com/openstack/charm-rabbitmq-server/blob/master/hooks/rabbitmq_context.py#L250" target="_blank" rel="external">https://github.com/openstack/charm-rabbitmq-server/blob/master/hooks/rabbitmq_context.py#L250</a></p>
<pre><code># TODO: this is legacy HA and should be removed since it is now
# deprecated.
if relation_ids(&apos;ha&apos;):
    if not config(&apos;ha-vip-only&apos;):
        # TODO: do we need to remove this setting if it already exists
        # and the above is false?
        context[&apos;settings&apos;][&apos;RABBITMQ_NODENAME&apos;] = \
            &apos;{}@localhost&apos;.format(service_name())
</code></pre><p>6, 登录到rabbitmq-server/0,　修改hostname并加入集群<br>root@juju-cbd760-octavia-9:~# grep -r ‘RABBITMQ_NODENAME’ /etc/rabbitmq/rabbitmq-env.conf<br>RABBITMQ_NODENAME=rabbitmq-server@juju-cbd760-octavia-9<br>systemctl restart rabbitmq-server<br>rabbitmqctl cluster_status<br>rabbitmqctl stop_app<br>rabbitmqctl reset<br>rabbitmqctl join_cluster rabbitmq-server@juju-cbd760-octavia-10<br>rabbitmqctl cluster_status</p>
<p>7, 登录到rabbitmq-server/2重复上一步，只是hostname不同, 这里为juju-cbd760-octavia-11</p>
<p>8, 此时．<br>root@juju-cbd760-octavia-10:~# rabbitmqctl cluster_status<br>Cluster status of node ‘rabbitmq-server@juju-cbd760-octavia-10’<br>[{nodes,[{disc,[‘rabbitmq-server@juju-cbd760-octavia-10’,<br>                ‘rabbitmq-server@juju-cbd760-octavia-11’,<br>                ‘rabbitmq-server@juju-cbd760-octavia-9’]}]},<br> {running_nodes,[‘rabbitmq-server@juju-cbd760-octavia-10’]},<br> {cluster_name,<a href="&#109;&#x61;&#x69;&#x6c;&#116;&#111;&#x3a;&#x3c;&#x22;&#114;&#97;&#98;&#98;&#x69;&#116;&#x6d;&#x71;&#45;&#115;&#101;&#114;&#x76;&#101;&#114;&#64;&#x6a;&#x75;&#x6a;&#x75;&#45;&#99;&#x62;&#100;&#x37;&#54;&#48;&#45;&#111;&#x63;&#116;&#97;&#x76;&#x69;&#x61;&#x2d;&#49;&#x30;&#x22;">&#x3c;&#x22;&#114;&#97;&#98;&#98;&#x69;&#116;&#x6d;&#x71;&#45;&#115;&#101;&#114;&#x76;&#101;&#114;&#64;&#x6a;&#x75;&#x6a;&#x75;&#45;&#99;&#x62;&#100;&#x37;&#54;&#48;&#45;&#111;&#x63;&#116;&#97;&#x76;&#x69;&#x61;&#x2d;&#49;&#x30;&#x22;</a>&gt;},<br> {partitions,[]},<br> {alarms,[{‘rabbitmq-server@juju-cbd760-octavia-10’,[]}]}]</p>
<p>9, 但上面修复只是systemd管理的native集群.<br>   如何被pacemaker管理呢？ 在rabbmitmq-server/1上(juju-cbd760-octavia-10)停掉systemd,启动corosync与packemaker，但前提是hacluster charm已经成功为corosync配置了res_rabbitmq_vip resources (当ha-vip-only=true时只有这一个）<br>   此时看到crm status没有res_rabbitmq_vip这个resource，</p>
<p>#juju run –application rabbitmq-server hooks/ha-relation-joined<br>juju run –application ha hooks/config-changed<br>juju run –unit ha/0 “sudo corosync-quorumtool -s”<br>juju run –unit ha/0 “sudo crm status”<br>juju run –unit ha/0 “sudo crm resource restart res_rabbitmq_vip”<br>juju run –unit ha/0 “sudo crm resource clean res_rabbitmq_vip”</p>
<p>#<a href="https://github.com/ClusterLabs/resource-agents/blob/master/heartbeat/rabbitmq-cluster" target="_blank" rel="external">https://github.com/ClusterLabs/resource-agents/blob/master/heartbeat/rabbitmq-cluster</a><br>ls /usr/lib/ocf/resource.d/rabbitmq/rabbitmq-server-ha<br>　　　若运行上面hook无法加入res_rabbitmq_vip的话，检查代码是应该设置ha-vip-only=true，然后使用下面方法添加:<br>juju remove-relation rabbitmq-server ha<br>juju add-relation rabbitmq-server ha<br>      然后可以看到：<br>root@juju-cbd760-octavia-11:~# crm status |grep vip<br> res_rabbitmq_vip       (ocf::heartbeat:IPaddr2):       Started juju-cbd760-octavia-10<br>     但是仍然看到这种错误：<br>ubuntu@zhhuabj-bastion:~$ juju status |grep waiting<br>rabbitmq-server             3.6.10   waiting      3  rabbitmq-server             jujucharms  358  ubuntu<br>rabbitmq-server/0                waiting   idle   9        10.5.0.35       5672/tcp                    Unit has peers, but RabbitMQ not clustered<br>rabbitmq-server/1*               waiting   idle   10       10.5.0.32       5672/tcp                    Unit has peers, but RabbitMQ not clustered<br>rabbitmq-server/2                waiting   idle   11       10.5.0.28       5672/tcp                    Unit has peers, but RabbitMQ not clustered</p>
<p>10, 继续调试, ha relation的数目不对，所以停在’ Unit has peers, but RabbitMQ not clustered’<br>ubuntu@zhhuabj-bastion:~$ juju run –unit rabbitmq-server/0 “relation-ids ha”<br>ha:41<br>ubuntu@zhhuabj-bastion:~$ juju run –unit rabbitmq-server/0 “relation-list -r ha:41”<br>ha/3   #because hacluster was named to ha so name is ha/3 now<br>ubuntu@zhhuabj-bastion:~$ juju run –unit rabbitmq-server/0 “relation-get -r ha:41 - ha/3”<br>clustered: “yes”<br>egress-subnets: 10.5.0.35/32<br>ingress-address: 10.5.0.35<br>private-address: 10.5.0.35</p>
<p>它在找rabbitmqctl cluster_status中为running_nodes的个数<br> 828 @cached<br> 829 def clustered():<br> 830     ‘’’ Determine whether local rabbitmq-server is clustered ‘’’<br> 831     # NOTE: A rabbitmq node can only join a cluster once.<br> 832     # Simply checking for more than one running node tells us<br> 833     # if this unit is in a cluster.<br> 834     if len(running_nodes()) &gt; 1:<br> 835         return True<br> 836     else:<br> 837         return False</p>
<p> 787 @cached<br> 788 def running_nodes():<br> 789     ‘’’ Determine the current set of running nodes in the RabbitMQ cluster ‘’’<br> 790     return nodes(get_running=True)</p>
<p> 770 @cached<br> 771 def nodes(get_running=False):<br> 772     ‘’’ Get list of nodes registered in the RabbitMQ cluster ‘’’<br> 773     out = rabbitmqctl_normalized_output(‘cluster_status’)<br> 774     cluster_status = {}<br> 775     for m in re.finditer(“{([^,]+),(?![{)[([^]]*)”, out):<br> 776         state = m.group(1)<br> 777         items = m.group(2).split(‘,’)<br> 778         items = [x.replace(“‘“, ‘’).strip() for x in items]<br> 779         cluster_status.update({state: items})<br> 780<br> 781     if get_running:<br> 782         return cluster_status.get(‘running_nodes’, [])<br> 783<br> 784     return cluster_status.get(‘disc’, []) + cluster_status.get(‘ram’, [])<br>Fix a rabbitmq issue<br><a href="https://zhhuabj.blog.csdn.net/article/details/105847301" target="_blank" rel="external">https://zhhuabj.blog.csdn.net/article/details/105847301</a></p>
<p>Reference<br>[1] <a href="https://gist.github.com/niedbalski/69a72103adad4f0f9609a0857c9810a4" target="_blank" rel="external">https://gist.github.com/niedbalski/69a72103adad4f0f9609a0857c9810a4</a></p>
<p>[2] <a href="https://pastebin.ubuntu.com/p/sJ94RmmS5x/" target="_blank" rel="external">https://pastebin.ubuntu.com/p/sJ94RmmS5x/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/12/Fix-a-rabbitmq-issue/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/12/Fix-a-rabbitmq-issue/" itemprop="url">Fix a rabbitmq issue</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-12-12T12:00:58+08:00">
                2020-12-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>问题</strong><br>rabbitmq cluster的3个节点挂了. 3个节点是lxd容器:<br>11-lxd-8<br>69-lxd-2<br>9-lxd-9<br><strong>初步sosreport分析</strong><br>11-lxd-8上看到system_limit错误<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">=ERROR REPORT==== 17-Apr-2020::13:19:44 ===</span><br><span class="line">Too many processes</span><br><span class="line">=ERROR REPORT==== 17-Apr-2020::13:19:44 ===</span><br><span class="line">Ranch listener &#123;acceptor,&#123;0,0,0,0,0,0,0,0&#125;,5672&#125; connection process start failure; rabbit_connection_sup:start_link/4 crashed with reason: error:system_limit</span><br></pre></td></tr></table></figure></p>
<p>但是已经设置了LimitNOFILE<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ grep -r &apos;Limit&apos; lib/systemd/system/rabbitmq-server.service</span><br><span class="line">LimitNOFILE=65536</span><br></pre></td></tr></table></figure></p>
<p>不过LimitNOFILE似乎是管file_descriptors的, 而不是erlang processes的.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;file_descriptors,     [&#123;total_limit,65436&#125;,      &#123;total_used,6415&#125;,      &#123;sockets_limit,58890&#125;,      &#123;sockets_used,6413&#125;]&#125;,</span><br><span class="line">16:28:22  &#123;processes,[&#123;limit,1048576&#125;,&#123;used,118304&#125;]&#125;,</span><br></pre></td></tr></table></figure></p>
<p>上面的输出显示似乎用了118304个erlang processes还未超出limit. 不过, 这可能是重启机器之后抓的sosreport. 并且limit已经很大, 单纯增加limit似乎也并不是正道. 所以继续寻找线索.</p>
<p>在69-lxd-2里也看到了’Error while waiting for Mnesia tables’这种错误, 似乎是Mnesia数据库未同步.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">var/log/rabbitmq/rabbit@juju-3182a3-69-lxd-2.log.1</span><br><span class="line">=INFO REPORT==== 17-Apr-2020::13:27:26 ===</span><br><span class="line">Waiting for Mnesia tables for 30000 ms, 9 retries left</span><br><span class="line">=WARNING REPORT==== 17-Apr-2020::13:27:56 ===</span><br><span class="line">Error while waiting for Mnesia tables: &#123;timeout_waiting_for_tables,</span><br><span class="line">[rabbit_user,rabbit_user_permission,</span><br><span class="line">rabbit_vhost,rabbit_durable_route,</span><br><span class="line">rabbit_durable_exchange,</span><br><span class="line">rabbit_runtime_parameters,</span><br><span class="line">rabbit_durable_queue]&#125;</span><br></pre></td></tr></table></figure></p>
<p>3个units上用’netstat -s’看到大量的reset tcp<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sos_commands/networking/netstat_-s |head -n2</span><br><span class="line">7154216 connection resets received</span><br><span class="line">34025359 resets sent</span><br></pre></td></tr></table></figure></p>
<h2 id="深入分析"><a href="#深入分析" class="headerlink" title="深入分析"></a>深入分析</h2><p>9-lxd-9上 通过下列命令看到各个openstack组件上都有到9-lxd-9的rabbitmq tcp连接, 一个组件就有约四五百个连接, 是不是多了一点? 其他两个units没这么多.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat sos_commands/networking/netstat_-W_-neopa| awk &apos;/:5672/ &#123; print $5 &#125;&apos; | awk -F: &apos;&#123; a[$1]++ &#125;; END &#123; for (i in a) print i, a[i] &#125;&apos; |sort -n -k 2 -r |more |head -n 80</span><br></pre></td></tr></table></figure></p>
<p>但一个组件就有四五百个连接, 每个组件都这么多, 不可能每个组件都有问题吧. 除了大量tcp连接可以造成容器cpu升高, host机器的cpu升高也可以造成容器cpu升高的吧. 所以接着检查host机器的cpu占用率:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ cat sos_commands/process/ps_auxwww |head -n 1</span><br><span class="line">USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</span><br><span class="line">$ cat sos_commands/process/ps_auxwww |head -n1 &amp;&amp; cat sos_commands/process/ps_auxwww |sort -n -k3 -r | head -n 3</span><br><span class="line">USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</span><br><span class="line">nova 1936435 130 0.0 347704 126140 ? Rs 11:40 0:01 /usr/bin/python2 /usr/bin/nova-api-metadata --config-file=/etc/nova/nova.conf --log-file=/var/log/nova/nova-api-metadata.log</span><br><span class="line">libvirt+ 1809025 100 0.0 51373088 236744 ? Sl Mar09 56456:32 qemu-system-x86_64 -enable-kvm -name guest=instance-000099e0,debug-threads=on -S -object secret,id=masterKey0,format=raw,file=/var/lib/libvirt/qemu/domain-26-instance-000099e0/master-key.aes -machine pc-i440fx-</span><br></pre></td></tr></table></figure></p>
<p>ok, 虚机占用cpu就是100%. 看样子是控制服务(rabbitmq)与计算服务(nova-compute)安装到同一台物理机了, 虚机占用的cpu或者内存大页什么的直接导致rabbitmq cluster挂掉.<br>注:<br>ps命令中第三列相加的cpu大于100%也不一定就意味着cpu一定高, 因为可以这些进程全跑在一个cpu核的, 它可能计算的是一个核的, 要想准确还得使用mpstat来查看</p>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>调整架构是不可能了, 可以暂时使用isolcpu隔离一些cpu单独通过taskset给rabbitmq使用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mpstat -P ALL 能看到所有cpu核的负载情况</span><br><span class="line">cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniq</span><br><span class="line">grub中添加isolcpus=1,3来隔离1和3的cpu待用, 然后运行update-grub后重启, 验证:</span><br><span class="line">1, cat /proc/cmdline |grep isolcpu</span><br><span class="line">2, ps -To &apos;pid,lwp,psr,cmd&apos; -p 310597</span><br><span class="line">3, ps -eo pid,cmd,psr |awk &apos;&#123;if($3=3) print $0&#125;&apos;</span><br><span class="line">taskset -p0x8 &lt;pid&gt;绑定&lt;pid&gt;到cpu3</span><br><span class="line">taskset -c 1 /etc/init.d/mysql start</span><br><span class="line">systemd manages the affinity for you. See &quot;man systemd.exec&quot; and CPUAffinity= option.</span><br></pre></td></tr></table></figure></p>
<h2 id="20201212更新-queue-master-locator-min-master"><a href="#20201212更新-queue-master-locator-min-master" class="headerlink" title="20201212更新 - queue_master_locator=min-master"></a>20201212更新 - queue_master_locator=min-master</h2><p>我们可能需要配置queue_master_locator=min-master - <a href="https://bugs.launchpad.net/charm-rabbitmq-server/+bug/1890759" target="_blank" rel="external">https://bugs.launchpad.net/charm-rabbitmq-server/+bug/1890759</a>, 进而可能造成这两个bug (控制平面居然影响了数据面） - <a href="https://bugs.launchpad.net/neutron/+bug/1871850" target="_blank" rel="external">https://bugs.launchpad.net/neutron/+bug/1871850</a><br><a href="https://bugs.launchpad.net/neutron/+bug/1869808" target="_blank" rel="external">https://bugs.launchpad.net/neutron/+bug/1869808</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># Queue masters are mostly on 11-lxd-8</span><br><span class="line">&gt; cat rabbitmqctl_report | awk -F&apos;\t&apos; &apos;/^Queues on openstack/ &#123;a=1&#125;; a &amp;&amp; /rabbit/ &#123;split($1,s,&quot;.&quot;); b[s[1]]++&#125;; NF==0 &#123;a=0&#125;; END &#123;for(i in b) &#123;print i, b[i]&#125;&#125;&apos;</span><br><span class="line">&lt;rabbit@juju-0aa49a-9-lxd-9 15</span><br><span class="line">&lt;rabbit@juju-0aa49a-11-lxd-8 15426</span><br><span class="line">&lt;rabbit@juju-0aa49a-10-lxd-8 309</span><br><span class="line"></span><br><span class="line"># Most connections are made to 11-lxd-8</span><br><span class="line">&gt; cat rabbitmqctl_report| awk -F&apos;\t&apos; &apos;/^Connections/ &#123;a=1&#125;; a &amp;&amp; /rabbit/ &#123;split($1,s,&quot;.&quot;); b[s[1]]++&#125;; NF==0 &#123;a=0&#125;; END &#123;for(i in b) &#123;print i, b[i]&#125;&#125;&apos;</span><br><span class="line">&lt;rabbit@juju-0aa49a-9-lxd-9 8763</span><br><span class="line">&lt;rabbit@juju-0aa49a-11-lxd-8 14843</span><br><span class="line">&lt;rabbit@juju-0aa49a-10-lxd-8 6413</span><br><span class="line"></span><br><span class="line"># 11-lxd-8 uses the most memory</span><br><span class="line">&gt; cat rabbitmqctl_report| awk &apos;/^Status/ &#123;a=1; print&#125;; a &amp;&amp; /total,/; !NF &#123;a=0&#125;&apos;</span><br><span class="line">Status of node &apos;rabbit@juju-0aa49a-10-lxd-8&apos;</span><br><span class="line">     [&#123;total,3536412448&#125;,</span><br><span class="line">Status of node &apos;rabbit@juju-0aa49a-11-lxd-8&apos;</span><br><span class="line">     [&#123;total,7030459176&#125;,</span><br><span class="line">Status of node &apos;rabbit@juju-0aa49a-9-lxd-9&apos;</span><br><span class="line">     [&#123;total,4256377400&#125;,</span><br></pre></td></tr></table></figure></p>
<p>crontab检查rabbitmq memory使用率:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"># process_memory_checker.sh</span><br><span class="line">MAILTO=root</span><br><span class="line">process=&quot;bin/beam.smp&quot;</span><br><span class="line">mem_percentage=`ps -o %mem,command ax | grep &quot;$&#123;process&#125;&quot; | grep -v grep | awk &apos;&#123;print $1&#125;&apos;`</span><br><span class="line">threshold_percentage=25</span><br><span class="line">if (( $(echo &quot;$&#123;mem_percentage&#125; &gt; $&#123;threshold_percentage&#125;&quot;|bc -l) ));</span><br><span class="line">then</span><br><span class="line">	echo &quot;Process $&#123;process&#125; on `hostname -f` is using $&#123;mem_percentage&#125;% of total memory (threshold is $&#123;threshold_percentage&#125;%).&quot; | \</span><br><span class="line">	mail -s &quot;Memory usage warning for process $&#123;process&#125; on $(hostname -s)&quot; $&#123;MAILTO&#125; 2&gt; /dev/null;</span><br><span class="line">	exit 1;</span><br><span class="line">fi</span><br><span class="line">echo &quot;No memory issues found for process $&#123;process&#125;&quot;</span><br><span class="line">exit 0</span><br></pre></td></tr></table></figure></p>
<p>另外，客户发现memory用到超过vm_memory_high_watermark_paging_ratio (<a href="https://www.rabbitmq.com/memory.html#paging" target="_blank" rel="external">https://www.rabbitmq.com/memory.html#paging</a> )时似乎memory也没有被page到硬盘造成memory一直在增大，这个网页（<a href="https://stackoverflow.com/questions/21666537/rabbitmq-memory-control-queue-is-full-and-is-not-paging-connection-hangs）说对“durable=true”queue才生效。" target="_blank" rel="external">https://stackoverflow.com/questions/21666537/rabbitmq-memory-control-queue-is-full-and-is-not-paging-connection-hangs）说对“durable=true”queue才生效。</a><br>“rabbitmqctl list_queues name  durable -p openstack”看到的durable都是false, 难道是在设置各组件的olso设置为(charm似乎不支持）amqp_durable_queues=True吗?<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[oslo_messaging_rabbit]</span><br><span class="line">amqp_durable_queues = True</span><br></pre></td></tr></table></figure></p>
<p>但这个patch说上面配置被废弃了 - <a href="https://bugs.launchpad.net/oslo.messaging/+bug/1433956" target="_blank" rel="external">https://bugs.launchpad.net/oslo.messaging/+bug/1433956</a>, 但master code中有这个配置，还没细查。另外，这个网页（<a href="https://elkano.org/blog/high-availability-rabbitmq-cluster-openstack/）说用这个配置：" target="_blank" rel="external">https://elkano.org/blog/high-availability-rabbitmq-cluster-openstack/）说用这个配置：</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[oslo_messaging_rabbit]</span><br><span class="line">rabbit_hosts=node1:5672,node2:5672,node3:5672</span><br><span class="line">rabbit_retry_interval=1</span><br><span class="line">rabbit_retry_backoff=2</span><br><span class="line">rabbit_max_retries=0</span><br><span class="line">rabbit_ha_queues=true</span><br><span class="line">rabbit_userid = openstack</span><br><span class="line">rabbit_password = openstack_pass</span><br><span class="line">amqp_auto_delete = true</span><br><span class="line">amqp_durable_queues=True</span><br></pre></td></tr></table></figure></p>
<p>charm ./charmhelpers/contrib/openstack/templates/section-rabbitmq-oslo 中有下面代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> 8 &#123;% if rabbitmq_ha_queues -%&#125;</span><br><span class="line"> 9 rabbit_ha_queues = True</span><br><span class="line">10 rabbit_durable_queues = False</span><br><span class="line">11 &#123;% endif -%&#125;</span><br></pre></td></tr></table></figure></p>
<p>其他debug手段:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo rabbitmqctl -p openstack list_queues|grep -vw 0</span><br><span class="line"></span><br><span class="line"># returns a list with info about memory dynamically allocated by the Erlang emulator</span><br><span class="line">rabbitmqctl eval &apos;erlang:memory().&apos;</span><br><span class="line">rabbitmq-diagnostics memory_breakdown  #for new version</span><br></pre></td></tr></table></figure></p>
<p>内存使用量一直突然增长可能是这个错误造成的， 详见：<a href="https://bugs.launchpad.net/oslo.messaging/+bug/1789177" target="_blank" rel="external">https://bugs.launchpad.net/oslo.messaging/+bug/1789177</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ grep -r &apos;no exchange&apos; var/log/rabbitmq/rabbit@juju-0aa49a-10-lxd-8.log |wc -l</span><br><span class="line">3511</span><br><span class="line">$ grep -r &apos;no exchange&apos; var/log/rabbitmq/rabbit@juju-0aa49a-10-lxd-8.log |head -n1</span><br><span class="line">operation basic.publish caused a channel exception not_found: no exchange &apos;reply_bee2ebfe01854e0595ddaa7462dc4054&apos; in vhost &apos;openstack&apos;</span><br></pre></td></tr></table></figure></p>
<p>这是关于rabbitm HA一个非常好的贴子（<a href="https://blog.csdn.net/zyz511919766/article/details/41896823" target="_blank" rel="external">https://blog.csdn.net/zyz511919766/article/details/41896823</a>), 也就是说，在rabbitmq non-ha中，exchange与 bindings也是在所在节点上的，而queue只处于一个节点。在rabbitmq ha环境中，只是将queue也根据policy放置在所有或某些节点上。当consumer从rabbitmq的某个节点消费queue时，consumer死掉了，consumer创建的reply-xxx queues也会被删掉但此时可能consumer没有收到basic cancel signal（eg：当reply-xxx被删后刚好consumer在重启）, 这样，consumer没ack消息这样server还会继续给consumer发, 但此时就会报no exchange (exchange与reply-xxx queue都是consumer创建的，reply-xxx queue为0时exchange也会被删除）。<br>但是，有一点没闹明白，consumer重启之后不是会调用 reconnect继续重建exchange与reploy-xxx queue么？不过在重建之前，server会继续发， 这段期间就会有no exchange错误吧，这样会耗费大量的cpu，这样或许也是cpu升级的原因 （Lots of exchanges create problems during failover under high load）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ grep -r &apos;^reply_&apos; sos_commands/rabbitmq/rabbitmqctl_report |grep exchange |head -n1</span><br><span class="line">reply_0003bee270e54c8cb9b78ba3b51ba4e2  exchange        reply_0003bee270e54c8cb9b78ba3b51ba4e2  queue   reply_0003bee270e54c8cb9b78ba3b51ba4e2  []      openstack</span><br><span class="line">$ grep -r &apos;^reply_&apos; sos_commands/rabbitmq/rabbitmqctl_report |grep exchange |wc -l</span><br><span class="line">24991</span><br></pre></td></tr></table></figure></p>
<p>24991肯定会造成memory一直升高，所以一个最好办法就是设置x-cancel-on-ha-failover让queue没了时也取消consumer。另一个办法是也可以哪出哪些consumer在作怪.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cat sos_commands/networking/netstat_-W_-neopa| awk &apos;/:5672/ &#123; print $5 &#125;&apos; | awk -F: &apos;&#123; a[$1]++ &#125;; END &#123; for (i in a) print i, a[i] &#125;&apos; |sort -n -k 2 -r |more |head -n 3</span><br><span class="line">10.160.0.106 96</span><br><span class="line">10.160.0.208 86</span><br><span class="line">10.160.0.75 83</span><br></pre></td></tr></table></figure></p>
<p>也可以用下面的policy缓解:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#rabbitmqctl set_policy min-masters-queue -p openstack &apos;.*&apos; &apos;&#123;&quot;queue-master-locator&quot;:&quot;min-masters&quot;&#125;&apos; --apply-to queues --priority 10</span><br><span class="line">rabbitmqctl set_policy HA -p openstack &apos;^(?!amq\\.).*&apos; &apos;&#123;&quot;queue-master-locator&quot;:&quot;min-masters&quot;, &quot;ha-mode&quot;:&quot;all&quot;, &quot;ha-sync-mode&quot;:&quot;automatic&quot;&#125;&apos;</span><br></pre></td></tr></table></figure></p>
<p>但是exchange只是记录的一个名称，是不占用内存的，queues可能会占用内存。但30481个queue也就1.2G左右，也不大啊。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat rabbitmq_report.txt |gawk -F&apos;\t&apos; &apos;/^Queues on openstack/ &#123;a=1;next&#125;; a &amp;&amp; NF &#123;mem+=$17; n+=1&#125;; !NF &#123;a=0&#125; END &#123;print n, mem&#125;&apos;</span><br><span class="line">30481 1248040000</span><br></pre></td></tr></table></figure></p>
<p>相较，rabbitmqctl status显示queue_procs与queue_slave_procs占用的内存更大, queue_procs占了约199G. 队列占用的内存指的是队列进程消耗的，并不包含消息体（在二进制中）。当内存不足时，这部分的内存将交换到磁盘上。</p>
<ul>
<li>queue_procs：主队列，索引和消息保存在内存中。排队的消息数量越多，通常会将此内容归因于此部分。但是，这在很大程度上取决于队列属性以及消息是否作为瞬态发布</li>
<li>queue_slave_procs：队列镜像，索引和消息保存在内存中。减少镜像（副本）的数量或不使用固有的瞬态数据镜像队列可以减少镜像使用的RAM量。排队的消息数量越多，通常会将此内容归因于此部分。但是，这在很大程度上取决于队列属性以及消息是否作为瞬态发布。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;queue_procs,199051279736&#125;,</span><br><span class="line">&#123;queue_slave_procs,1243762680&#125;,</span><br></pre></td></tr></table></figure>
<p>这个网页（<a href="https://www.rabbitmq.com/monitoring.html#diagnostics-observer）提到&#39;rabbitmq-diagnostics" target="_blank" rel="external">https://www.rabbitmq.com/monitoring.html#diagnostics-observer）提到&#39;rabbitmq-diagnostics</a> observer’命令可以像top一样查看erlang虚拟机内进程的内存使用量，但rabbitmq-server 3.8版本才支持啊。那perf是否可以查看erlang虚机机内进程的call trace呢？但这篇文章说了怎么查erlang下的进程所用内存：<br>RabbitMQ及Erlang内存使用分析 - <a href="https://blog.csdn.net/jaredcoding/article/details/78115235" target="_blank" rel="external">https://blog.csdn.net/jaredcoding/article/details/78115235</a><br>RabbitMQ运维经验分享 - <a href="https://my.oschina.net/hackandhike/blog/801052" target="_blank" rel="external">https://my.oschina.net/hackandhike/blog/801052</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl status #single node status</span><br><span class="line">rabbitmqctl report #cluster status</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/27/Shell编程注意点/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/27/Shell编程注意点/" itemprop="url">Shell编程注意点</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-27T09:53:02+08:00">
                2020-11-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华 写于：发表于：2011-04-06<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )</p>
<p>1在shell脚本中调用另一个脚本的三种不同方式(fork,exec, source)<br>       我们先谈谈在shell脚本中调用另一个脚本的三种不同方式的区别（fork,exec, source )</p>
<p>fork ( /directory/script.sh ), fork是最普通的,就是直接在脚本里面用/directory/script.sh来调用script.sh这个脚本.运行的时候开一个sub-shell执行调用的脚本，sub-shell执行的时候,parent-shell还在。sub-shell执行完毕后返回parent-shell.sub-shell从parent-shell继承环境变量.但是sub-shell中的环境变量不会带回parent-shell</p>
<p>exec(exec /directory/script.sh) exec与fork不同，不需要新开一个sub-shell来执行被调用的脚本. 被调用的脚本与父脚本在同一个shell内执行。但是使用exec调用一个新脚本以后,父脚本中exec行之后的内容就不会再执行了。这是exec和source的区别</p>
<p>source(source/directory/script.sh)与fork的区别是不新开一个sub-shell来执行被调用的脚本，而是在同一个shell中执行.所以被调用的脚本中声明的变量和环境变量,都可以在主脚本中得到和使用.</p>
<p>可以通过下面这两个脚本来体会三种调用方式的不同:</p>
<p>1.sh </p>
<p>#!/bin/bash<br>A=B<br>echo “PID for 1.sh before exec/source/fork:$$”<br>exportA<br>echo “1.sh: /$A is $A”<br>case $1 in<br>       exec)<br>               echo “using exec…”<br>               exec ./2.sh ;;<br>       source)<br>               echo “using source…”<br>               ../2.sh ;;<br>        *)<br>               echo”using fork by default…”<br>               ./2.sh ;;<br>esac<br>echo”PID for 1.sh after exec/source/fork:$$”<br>echo “1.sh:/$A is $A”<br>2.sh </p>
<p>#!/bin/bash<br>echo”PID for 2.sh: $$”<br>echo “2.sh get /$A=$A from1.sh”<br>A=C<br>export A<br>echo “2.sh: /$A is $A”</p>
<p>执行情况：<br>$./1.sh<br>PID for 1.sh beforeexec/source/fork:5845364<br>1.sh: $A is B<br>using fork bydefault…<br>PID for 2.sh: 5242940<br>2.sh get $A=B from 1.sh<br>2.sh:$A is C<br>PID for 1.sh after exec/source/fork:5845364<br>1.sh: $A isB<br>$ ./1.sh exec<br>PID for 1.sh beforeexec/source/fork:5562668<br>1.sh: $A is B<br>using exec…<br>PID for2.sh: 5562668<br>2.sh get $A=B from 1.sh<br>2.sh: $A is C<br>$./1.sh source<br>PID for 1.sh beforeexec/source/fork:5156894<br>1.sh: $A is B<br>using source…<br>PIDfor 2.sh: 5156894<br>2.sh get $A=B from 1.sh<br>2.sh: $A is C<br>PIDfor 1.sh after exec/source/fork:5156894<br>1.sh: $A is C<br>$<br>2函数调用<br>先看一个例子，执行mysql的函数mysqlExec,如下：</p>
<p>source“mysql.conf”</p>
<p>mysqlExec(){<br>sql=$1<br>sqlOp=<code>echo${sql:0:6}| tr A-Z a-z</code><br>if[ “$sqlOp” != “select” ]; then<br>  sql=$sql”;select row_count();”<br>fi</p>
<p>#use different mysql command depends on the password</p>
<p>if[ -z $MYSQL_PASSWORD ]<br>then<br>  $mysql $MYSQL_DATABASE -h$MYSQL_HOSTNAME -u$MYSQL_USERNAME -se “${sql};”<br>else<br>  $mysql $MYSQL_DATABASE -h$MYSQL_HOSTNAME -u$MYSQL_USERNAME -p$MYSQL_PASSWORD-se “${sql};”<br>fi<br>status=$?<br>if[ $status -eq 0 ]; then<br>  if[ “$sqlOp” != “select” ]; then<br>    log “OK $sql”<br>  fi<br>else<br>  log “Occur DB Error, can retry in 3 seconds later -&gt; $sql”<br>  sleep3<br>  echo “DB_ERROR”<br>fi<br>return $status<br>exit<br>}</p>
<p>函数调用要注意两点：<br>1）函数中可以用echo,如上面的echo “DB_ERROR”，在调用时要获取echo的值，应该这样:<br>campaign=<code>mysqlExec&quot;$sql&quot;</code><br>if[ “x$campaign” == “x” -o “$campaign” ==”DB_ERROR” ]; then<br>continue<br>fi<br>2)函数中也可以有返回值，如上面的return $status，在调用时应该通过$?获得，如：<br>if[ $? -eq 0 ]; then<br>echo“zhanghua”<br>fi</p>
<p>3) 如果想从被调用的函数处返回一个值，可以这样</p>
<p>   调用gen_conf函数，传一个引用（注意不是变量）config_file进去， gen_conf $host config_file</p>
<p>   在gen_conf函数中通过__resultvar变量返回值:</p>
<p>   gen_config{</p>
<pre><code> local  __resultvar=$2
eval $__resultvar=&quot;&apos;$config&apos;&quot;
</code></pre><p>   }</p>
<p>3shell中的要用局部变量很纠结<br>你会看到shell有一个非常大的缺点，就是它在函数调用时，没有局部变量与全局变量之分，如A脚本调用B脚本中的一个函数，在B脚本内部有一个变量vari（你可能受JAVA影响认为它是局部变量那就大错特错了），如果A脚本中也有这个名为vari的变量，那么在函数返回时，B脚本的那个vari变量会将A脚本的vari变量覆盖，举个例子：<br>updateWithOptimisticLock(){<br>rand=$1<br>campaignId=$2<br>seq=1<br>updateVal=-1<br>status=1<br>while[ true ]; do<br>if[ $seq -gt 3 ]; then<br>log”FATAL ERROR, Update num error, $updatesql”<br>break<br>fi<br>cur=<code>queryCur&quot;$campaignId&quot;</code><br>if[ “$cur” == “DB_ERROR” ]; then<br>continue<br>fi<br>updateVal=$(($cur+$rand))<br>if[ “$rand” == “0” ]; then<br>break<br>fi<br>updatesql=”updatet<em>campaign</em> set num=$updateVal where campaign<em>id=$campaignId andnum=$cur”<br>affectRows=<code>mysqlExec&quot;$updatesql&quot;</code><br>if[ “$affectRows” == “1” ]; then<br>status=0<br>break<br>fi<br>seq=$(($seq+1))<br>done<br>echo$updateVal<br>return$status<br>}<br>调用的伪码如下，这时里面的seq变量会被上述updateWithOptimisticLock函数中的变量seq给覆盖，所以在shell中没有局部变量一说<br>seq=1<br>while[ $seq -le $cycleNum ]; do<br>updateFakeNumWithOptimisticLock$rand $campaignId<br>done<br>4使用xargs来传参数<br>在shell中的管道符|很强大，可以将前一条命令的标准输出作为下一条命令的标准输入，但是如果下一条命令不是标准输入而是需要传参的话，那怎么办呢，用xargs即可，例如下列shell中xargs命令的-i选项告诉xargs用前一条命令的标准输出去替换{}。<br>find. | xargs zgrep “<url>/Search?” | sed’s/.*q=/([-</url></em><em>()~.%+0-9A-Za-z]</em>/).*//1/‘ | sort -nr | uniq -c | sort-nr | head -1000 | xargs -i php -r “echorawurldecode(‘{}’)./“/n/“;” &gt; result.out &amp;</p>
<p>另， 修改/etc/hosts, 能处理127或localhost打头的多个hostname项如(ubuntu.me.com  ubuntu)， 用‘/usr/bin/awk ‘$1 ~ /^127|localhost/ {print $0}’ /etc/hosts’是为了避免IPv6 中的node如ff02::1 ip6-allnodes</p>
<p>/usr/bin/awk ‘$1 ~ /^127|localhost/ {print $0}’ /etc/hosts |awk ‘$1 ~ /^127|localhost/ {print $0}’ | /bin/sed “s/\s<em>(${CURRENT_HOSTNAME})(\s</em>)/\t${NETCFG_HOSTNAME}/g”</p>
<p>当然exec也可以实现上述功能，只是exec都是一次性读入内存容易内存溢出，如：<br>find. -name “<em>.m4” -exec grep –color -H “catalina”{} /;<br>5shell中的sed命令使用的正则是缩水版<br>shell中的sed命令使用的正则引擎和我们java中平常所用正则引擎并不一样，功能比较弱。<br>如上节中的|sed ‘s/.</em>q=/([-_<em>()~.%+0-9A-Za-z]</em>/)，就是因为shell的很多正则不支持，才在使用sed命令时用了那么多枚举。</p>
<p>20191115更新</p>
<p>上面说法不正确, sed可以加-r参数不用枚举, 例:</p>
<p>grep -r ‘get_or_set_cached_cell_and_set_connections’ var/log/nova/ |sed -r ‘s/.+(waited|held) (.+) inner.+/\2/g;t;d’ |sort -nr |head -n 5</p>
<p>nova service-list –bi nova-compute | grep nova-compute | cut -d ‘ ‘ -f 4 | xargs -n 1 -I {} ssh -o StrictHostKeyChecking=no  ubuntu@{} “date; hostname; zgrep MessagingTimeout /var/log/nova/nova-compute.log*; echo -e ‘—————————–\n’”</p>
<p>数组</p>
<p>readarray -t cookies&lt;&lt;&lt;”<code>ls -1 /var/snap/ovs-stat/common/tmp/tmp.662kJWxfEg/juju-4f585d-sf00272961-cisco-7/ovs/bridges/br-int/flowinfo/cookies/| grep -v 6cc04af0837d3b42</code>“<br>for c in ${cookies[@]}; do grep -rl $c /var/snap/ovs-stat/common/tmp/tmp.662kJWxfEg/juju-4f585d-sf00272961-cisco-7/ovs/bridges/br-int/flowinfo/tables; done| sort| uniq -c</p>
<p>svcs=(<br>nova-compute-kvm<br>neutron-openvswitch<br>)<br>for svc in ${svcs[@]}; do<br>juju config $svc worker-multiplier<br>done</p>
<p>排序相加</p>
<p>cat ps.txt | sed ‘s/.<em>](.</em>)/\1/g’ | column -t | body sort -n -k4 -r<br>uid     tgid     total_vm  rss      pgtables_bytes  swapents  oom_score_adj  name<br>100112  832785   5760963   1650592  16789504        76362     0              beam.smp</p>
<p>cat ps.txt | sed ‘s/.<em>](.</em>)/\1/g’ | column -t | body sort -n -k4 -r | awk ‘{sum+=$4;} END{print sum;}’</p>
<p>rabbitmq相序相加举例</p>
<p>$ cat sos<em>commands/networking/netstat</em>-W_-neopa |head -n1<br>Proto Recv-Q Send-Q Local Address           Foreign Address         State       User       Inode      PID/Program name     Timer<br>tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      101        1671174047 178/systemd-resolve  off (0.00/0/0)</p>
<p>$ cat sos<em>commands/networking/netstat</em>-W_-neopa| awk ‘/:5672/ { print $5 }’ | awk -F: ‘{ a[$1]++ }; END { for (i in a) print i, a[i] }’ |sort -n -k 2 -r |more |head -n 10<br>10.164.0.107 69</p>
<p>实例 - 抓取</p>
<p>#!/bin/bash</p>
<h1 id="Usage-nohup-proxychains4-crawl-sh-gt-log-txt-2-gt-1-amp"><a href="#Usage-nohup-proxychains4-crawl-sh-gt-log-txt-2-gt-1-amp" class="headerlink" title="Usage: nohup proxychains4 ./crawl.sh &gt; log.txt 2&gt;1&amp;"></a>Usage: nohup proxychains4 ./crawl.sh &gt; log.txt 2&gt;1&amp;</h1><p>#set -x<br>[[ -f page.txt ]] &amp;&amp; echo ‘skip lynx’ || lynx -dump ftp://ftp.hycom.org/datasets/force/ncep_cfsr/netcdf/ &gt;page.txt<br>grep -r “ftp://“ page.txt |awk ‘{print $2}’ &gt; urls.txt<br>readarray -t exist_files&lt;&lt;&lt;”<code>ls .</code>“<br>for url in $(cat urls.txt)<br>do<br>  skip=”0”<br>  name=$(echo $url |awk -F ‘/‘ ‘{print $NF}’)<br>  readarray -t exist_files&lt;&lt;&lt;”<code>ls .</code>“<br>  for f in ${exist_files[@]}; do<br>    [[ “$name” == “$f” ]] &amp;&amp; skip=”1”; break;<br>  done<br>  if [ “$skip” == “1” ]; then<br>    echo “skipping ${f}”;<br>  else<br>    echo “download ${url}”;<br>    wget -c ${url}<br>  fi<br>done<br>改进版</p>
<p>#!/bin/bash</p>
<h1 id="Usage-nohup-proxychains4-crawl-sh-gt-log-txt-2-gt-1-amp-1"><a href="#Usage-nohup-proxychains4-crawl-sh-gt-log-txt-2-gt-1-amp-1" class="headerlink" title="Usage: nohup proxychains4 ./crawl.sh &gt; log.txt 2&gt;1&amp;"></a>Usage: nohup proxychains4 ./crawl.sh &gt; log.txt 2&gt;1&amp;</h1><p>#set -x<br>[[ -f page.txt ]] &amp;&amp; echo ‘skip creating lynx’ || lynx -dump <a href="http://tds.hycom.org/thredds/catalog/datasets/force/ncep_cfsr/netcdf/catalog.html" target="_blank" rel="external">http://tds.hycom.org/thredds/catalog/datasets/force/ncep_cfsr/netcdf/catalog.html</a> &gt;page.txt<br>[[ -f urls.txt ]] &amp;&amp; echo ‘skip creating urls.txt’ || grep -r “http://“ page.txt |awk ‘{print $2}’ |grep -E ‘.nc$’ &gt; urls.txt<br>sed -i ‘/dswsfc/d’ urls.txt<br>sed -i ‘/dlwsfc/d’ urls.txt<br>readarray -t exist_files&lt;&lt;&lt;”<code>ls . |grep -E &#39;\.nc$&#39;</code>“;  #can’t use multiple commands in [[ ]]<br>[[ -f skip.txt ]] &amp;&amp; echo ‘will use skip.txt’ || printf “%s\n” “${exist_files[@]}” &gt; skip.txt<br>readarray -t skip_files&lt;&lt;&lt;”<code>cat skip.txt</code>“<br>for url in $(cat urls.txt)<br>do<br>  name=$(echo $url |awk -F ‘/‘ ‘{print $NF}’)<br>  realurl=’<a href="http://tds.hycom.org/thredds/fileServer/datasets/force/ncep_cfsr/netcdf/&#39;$name" target="_blank" rel="external">http://tds.hycom.org/thredds/fileServer/datasets/force/ncep_cfsr/netcdf/&#39;$name</a><br>  if <code>echo ${skip_files[@]} |grep -q &quot;$name&quot;</code>; then<br>    echo “skipping ${f}”;<br>  else<br>    echo “download ${realurl}”;</p>
<pre><code>#wget -c --limit-rate=3m ${realurl}
wget -c ${realurl}
#echo ${realurl} &gt;&gt; skip.txt
</code></pre><p>  fi<br>done<br>获取绝对目录</p>
<p>export OS_CACERT=$(dirname “$(realpath -s “${BASH_SOURCE[0]}”)”)/ssl/openstack-ssl/results/cacert.pem<br>从国外下载气象数据</p>
<p>#gdisk /dev/sdd1   #t, 0700</p>
<p>#sudo mkfs.ntfs -f -L win /dev/sdd1</p>
<p>#sudo ntfsfix /dev/sdd1<br>sudo mkfs.ext4 /dev/xvdb<br>sudo parted /dev/xvdb  #print<br>sudo mount /dev/xvdb /mnt/<br>sudo mkdir -p /mnt/ftp<br>sudo chown -R $USER /mnt/<br>sudo apt install curlftpfs -y</p>
<p>#sudo fusermount -zu /mnt/ftp<br>sudo curlftpfs ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/ /mnt/ftp<br>sudo curlftpfs -o rw,allow_other,uid=1000,gid=1000 ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/ /mnt/ftp<br>$ scp -i ~/.aws/zhhuabj.pem ubuntu@13.114.59.98:/mnt/ftp/uvel/rarchv.2012_205_00_3zu.nc4 /tmp/<br>rarchv.2012_205_00_3zu.nc4                                                                                                94%  236MB   7.6MB/s   00:01 ETA</p>
<h1 id="using-direct-io-and-cache-no-to-avoid-using-disk"><a href="#using-direct-io-and-cache-no-to-avoid-using-disk" class="headerlink" title="using direct_io and cache=no to avoid using disk"></a>using direct_io and cache=no to avoid using disk</h1><p>sudo bash -c ‘cat &gt;&gt;/etc/fstab’ &lt;&lt;EOF<br>curlftpfs#@ftp.hycom.org/datasets/global/GLBa0.08_rect/data/ /mnt/ftp fuse defaults,direct_io,cache=no,rw,allow_other,uid=1000,gid=1000,_netdev 0 0<br>EOF<br>sudo mount -a</p>
<p>rsync -avztur -e “ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ~/.aws/lxj.pem” –progress ubuntu@3.18.107.xxx:/mnt/ftp .</p>
<p>注意：　上面加了direct_io,cache=no后总是hang，得去掉．</p>
<p>#debug curlftpfs<br>sudo fusermount -zu /mnt/ftp/2d<br>sudo curlftpfs -f -v -o debug,ftpfs_debug=3,rw,allow_other,uid=1000,gid=1000 ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/2d /mnt/ftp/2d<br>1, use lftp to mirror</p>
<h1 id="lftp-e-“mirror-–delete-–only-newer-–verbose-–parallel-2”-ftp-ftp-hycom-org-datasets-global-GLBa0-08-rect-data"><a href="#lftp-e-“mirror-–delete-–only-newer-–verbose-–parallel-2”-ftp-ftp-hycom-org-datasets-global-GLBa0-08-rect-data" class="headerlink" title="lftp -e “mirror –delete –only-newer –verbose –parallel=2” ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/"></a>lftp -e “mirror –delete –only-newer –verbose –parallel=2” ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/</h1><p>lftp -c “set ftp:list-options -a;<br>open ‘ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/‘;<br>lcd /home/ubuntu/test;<br>cd 2d<br>mirror -c –use-cache –verbose –allow-chown –allow-suid –no-umask –verbose –parallel=2”</p>
<p>2, use wget to mirror</p>
<p>wget -c -m ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/<br>lynx -dump <a href="http://13.59.199.151/new_01hr/" target="_blank" rel="external">http://13.59.199.151/new_01hr/</a> &gt; page.txt<br>grep -r “http://“ page.txt |awk ‘{print $2}’ |grep -E ‘.nc$’ &gt; urls.txt<br>aria2c -x 10 -i urls.txt &gt;/dev/null 2&gt;/dev/null &amp;</p>
<p>并行运行</p>
<p>token=’111’<br>seq $PARALLEL_REQS | xargs -I {} -n1 -P$PARALLEL_REQS echo “$token”</p>
<p>#!/bin/bash -eux<br>PARALLEL_REQS=5<br>SLEEP_SECS=0.1<br>TEMPLOG=$(mktemp).log<br>K8S_ENDPOINT=’/api’<br>export CLUSTER_NAME=”juju-cluster”<br>APISERVER=$(kubectl config view -o jsonpath=”{.clusters[?(@.name==\”$CLUSTER_NAME\”)].cluster.server}”)<br>TOKEN=$(kubectl get secrets -o jsonpath=”{.items[?(@.metadata.annotations[‘kubernetes.io/service-account.name’]==’default’)].data.token}”|base64 –decode)</p>
<h1 id="trap-ctrl-c-and-call-ctrl-c"><a href="#trap-ctrl-c-and-call-ctrl-c" class="headerlink" title="trap ctrl-c and call ctrl_c()"></a>trap ctrl-c and call ctrl_c()</h1><p>trap ctrl_c INT<br>function ctrl<em>c() {<br>  local DEST=query-kubeapiserver.$(date ‘+%Y%m%d</em>%H%M’).log<br>  mv $TEMPLOG $DEST<br>  echo “output at $DEST”<br>  exit 0<br>}</p>
<p>set +x<br>while true; do<br>  echo “in while loop”<br>  seq $PARALLEL_REQS | xargs -I {} -n1 -P$PARALLEL_REQS curl -s -X GET $APISERVER$K8S_ENDPOINT –header “Authorization: Bearer $TOKEN” –insecure 2&gt;&amp;1 &gt;&gt; $TEMPLOG</p>
<h1 id="seq-PARALLEL-REQS-xargs-I-P-PARALLEL-REQS-sudo-etcd-etcdctl-–cacert-root-cdk-etcd-client-ca-pem-–cert-root-cdk-etcd-client-cert-pem-–key-root-cdk-etcd-client-key-pem-–endpoints-ETCD-ENDPOINTS-get-w-json-KEY-2-gt-amp-1-gt-gt-TEMPLOG"><a href="#seq-PARALLEL-REQS-xargs-I-P-PARALLEL-REQS-sudo-etcd-etcdctl-–cacert-root-cdk-etcd-client-ca-pem-–cert-root-cdk-etcd-client-cert-pem-–key-root-cdk-etcd-client-key-pem-–endpoints-ETCD-ENDPOINTS-get-w-json-KEY-2-gt-amp-1-gt-gt-TEMPLOG" class="headerlink" title="seq $PARALLEL_REQS | xargs -I {} -P$PARALLEL_REQS sudo etcd.etcdctl –cacert /root/cdk/etcd/client-ca.pem –cert /root/cdk/etcd/client-cert.pem –key /root/cdk/etcd/client-key.pem –endpoints=$ETCD_ENDPOINTS get -w json $KEY 2&gt;&amp;1 &gt;&gt; $TEMPLOG"></a>seq $PARALLEL_REQS | xargs -I {} -P$PARALLEL_REQS sudo etcd.etcdctl –cacert /root/cdk/etcd/client-ca.pem –cert /root/cdk/etcd/client-cert.pem –key /root/cdk/etcd/client-key.pem –endpoints=$ETCD_ENDPOINTS get -w json $KEY 2&gt;&amp;1 &gt;&gt; $TEMPLOG</h1><p>  echo “sleeping..”<br>  /bin/sleep $SLEEP_SECS<br>  echo “sleep done”<br>done</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/13/恢复系统记录/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/13/恢复系统记录/" itemprop="url">恢复系统记录</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-13T09:26:11+08:00">
                2020-11-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>作者：张华 发表于：2017-02-09<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )</strong></p>
<p>今天系统又无故crash并无法启动了，折腾了一下午，记录一下。</p>
<p>突然运行“sudo apt-get update”时发生错误，一看是写保护，所以运行”sudo mount -o rw,remount /“, 但是系统报”unknown filesystem”，接着就crash了。</p>
<p>重启后出现了grub resue界面，试图通过下列命令恢复grub时仍然报”unknown filesystem”错误。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ls (hd1,msdos5)</span><br><span class="line">set root=(hd1,msdos5)</span><br><span class="line">set prefix=(hd1,5)/boot/grub</span><br><span class="line">lsmod normal</span><br><span class="line">normal</span><br><span class="line"></span><br><span class="line">sudo update-grub</span><br><span class="line">sudo grub-install /dev/sdb</span><br></pre></td></tr></table></figure></p>
<p>通过usb启动盘启动后运行“sudo fsck.ext4 -y /dev/sda9”之后上面磁盘的问题是解决了，也出现了登录界面，但是却无法登录，原本想通过下列命令重置:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -fr ~/.cache/compizconfig-1</span><br><span class="line">sudo rm -fr ~/.Xauthority</span><br><span class="line">sudo apt-get install --reinstall ubuntu-desktop unity compizconfig-settings-manager</span><br><span class="line">sudo dconf reset -f /org/compiz/</span><br><span class="line">setsid unity</span><br><span class="line">sudo rm -fr .cache/</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">http://kuring.me/ref/x_window/X_client_server_example.svg.png</span><br><span class="line">linux itself –&gt; X Server(xorg|wayland) &lt;- [X Protocol] -&gt; unity-greeter -&gt; Display Manager(GDM|lightdm, login GUI, /etc/X11/default-display-manager, or sudo dpkg-reconfigure lightdm) -&gt; Desktop Manager(GNOME, KDE) -&gt; X Application(eg: xclock)</span><br><span class="line">1, ctrl+alt+fn [n: 1-12]: used to switch virtual control console, 1-6 is used for linux (eg: echo $DISPLAY &amp;&amp; xclock -display :0 &amp;&amp; DISPLAY=:0 xclock)</span><br><span class="line">2, startx process (login -&gt; bash -&gt; startx -&gt; xinit -&gt; X -&gt; ck-xinit-session -&gt; gnome-session), xinit is used to start X Server software.</span><br><span class="line">3, &apos;init 5&apos; is used to restart X window</span><br></pre></td></tr></table></figure>
<p>但是发现/var/lib/dpkg目录不存在了，另外也有其他很多程序出现少文件的问题，不是我删除的，应该是fsck命令没有全部正确恢复inode吧。这种情况只能重装操作系统了，将所有工作需要的应用都安装好后也做了一个备份（dd if=/dev/sdb conv=sync,noerror bs=64K | gzip -c  &gt; /images/working_os_bak.img.gz）， 今后再出问题时希望通过命令（gunzip -c /images/working_os_bak.img.gz | dd of=/dev/sdb conv=sync,noerror bs=64K）能快速恢复操作系统和所需要的应用吧。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># dd if=/dev/sdb conv=sync,noerror bs=64K | gzip -c  &gt; /images/working_os_bak.img.gz</span><br><span class="line">1831575+1 records in</span><br><span class="line">1831576+0 records out</span><br><span class="line">120034164736 bytes (120 GB, 112 GiB) copied, 914.123 s, 131 MB/s</span><br><span class="line"></span><br><span class="line">$ ll /images/working_os_bak.img.gz -h</span><br><span class="line">-rw-r--r-- 1 root root 4.7G Feb  9 17:36 /images/working_os_bak.img.gz</span><br></pre></td></tr></table></figure>
<p>硬盘损坏看起来像：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;- - -&quot; &gt; /sys/class/scsi_host/host0/scan</span><br><span class="line"></span><br><span class="line">#Unused HDD</span><br><span class="line">Mar  1 10:24:28 node1 kernel: [ 2697.930058] ata3: hard resetting link</span><br><span class="line">Mar  1 10:24:28 node1 kernel: [ 2698.245478] ata3: SATA link down (SStatus 0 SControl 300)</span><br><span class="line">Mar  1 10:24:28 node1 kernel: [ 2698.245490] ata3: EH complete</span><br><span class="line"></span><br><span class="line">#Good HDD</span><br><span class="line">Mar  1 10:23:43 node1 kernel: [ 2652.763243] ata1: hard resetting link</span><br><span class="line">Mar  1 10:23:43 node1 kernel: [ 2653.077298] ata1: SATA link up 6.0 Gbps (SStatus 133 SControl 300)</span><br><span class="line">Mar  1 10:23:43 node1 kernel: [ 2653.097249] ata1.00: configured for UDMA/133</span><br><span class="line">Mar  1 10:23:43 node1 kernel: [ 2653.097251] ata1: EH complete</span><br><span class="line"></span><br><span class="line">#Bad HDD</span><br><span class="line">Feb 28 16:00:30 node1 kernel: [  231.696088] ata2.00: exception Emask 0x0 SAct 0x80000 SErr 0x0 action 0x6 frozen</span><br><span class="line">Feb 28 16:00:30 node1 kernel: [  231.696098] ata2.00: failed command: READ FPDMA QUEUED</span><br><span class="line">Feb 28 16:00:30 node1 kernel: [  231.696108] ata2.00: cmd 60/10:98:b0:bd:3c/00:00:a6:00:00/40 tag 19 ncq 8192 in</span><br><span class="line">Feb 28 16:00:30 node1 kernel: [  231.696108]          res 40/00:00:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)</span><br><span class="line">Feb 28 16:00:30 node1 kernel: [  231.696113] ata2.00: status: &#123; DRDY &#125;</span><br><span class="line">Feb 28 16:00:30 node1 kernel: [  231.696119] ata2: hard resetting link</span><br><span class="line">Feb 28 16:00:36 node1 kernel: [  237.059963] ata2: link is slow to respond, please be patient (ready=0)</span><br><span class="line">Feb 28 16:00:40 node1 kernel: [  241.707975] ata2: COMRESET failed (errno=-16)</span><br><span class="line">Feb 28 16:00:40 node1 kernel: [  241.707980] ata2: hard resetting link</span><br><span class="line">Feb 28 16:00:46 node1 kernel: [  247.067970] ata2: link is slow to respond, please be patient (ready=0)</span><br></pre></td></tr></table></figure></p>
<p>换硬盘不需要重装系统，将fstab文件修改一下即可:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LABEL=&quot;images&quot; /images         ext4    defaults        0       2</span><br><span class="line">LABEL=&quot;bak&quot; /bak         ext4    defaults        0       2</span><br><span class="line">192.168.99.122:/Public /nas nfs noauto,x-systemd.automount,x-systemd.device-timeout=10,timeo=14,x-systemd.idle-timeout=1min 0 0</span><br></pre></td></tr></table></figure></p>
<h2 id="201923更新-解决login-loop问题"><a href="#201923更新-解决login-loop问题" class="headerlink" title="201923更新 - 解决login loop问题"></a>201923更新 - 解决login loop问题</h2><p>反复出现登录界面, 根据下列流程, 似乎问题只可能出现在gnome这块出问题了才会回到lightdm的登录界面处.<br>linux itself –&gt; X Server(xorg|wayland) &lt;- [X Protocol] -&gt; unity-greeter -&gt; Display Manager(GDM|lightdm, login GUI, /etc/X11/default-display-manager, or sudo dpkg-reconfigure lightdm) -&gt; Desktop Manager(GNOME, KDE) -&gt; X Application(eg: xclock)<br>搜索kern.log的Call strace确认问题与nvidia驱动相关:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"> 733 Nov 16 22:47:03 localhost kernel: [    1.896228] Could not determine valid watermarks for inherited state</span><br><span class="line"> 734 Nov 16 22:47:03 localhost kernel: [    1.896290] WARNING: CPU: 2 PID: 183 at /build/linux-O4X9pa/linux-4.15.0/drivers/gpu/d</span><br><span class="line"> 735 Nov 16 22:47:03 localhost kernel: [    1.896293] Modules linked in: nvidia_drm(POE) nvidia_modeset(POE) nvidia(POE) rtsx_pc</span><br><span class="line"> 736 Nov 16 22:47:03 localhost kernel: [    1.896309] CPU: 2 PID: 183 Comm: systemd-udevd Tainted: P           OE    4.15.0-68-g</span><br><span class="line"> 737 Nov 16 22:47:03 localhost kernel: [    1.896311] Hardware name: LENOVO 20AN002LCD/20AN002LCD, BIOS GLET64WW (2.18 ) 12/18/2</span><br><span class="line"> 738 Nov 16 22:47:03 localhost kernel: [    1.896342] RIP: 0010:intel_modeset_init+0xfcf/0x1010 [i915]</span><br><span class="line"></span><br><span class="line">748 Nov 16 22:47:03 localhost kernel: [    1.896362] Call Trace:</span><br><span class="line"> 749 Nov 16 22:47:03 localhost kernel: [    1.896389]  i915_driver_load+0xa73/0xe60 [i915]</span><br><span class="line"> 750 Nov 16 22:47:03 localhost kernel: [    1.896414]  i915_pci_probe+0x42/0x70 [i915]</span><br><span class="line"> 751 Nov 16 22:47:03 localhost kernel: [    1.896418]  local_pci_probe+0x47/0xa0</span><br><span class="line"> 752 Nov 16 22:47:03 localhost kernel: [    1.896421]  pci_device_probe+0x10e/0x1c0</span><br><span class="line"> 753 Nov 16 22:47:03 localhost kernel: [    1.896424]  driver_probe_device+0x30c/0x490</span><br><span class="line"> 754 Nov 16 22:47:03 localhost kernel: [    1.896426]  __driver_attach+0xcc/0xf0</span><br><span class="line"> 755 Nov 16 22:47:03 localhost kernel: [    1.896429]  ? driver_probe_device+0x490/0x490</span><br><span class="line"> 756 Nov 16 22:47:03 localhost kernel: [    1.896431]  bus_for_each_dev+0x70/0xc0</span><br><span class="line"> 757 Nov 16 22:47:03 localhost kernel: [    1.896433]  driver_attach+0x1e/0x20</span><br><span class="line"> 758 Nov 16 22:47:03 localhost kernel: [    1.896436]  bus_add_driver+0x1c7/0x270</span><br><span class="line"> 759 Nov 16 22:47:03 localhost kernel: [    1.896438]  ? 0xffffffffc041b000</span><br><span class="line"> 760 Nov 16 22:47:03 localhost kernel: [    1.896440]  driver_register+0x60/0xe0</span><br><span class="line"> 761 Nov 16 22:47:03 localhost kernel: [    1.896442]  ? 0xffffffffc041b000</span><br><span class="line"> 762 Nov 16 22:47:03 localhost kernel: [    1.896444]  __pci_register_driver+0x5a/0x60</span><br><span class="line"> 763 Nov 16 22:47:03 localhost kernel: [    1.896465]  i915_init+0x5c/0x5f [i915]</span><br></pre></td></tr></table></figure>
<p>升级nvidia驱动从nvidia-driver-390到435看是否能解决问题<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">deb http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic main</span><br><span class="line">sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys FCAE110B1118213C</span><br><span class="line">sudo apt purge nvidia-driver-390 &amp;&amp; sudo apt autoremove &amp;&amp; sudo apt install nvidia-driver-435</span><br></pre></td></tr></table></figure></p>
<p>也可以试试:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#https://launchpad.net/~canonical-kernel-team/+archive/ubuntu/ppa</span><br><span class="line">sudo apt upgrade-dist</span><br><span class="line">sudo add-apt-repository ppa:canonical-kernel-team/ppa</span><br><span class="line">#can install the latest hwe kernel by using above ppa</span><br><span class="line">sudo apt install --install-recommends linux-generic-hwe-18.04 xserver-xorg-hwe-18.04</span><br></pre></td></tr></table></figure></p>
<p>20200203更 , 还是不行, 用下列方法重装了desktop再观察:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">sudo rm /var/lib/apt/lists/lock</span><br><span class="line">sudo rm /var/lib/dpkg/lock</span><br><span class="line">sudo rm /var/lib/dpkg/lock-frontend</span><br><span class="line">sudo dpkg --configure -a</span><br><span class="line">sudo apt clean</span><br><span class="line">sudo apt update --fix-missing</span><br><span class="line">sudo apt install -f</span><br><span class="line">sudo dpkg --configure -a</span><br><span class="line">sudo apt upgrade</span><br><span class="line">sudo apt dist-upgrade</span><br><span class="line"></span><br><span class="line">sudo apt purge *unity*</span><br><span class="line">sudo apt purge *gnome*</span><br><span class="line">sudo apt autoremove</span><br><span class="line">sudo apt-get install --reinstall ubuntu-desktop</span><br><span class="line">sudo update-grub</span><br><span class="line">sudo apt install gdm3</span><br><span class="line">sudo dpkg-reconfigure gdm3</span><br><span class="line"></span><br><span class="line">#Install Gnome and set it as default</span><br><span class="line">sudo apt install gnome-session</span><br><span class="line">sudo update-alternatives --config gdm3.css</span><br><span class="line">sudo reboot</span><br><span class="line">#Switch to a Windows-like Taskbar - https://vitux.com/how-to-get-the-windows-look-feel-on-ubuntu/</span><br><span class="line">sudo apt install gnome-shell-extensions gnome-shell-extension-dash-to-panel gnome-tweaks adwaita-icon-theme-full</span><br><span class="line"></span><br><span class="line">#Install windows-style desktop Cinnamon, but it&apos;s bluetooth doesn&apos;t work</span><br><span class="line">sudo add-apt-repository ppa:embrosyn/cinnamon</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install cinnamon</span><br><span class="line">#sudo apt remove --autoremove cinnamon cinnamon-desktop-data</span><br><span class="line">#sudo add-apt-repository --remove ppa:embrosyn/cinnamon</span><br></pre></td></tr></table></figure>
<p>20201113更新，切换到使用wayland再试试:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hua@t440p:~$ grep -r &apos;Wayland&apos; /etc/gdm3/custom.conf</span><br><span class="line">WaylandEnable=true</span><br><span class="line">sudo systemctl restart gdm3</span><br><span class="line">hua@t440p:~$ echo $XDG_SESSION_TYPE</span><br><span class="line">wayland</span><br></pre></td></tr></table></figure>
<p>但是gdm3是超级难用啊，刚装上就遇到chrome在gdm3上在复制粘贴时chrome就无响应的问题，只得再切回x11了，但我删除了lightdm遗留并且添加了下列配置，再难容一下吧。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/gdm3/PostSession/Default</span><br><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">killall -9 -u $USER</span><br><span class="line"></span><br><span class="line">exit 0</span><br></pre></td></tr></table></figure></p>
<p>20210118继续尝试，今天继续看到”systemd-logind got pause“错误,以及”PowerSaveMode, Timeout was reached”相关错误， 将尝试一下使用lightdm试试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install lightdm</span><br></pre></td></tr></table></figure></p>
<p>另外，也将尝试下列命令是否能恢复</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl daemon-reexec &amp;&amp; sudo systemctl start systemd-logind</span><br></pre></td></tr></table></figure>
<h2 id="20210122更新-－-台式机死机的问题"><a href="#20210122更新-－-台式机死机的问题" class="headerlink" title="20210122更新　－　台式机死机的问题"></a>20210122更新　－　台式机死机的问题</h2><p>台式机经常莫名其秒就死了，尤其在开kvm虚机时，今天又死了，然后在/var/crash目录发现了_usr_lib_x86_64-linux-gnu_indicator-keyboard_indicator-keyboard-service.108.crash<br>也发现了如下日志：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/var/log/kern.log</span><br><span class="line">Jan 22 17:46:21 node1 kernel: [ 2163.654654] indicator-keybo[2918]: segfault at 20 ip 00007f259e9213d0 sp 00007ffde892f540 error 6 in libglib-2.0.so.0.6400.3[7f259e919000+84000]</span><br><span class="line"></span><br><span class="line">/var/log/syslog</span><br><span class="line">Jan 22 17:46:22 node1 systemd[2065]: indicator-keyboard.service: Main process exited, code=dumped, status=11/SEGV</span><br><span class="line">Jan 22 17:46:22 node1 systemd[2065]: indicator-keyboard.service: Failed with result &apos;core-dump&apos;.</span><br><span class="line">Jan 22 17:46:22 node1 systemd[2065]: indicator-keyboard.service: Scheduled restart job, restart counter is at 1.</span><br><span class="line">Jan 22 17:46:22 node1 systemd[2065]: Stopped Indicator Keyboard Backend.</span><br><span class="line">Jan 22 17:46:22 node1 systemd[2065]: Started Indicator Keyboard Backend.</span><br><span class="line">Jan 22 17:46:22 node1 indicator-keyboard-service[18705]: Unable to init server: Could not connect: Connection refused</span><br><span class="line">Jan 22 17:46:22 node1 indicator-keybo[18705]: gtk_icon_theme_get_for_screen: assertion &apos;GDK_IS_SCREEN (screen)&apos; failed</span><br></pre></td></tr></table></figure></p>
<p>原本想运行”apt remove –purge indicator-keyboard“删除它，但使用ps -ef |grep indicator查看有lightdm的字眼，所以改用gdm3就好了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install gdm3 -y</span><br><span class="line">sudo dpkg-reconfigure gdm3</span><br><span class="line">cat /etc/X11/default-display-manager</span><br><span class="line">sudo systemctl status display-manager</span><br></pre></td></tr></table></figure>
<p>但是笔记本上经常性登录crash，似乎是gdm3的问题，目前正在改成lightdm试。</p>
<p>目前系统很慢，使用“ps aux –sort=-pcpu | head”看到的是chrome慢，也看到大量这种错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Feb  1 15:09:30 t440p gnome-shell[3320]: libinput error: client bug: timer event25 debounce short: scheduled expiry is in the past (-19ms), your system is too slow</span><br></pre></td></tr></table></figure></p>
<p>修改了下面但也是无效：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chrome://settings/?search=hardware selects &apos;Use hardware acceleration when available&apos;</span><br><span class="line">chrome://flags/#disable-accelerated-video-decode selects &apos;Hardware-accelerated video decode&apos;</span><br></pre></td></tr></table></figure>
<p>似乎是遇到了这个问题　－　<a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1122224" target="_blank" rel="external">https://bugs.chromium.org/p/chromium/issues/detail?id=1122224</a><br>回退到老版本试试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">VERSION_STRING=&quot;85.0.4183.121-1&quot; # Replace this value with the one you copied earlier</span><br><span class="line">wget &quot;https://dl.google.com/linux/chrome/deb/pool/main/g/google-chrome-stable/google-chrome-stable_$&#123;VERSION_STRING&#125;_amd64.deb&quot;</span><br><span class="line">sudo dpkg -i &quot;google-chrome-stable_$&#123;VERSION_STRING&#125;&quot;</span><br></pre></td></tr></table></figure></p>
<p>最终解决问题的是disable GPU<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">https://blog.csdn.net/weixin_39726131/article/details/111653527</span><br><span class="line"></span><br><span class="line">chrome://settings/?search=hardware uncheck &apos;Use hardware acceleration when available&apos;</span><br><span class="line">chrome://flags/ search &apos;gpu&apos; then disable &apos;Accelerated 2D canvas&apos;  and &apos;GPU rasterization&apos;</span><br><span class="line"></span><br><span class="line">shift + esc to see chrome task</span><br><span class="line"></span><br><span class="line">Chrome 浏览器默认开启了“GPU 渲染”的特性，当开启了硬件加速选项之后，所有的 WEB 网页内容都会使用显卡 GPU 来进行解析渲染</span><br></pre></td></tr></table></figure></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>t440p笔记本上只能使用lightdm (使用gdm3会产生login loop问题），而node1台式机上却只能使用gdm3 (使用lightdm会因indicator-keyboard莫名其妙的死机）<br>t440p笔记本上的chrome需要disable GPU，否则所有tab都使用GPU渲染若GPU性能不好反而慢。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/12/解决ssh-server偶尔连不上或者发现网络连接初始很慢的状况/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/12/解决ssh-server偶尔连不上或者发现网络连接初始很慢的状况/" itemprop="url">解决ssh server偶尔连不上或者发现网络连接初始很慢的状况</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-12T11:35:06+08:00">
                2020-11-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (<a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a>)</strong></p>
<p>今天花了近一天时间解决了一个恶心的问题，回过头看来当然很简单了，但要记录一下。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>我将办公用的台式机搬到了客厅，今后打算每天在笔记本上通过ssh连接到台式机办公。但是发生ssh连接老断，刚开始以为是ssh keepalive的问题，照着这个博客（<a href="http://blog.csdn.net/quqi99/article/details/51434248）配置后仍然无效。ssh连接断了之后，到台式机上查看发现网络是好的，重启台式机上的网络和ssh服务仍然无效，但重启笔记本上的网络服务就好了。大概五六分钟发生一次吧。" target="_blank" rel="external">http://blog.csdn.net/quqi99/article/details/51434248）配置后仍然无效。ssh连接断了之后，到台式机上查看发现网络是好的，重启台式机上的网络和ssh服务仍然无效，但重启笔记本上的网络服务就好了。大概五六分钟发生一次吧。</a></p>
<h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>家里的tplink摄像头指定了固定IP 192.168.99.123，不知道为什么openwrt路由器将这个已占用的IP分配给了台式机。因为摄像头长期没用了，这个IP也ping不通，并且dnsmasq的/tmp/dhcp.leases配置文件中显示的这个IP都是和台式机的hostname在一起的，所以刚开始就没有想到这个重要的问题。</p>
<p>1, 当在台式机上ping网关时，居然第一次要停顿11秒，但第二次之后就正常了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hua@node1:~$ time ping -c 1 192.168.99.1</span><br><span class="line">PING 192.168.99.1 (192.168.99.1): 56 data bytes</span><br><span class="line">--- 192.168.99.1 ping statistics ---</span><br><span class="line">1 packets transmitted, 0 packets received, 100% packet loss</span><br><span class="line">real	0m11.012s</span><br><span class="line">user	0m0.000s</span><br><span class="line">sys	0m0.000s</span><br></pre></td></tr></table></figure></p>
<p>2, 台式机给网关发消息，台式机通过arp广播要网关的mac地址，网关向台式机返回arp响应，但这个响应被错误的发到摄像头的mac f0:7d:68:0c:90:40之上，所以当问题发生时，路由器抓包到了大量的下列消息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@OpenWrt:~# tcpdump -i br-lan  -e -n &apos;icmp&apos; and src host 192.168.99.123</span><br><span class="line">14:10:30.449983 f8:32:e4:be:87:cd &gt; 04:a1:51:8a:2c:a7, ethertype IPv4 (0x0800), length 98: 192.168.99.123 &gt; 192.168.99.1: ICMP echo request, id 9604, seq 0, length 64</span><br><span class="line">14:10:33.403550 f0:7d:68:0c:90:40 &gt; 04:a1:51:8a:2c:a7, ethertype IPv4 (0x0800), length 118: 192.168.99.123 &gt; 85.199.214.101: ICMP 192.168.99.123 udp port 123 unreachable, length 84</span><br></pre></td></tr></table></figure></p>
<p>3, 台式机因为等不到arp请求，从而停顿。</p>
<p>使用“ip -s -s neigh flush all”命令清台式机与路由器arp cache均无效，在台式机上运行arp请求会很慢。</p>
<p>后来给台式机更换其他IP就好了.</p>
<p>我看到的这个问题的影响如下：<br>1, 当笔记本采用ssh连接到台式机上，也就会时不时发生停顿现象<br>2, 使用apt update时会第一次连接时像断了一样，但后面又好了</p>
<h2 id="附录：Ubuntu下的远程桌面与X转发"><a href="#附录：Ubuntu下的远程桌面与X转发" class="headerlink" title="附录：Ubuntu下的远程桌面与X转发"></a>附录：Ubuntu下的远程桌面与X转发</h2><p>上面解决了ssh连接办公的问题，下面将远程桌面一并解决。</p>
<p>Ubuntu下有一个默认的vino-server, 可在”Desktop Sharing”中开启，另外，需要运行dconf-editor将org-&gt;gnome-&gt;desktop-&gt;remote-access中的requre-encryption去掉，最后使用Remmina Remote Desktop Client选择vnc协议输入“IP :0”登录。不过，这种自带的Vino-Server方式有一个最显著的缺点：那就是当你重启机器之后，必须首先到远程服务器那边登录机器，进入系统（相当于创建了一个Session）之后，才能在本地使用远程桌面连接这个远程服务器。所以我们改使用vnc4server。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo apt -y install vnc4server xvnc4viewer</span><br><span class="line">vncpasswd</span><br><span class="line"># First time to run it to generate ~/.vnc</span><br><span class="line">vncserver :2</span><br><span class="line">ll ~/.vnc/</span><br><span class="line">vncserver -kill :2</span><br><span class="line"># Second time to run it after modifying ~/.vnc</span><br><span class="line">sudo apt install mate-session-manager</span><br><span class="line">echo &apos;exec /usr/bin/mate-session &amp;&apos; &gt;&gt; ~/.vnc/xstartup</span><br><span class="line">vncserver :2 -geometry 1280x680 -alwaysshared</span><br><span class="line">Client Side: vncviewer node1:2</span><br></pre></td></tr></table></figure>
<p>运行”ssh -vvv -X -o ForwardX11=yes node1“时报错”X11 forwarding request failed on channel 0“， 这是因为出于安全原因，OpenSSH默认将X11转发请求绑定到本地回环地址上，并且在DISPLAY环境变量中将主机名设置为“localhost”。在这样的设定下，一些 X11客户端不能正确处理X11转发，这会导致报告中的错误。要解决这个问题，在/etc/ssh/sshd配置文件中加入下面这几行，它可以将X11转发请求绑定到外网卡地址上。<br>X11Forwarding yes<br>X11UseLocalhost no<br>如果远程主机的SSH服务禁止了IPv6，那么X11转发失败的错误也有可能发生。故还需要添加： AddressFamily inet</p>
<h2 id="20191220更新-另一个例子"><a href="#20191220更新-另一个例子" class="headerlink" title="20191220更新 - 另一个例子"></a>20191220更新 - 另一个例子</h2><p><img src="https://img-blog.csdnimg.cn/20191220175958218.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly96aGh1YWJqLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">客户有两个external的dmz网络172.21.8.0/24与193.169.205.0/24</span><br><span class="line">两个neutron vrouter, dmzrouterinternal接172.21.8.0/24(qg-xxx=172.21.8.87), dmzroutereexternal接193.169.205.0/24</span><br><span class="line">dmzrouterinternal接一个subnet(192.168.4.1), 这样创建一个测试VM=192.168.4.199 FIP=172.21.8.85</span><br><span class="line">一个外部物理路由器有172.21.8.1与193.169.205.1, 外部路由器上接一个外部dns服务器(193.169.205.141)</span><br><span class="line">问题是VM无法ping 193.169.205.141, 抓包看到ARP reply到达了dmzrouterinternal却被drop掉了</span><br><span class="line">15:26:20.634541 ARP, Request who-has 193.169.205.141 tell 172.21.8.87, length 28</span><br><span class="line">15:26:20.634810 ARP, Reply 193.169.205.141 is-at 00:15:5d:2d:d9:2c (oui Unknown), length 46</span><br><span class="line"></span><br><span class="line">发现它有这些路由:</span><br><span class="line"># ip netns exec qrouter-ffd74122-45b4-4155-b585-fe62b2d3edd4 ip route</span><br><span class="line">default via 172.21.8.1 dev qg-d84aed2a-34</span><br><span class="line">172.21.8.0/24 dev qg-d84aed2a-34 proto kernel scope link src 172.21.8.87</span><br><span class="line">192.168.4.0/24 dev qr-3c248a21-06 proto kernel scope link src 192.168.4.1</span><br><span class="line">193.169.205.0/24 dev qg-d84aed2a-34 scope link</span><br><span class="line"></span><br><span class="line">显然问题是这条路由&apos;193.169.205.0/24 dev qg-d84aed2a-34 scope link&apos;造成dmzrouterinternal与externaldns直连:</span><br><span class="line">ping之前, 看到193.169.205.141 at 00:00:0c:9f:f0:2c, 00:00:0c:9f:f0:2c是172.21.8.1的mac</span><br><span class="line"># arp -a</span><br><span class="line">? (192.168.4.42) at fa:16:3e:79:07:f0 [ether] on qr-3c248a21-06</span><br><span class="line">? (193.169.205.141) at 00:00:0c:9f:f0:2c [ether] on qg-d84aed2a-34</span><br><span class="line">当ping时, tcpdump收到ARP reply里的mac是00:15:5d:2d:d9:2c, 而不是00:00:0c:9f:f0:2c, 所以drop掉.</span><br><span class="line"></span><br><span class="line">如果临时运行下列命令, 也能ping通, 但一会儿这个arp就会过期, 问题就会再次重现, 所以解决的办法是删除这条路由(193.169.205.0/24 dev qg-d84aed2a-34 scope link)</span><br><span class="line">arp -d 193.169.205.141</span><br><span class="line">arp -i qg-d84aed2a-34 -s 193.169.205.141 02:00:5e:10:de:14</span><br><span class="line"></span><br><span class="line">下面做一个实验, 用一个neutron vrouter externalrouter来模拟外部物理路由器.</span><br><span class="line">./generate-bundle.sh -s bionic -r stein --num-compute 2</span><br><span class="line"></span><br><span class="line">openstack router create externalrouter</span><br><span class="line">openstack network create dmzexternalnet --external</span><br><span class="line">openstack network create dmzinternalnet --external</span><br><span class="line">openstack subnet create --subnet-range 172.21.8.0/24 --network dmzinternalnet --allocation-pool start=172.21.8.86,end=172.21.8.100 --gateway 172.21.8.1 dmzsubnetinternal</span><br><span class="line">openstack subnet create --subnet-range 193.169.205.0/24 --network dmzexternalnet --allocation-pool start=193.169.205.87,end=193.169.205.100 --gateway 193.169.205.1 dmzsubnetexternal</span><br><span class="line">openstack router add subnet externalrouter dmzsubnetinternal</span><br><span class="line">openstack router add subnet externalrouter dmzsubnetexternal</span><br><span class="line"></span><br><span class="line">#注:也可以不创建两个network, 只创建一个network, 然后将两个subnet都加到这个network, 在和router关联时特殊处理, 如:</span><br><span class="line">openstack router create --disable --centralized --ha dmzrouterinternal</span><br><span class="line">openstack router add subnet dmzrouterinternal dmztenantinternalsubnet</span><br><span class="line">openstack router set --external-gateway dmznet --fixed-ip subnet=dmzsubnetinternal,ip-address=172.21.8.87 dmzrouterinternal</span><br><span class="line">openstack router set --enable dmzrouterinternal</span><br><span class="line"></span><br><span class="line">openstack router create dmzrouterinternal</span><br><span class="line">openstack router set --external-gateway dmzinternalnet dmzrouterinternal</span><br><span class="line">openstack network create tenantnet</span><br><span class="line">openstack subnet create --subnet-range 192.168.4.0/24 --network tenantnet --allocation-pool start=192.168.4.87,end=192.168.4.100 --gateway 192.168.4.1 tenantsubnet</span><br><span class="line">openstack router add subnet dmzrouterinternal tenantsubnet</span><br><span class="line"></span><br><span class="line">openstack router create dmzrouterexternal</span><br><span class="line">openstack router set --external-gateway dmzexternalnet dmzrouterexternal</span><br><span class="line"></span><br><span class="line">openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey</span><br><span class="line">openstack server create --wait --image cirros2 --flavor m1.small --key-name mykey --network=$(openstack network show dmzexternalnet -c id -f value) externaldns</span><br><span class="line">openstack server create --wait --image cirros2 --flavor m1.small --key-name mykey --network=$(openstack network show tenantnet -c id -f value) i1</span><br><span class="line">dmzinternalnet=$(openstack network show dmzinternalnet -f value -c id)</span><br><span class="line">fip=$(openstack floating ip create $dmzinternalnet -f value -c floating_ip_address)</span><br><span class="line">openstack floating ip set $fip --fixed-ip-address 192.168.4.100 --port $(openstack port list --fixed-ip ip-address=192.168.4.100 -c id -f value)</span><br><span class="line"></span><br><span class="line">$ openstack server list</span><br><span class="line">+--------------------------------------+-------------+--------+--------------------------------------+---------+----------+</span><br><span class="line">| ID                                   | Name        | Status | Networks                             | Image   | Flavor   |</span><br><span class="line">+--------------------------------------+-------------+--------+--------------------------------------+---------+----------+</span><br><span class="line">| 3a1d1c70-6165-4060-ad78-9c67717e61e8 | externaldns | ACTIVE | dmzexternalnet=193.169.205.99        | cirros2 | m1.small |</span><br><span class="line">| 89b497c0-f236-4df4-8293-18f44547e239 | i1          | ACTIVE | tenantnet=192.168.4.100, 172.21.8.95 | cirros2 | m1.small |</span><br><span class="line">+--------------------------------------+-------------+--------+--------------------------------------+---------+----------+</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~/stsstack-bundles/openstack$ openstack router list</span><br><span class="line">+--------------------------------------+-------------------+--------+-------+----------------------------------+-------------+-------+</span><br><span class="line">| ID                                   | Name              | Status | State | Project                          | Distributed | HA    |</span><br><span class="line">+--------------------------------------+-------------------+--------+-------+----------------------------------+-------------+-------+</span><br><span class="line">| 09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 | dmzrouterinternal | ACTIVE | UP    | e2402e56ee0a4c31904e46b5c01c80f8 | False       | False |</span><br><span class="line">| 8320db06-5a9d-46fb-8af6-5c7b974ff7a9 | externalrouter    | ACTIVE | UP    | e2402e56ee0a4c31904e46b5c01c80f8 | False       | False |</span><br><span class="line">| ef19ea38-13e0-416a-9ce1-ac25c8aba9c0 | dmzrouterexternal | ACTIVE | UP    | e2402e56ee0a4c31904e46b5c01c80f8 | False       | False |</span><br><span class="line">+--------------------------------------+-------------------+--------+-------+----------------------------------+-------------+-------+</span><br><span class="line"></span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 ip route add 193.169.205.0/24 dev qg-e6cd9397-b8 scope link</span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 ip route</span><br><span class="line">default via 172.21.8.1 dev qg-e6cd9397-b8</span><br><span class="line">172.21.8.0/24 dev qg-e6cd9397-b8 proto kernel scope link src 172.21.8.92</span><br><span class="line">192.168.4.0/24 dev qr-e4d7b70a-14 proto kernel scope link src 192.168.4.1</span><br><span class="line">193.169.205.0/24 dev qg-e6cd9397-b8 scope link</span><br><span class="line">$ ping 193.169.205.99 -c 1</span><br><span class="line">...</span><br><span class="line">1 packets transmitted, 0 packets received, 100% packet loss</span><br><span class="line"></span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 arp -d 193.169.205.99</span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 arp -i qg-e6cd9397-b8 -s 193.169.205.99 fa:16:3e:7b:af:d6</span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 arp -a</span><br><span class="line">? (172.21.8.1) at fa:16:3e:0f:87:da [ether] on qg-e6cd9397-b8</span><br><span class="line">? (193.169.205.99) at fa:16:3e:7b:af:d6 [ether] PERM on qg-e6cd9397-b8</span><br><span class="line">? (192.168.4.100) at fa:16:3e:5d:2f:5b [ether] on qr-e4d7b70a-14</span><br><span class="line">? (172.21.8.87) at fa:16:3e:a1:c2:e4 [ether] on qg-e6cd9397-b8</span><br><span class="line">$ ping 193.169.205.99 -c 1</span><br><span class="line">...</span><br><span class="line">1 packets transmitted, 0 packets received, 100% packet loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 ip route del 193.169.205.0/24 dev qg-e6cd9397-b8 scope link</span><br><span class="line">$ ping 193.169.205.99 -c 1</span><br><span class="line">PING 193.169.205.99 (193.169.205.99): 56 data bytes</span><br><span class="line">64 bytes from 193.169.205.99: seq=0 ttl=62 time=11.998 ms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 arp -a</span><br><span class="line">? (172.21.8.1) at fa:16:3e:0f:87:da [ether] on qg-e6cd9397-b8</span><br><span class="line">? (192.168.4.100) at fa:16:3e:5d:2f:5b [ether] on qr-e4d7b70a-14</span><br><span class="line">? (172.21.8.87) at fa:16:3e:a1:c2:e4 [ether] on qg-e6cd9397-b8</span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 ip route</span><br><span class="line">default via 172.21.8.1 dev qg-e6cd9397-b8</span><br><span class="line">172.21.8.0/24 dev qg-e6cd9397-b8 proto kernel scope link src 172.21.8.92</span><br><span class="line">192.168.4.0/24 dev qr-e4d7b70a-14 proto kernel scope link src 192.168.4.1</span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-8320db06-5a9d-46fb-8af6-5c7b974ff7a9 arp -a</span><br><span class="line">? (172.21.8.92) at fa:16:3e:d5:36:a6 [ether] on qr-0912ab72-90</span><br><span class="line">? (193.169.205.87) at fa:16:3e:09:83:10 [ether] on qr-6fa183e9-a6</span><br><span class="line">? (172.21.8.86) at fa:16:3e:89:ce:85 [ether] on qr-0912ab72-90</span><br><span class="line">? (193.169.205.99) at fa:16:3e:7b:af:d6 [ether] on qr-6fa183e9-a6</span><br><span class="line">? (172.21.8.95) at fa:16:3e:d5:36:a6 [ether] on qr-0912ab72-90</span><br><span class="line">? (172.21.8.87) at fa:16:3e:a1:c2:e4 [ether] on qr-0912ab72-90</span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-8320db06-5a9d-46fb-8af6-5c7b974ff7a9 ip route</span><br><span class="line">172.21.8.0/24 dev qr-0912ab72-90 proto kernel scope link src 172.21.8.1</span><br><span class="line">193.169.205.0/24 dev qr-6fa183e9-a6 proto kernel scope link src 193.169.205.1</span><br></pre></td></tr></table></figure></p>
<h2 id="20201112更新"><a href="#20201112更新" class="headerlink" title="20201112更新"></a>20201112更新</h2><p>ssh这次失去连接，是因为台式机上的虚机使用mavtap时不小心将bridge改成了passthough导致虚机启动时失去网络.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/12/SSH连接总是定期断掉的解决办法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/12/SSH连接总是定期断掉的解决办法/" itemprop="url">SSH连接总是定期断掉的解决办法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-12T10:04:27+08:00">
                2020-11-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华  发表于：2016-05-17<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</p>
<p>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )</p>
<p>SSH连接总是隔一段时间没有输入时就断开，解决办法如下：</p>
<p>服务端配置<br>sudo vi /etc/ssh/sshd_config<br>ClientAliveInterval 60     #服务端主动向客户端请求响应的间隔<br>ClientAliveCountMax 10    #服务器发出请求后客户端没有响应的次数达到一定值就自动断开<br>sudo restart ssh</p>
<p>客户端配置<br>sudo vi /etc/ssh/ssh_config  #或~/.ssh/config</p>
<pre><code>TCPKeepAlive yes
ServerAliveInterval 15
ServerAliveCountMax 6
StrictHostKeyChecking no
ForwardAgent yes
Compression yes

 IPQoS throughput
</code></pre><p>或</p>
<p>ssh -i <key-file> IPQoS=throughput -o StrictHostKeyChecking=no -o TCPKeepAlive=yes -o ServerAliveInterval=30 ubuntu@<ip></ip></key-file></p>
<p>上面方式任选一种，我选客户端配置方式。</p>
<p>20200316更新, 如果ssh总是断时也得考虑使用白名单模式时是否将某些IP排除了.</p>
<p>20200902更新, 如果使用Compression yes会被墙看上, 会时不时ssh断掉, 去掉Compression即可.</p>
<p>20201016更新, 到国外的ssh连接总断, 除了ISP QoS外, 还试试在sshd.conf中添加:</p>
<p>UseDNS no</p>
<p>GSSAPIAuthentication no</p>
<p>20201028更新，</p>
<p>这次应该是找到了ssh总断的根源，因为bastion上安装了devstack所以在并行运行’opestack image set”时ssh会中断．所以习惯很重要，不能乱安装东西．</p>
<p>#openstack image set –property $p=${props[$p]} $img_name</p>
<p>openstack image set –property $p=${props[$p]} $img_name &amp;</p>
<p>另外，使用压缩传输数据：</p>
<p>#strings /usr/sbin/sshd |egrep “chacha|aes”<br>scp -p -C -o ‘IPQoS throughput’  -c chacha20-poly1305@openssh.com win10.img hua@node1:/images/</p>
<p>20201112更新</p>
<p>ssh总断的原因是因为机器上为反复重启virtualbox导致chrome会通过fast.com测速变得只有十几K, virutalbox改成了kvm现在观察了几天似乎这个问题消失了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/09/在KVM中运行win10虚机/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/09/在KVM中运行win10虚机/" itemprop="url">在KVM中运行win10虚机</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-09T09:55:35+08:00">
                2020-11-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华  发表于：2015-12-22<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )<br>KVM默认不支持windows 10，需要做一些设置：<br>1, sudo usermod -a -G vboxusers hua<br>2, 修改下列配置使KVM支持windows 10所需的PAE、NX、SSE2特性。PAE支持32位处理器可以访问4GB以上物理内存功能版本的Windows，并且它是NX的先决条件。NX可让处理器帮助保护电脑免受恶意软件的攻击。SSE2（一项使用已久的关于处理器的标准）是一套由越来越多的第三方应用和驱动程序使用的指令集。<br>   cpu model: core2duo<br>   cpu features: nx=require<br>   nic: rtl8139 to e1000</p>
<p>在ubuntu上制作win 10 usb启动盘的工具：</p>
<p>sudo add-apt-repository ppa:gezakovacs/ppa<br>sudo apt-get update<br>sudo apt-get install unetbootin</p>
<p>后续的工作是采用SR-IOV与USB直通，待续。</p>
<p>20200603更新　－　virtualbox中安装win10</p>
<p>立刻仅支持windows所以需要一个windows虚机，ubuntu 20.04上的kvm安装win10报错一时半会解不了，换成virtualbox</p>
<p>安装　－　<a href="https://www.oracle.com/virtualization/technologies/vm/downloads/virtualbox-downloads.html#extpack" target="_blank" rel="external">https://www.oracle.com/virtualization/technologies/vm/downloads/virtualbox-downloads.html#extpack</a></p>
<p>在虚机中若想使用host上的摄像头，还得安装上面链接中的ExtPack, 这样在启动虚机之后在devices -&gt; webcams菜单就能使用host的摄像头了．</p>
<p>在虚机中若想使用host上的麦克风和音箱怎么办？　在’devices -&gt; audio”菜单中将’audio output’与’audio input’都选上即可．</p>
<p>鼠标在虚机与host机上无缝切换，　在’devices -&gt; insert guest cd’然后在虚机里打开cd安装驱动即可．</p>
<p>全屏切换　host + F,  host是指right ctrl键，进入全屏退出全屏都是它．</p>
<p>20201109更新 - kvm安装win10<br>之前virtualbox安装的win10在开关多次之后，会造成这样一个问题，在chrome上使用fast.com测速时会降到几十K到12M之间，</p>
<p>而同时在firefox上测速能达到69M左右，在重启机器之后chrome上的测速又能恢复到92M。</p>
<p>所以不敢再继续使用virtualbox了，改用KVM吧。</p>
<p>1, 直接将之前的virtualbox镜像转成kvm的 -　qemu-img convert -p -f vdi -O raw win10.vdi win10.img</p>
<p>2, 导入镜像创建虚机时注意一点，在”Choose the operating system you are installing”处输入win搜索windows时没有win10的版本，需勾选”Include end of life operating systems“才出来。</p>
<p>3, 第一次默认安装对硬盘和网卡均使用默认驱动。</p>
<ol>
<li>安装完启动win10后在里面下载安装virtualbox驱动 - <a href="https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/stable-virtio/virtio-win.iso" target="_blank" rel="external">https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/stable-virtio/virtio-win.iso</a></li>
</ol>
<p>5, 此时只能将网卡驱动改为virtio(也可以改成mapvtap + virtio)重启机器可以进win10</p>
<p>6, 但此时若将硬盘也改为virtio驱动会报错的。因为硬盘里还根本没有iscsi-virtio驱动啊。解决办法是在硬盘默认驱动进入win10后，</p>
<p>   再添加第二块virtio驱动这样iscsi-virtio就装在硬盘了。这时再重启就可以了。见：　<a href="https://superuser.com/questions/1057959/windows-10-in-kvm-change-boot-disk-to-virtio" target="_blank" rel="external">https://superuser.com/questions/1057959/windows-10-in-kvm-change-boot-disk-to-virtio</a></p>
<p>7, 使用spice这样声音和麦克就都用了。注：如果不将网络与硬盘改为virtio驱动，在likeshuo使用音频会有延时听不清楚。</p>
<p>8, 可使用spice redirect usb特性将集成视频卡加到虚机，注意：选错usb可能造成蓝牙键盘失去连接。</p>
<p>９，”rsync -avztur –progress /bak/* /mnt/share/“, /mnt/share是一块iscis盘，bak缺空间，第１步转换后的win10硬盘在iscsi盘上，但之前加了–delete参数导制被删除转了两遍。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/08/使用iSCSI挂载QNAP存储/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/08/使用iSCSI挂载QNAP存储/" itemprop="url">使用iSCSI挂载QNAP存储</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-08T15:16:38+08:00">
                2020-11-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华  发表于：2015-12-28<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )</p>
<p>QNAP服务端<br>系统设置 -&gt; 存储空间总管 -&gt; iSCSI选项卡<br>入口管理: 启动iSCSI目标服务<br>iSCSI目标管理：创建一个iSCSI目标并挂载一个iSCSI LUN<br>  iqn.2004-04.com.qnap:ts-212p:iscsi.quqiSCSI.d5ad13<br>  目标名称：quqiSCSI<br>  目标别名：quqiSCSI<br>  启用CHAP认证：username/password<br>  动态配置：quqiSCSI, 800G</p>
<p>在qnap中文件将存储于：/share/HDA_DATA/.@iscsi.img/iSCSI-quqiSCSI-56809a35.000</p>
<p>OpenWRT服务端<br>opkg install tgt<br>dd if=/dev/zero of=/mnt/sda1/images/sdb_for_t440p.raw bs=1M count=80000</p>
<p>vi /etc/config/tgt<br>config target 1<br>       option name ‘iqn.2018-05.com.quqi99:sdb_for_t440p’<br>       option allow 192.168.99.0/24<br>config lun 1_1</p>
<h1 id="option-readonly-0"><a href="#option-readonly-0" class="headerlink" title="option readonly 0"></a>option readonly 0</h1><pre><code>option device /mnt/sda1/images/sdb_for_t440p.raw
</code></pre><p>/etc/init.d/tgt enable<br>/etc/init.d/tgt restart<br>20201105更新 - 但现在报这个错误：　tgt: Failed to create target，　运行’tgtd  -f -d -D’显示Segmentation fault</p>
<p>iSCSI Initiator客户端<br>hua@node1:~$ sudo /etc/init.d/open-iscsi start</p>
<p>hua@node1:~$ sudo update-rc.d -f open-iscsi remove<br> Removing any system startup links for /etc/init.d/open-iscsi …<br>   /etc/rc0.d/K81open-iscsi<br>   /etc/rc0.d/S45open-iscsi<br>   /etc/rc1.d/K81open-iscsi<br>   /etc/rc6.d/K81open-iscsi<br>hua@node1:~$ sudo update-rc.d open-iscsi start 20 2 3 4 5 . stop 20 0 1 6 .<br>update-rc.d: warning:  start runlevel arguments (2 3 4 5) do not match open-iscsi Default-Start values (S)<br> Adding system startup for /etc/init.d/open-iscsi …<br>   /etc/rc0.d/K20open-iscsi -&gt; ../init.d/open-iscsi<br>   /etc/rc1.d/K20open-iscsi -&gt; ../init.d/open-iscsi<br>   /etc/rc6.d/K20open-iscsi -&gt; ../init.d/open-iscsi<br>   /etc/rc2.d/S20open-iscsi -&gt; ../init.d/open-iscsi<br>   /etc/rc3.d/S20open-iscsi -&gt; ../init.d/open-iscsi<br>   /etc/rc4.d/S20open-iscsi -&gt; ../init.d/open-iscsi<br>   /etc/rc5.d/S20open-iscsi -&gt; ../init.d/open-iscsi<br>hua@node1:~$ sudo update-rc.d open-iscsi enable<br>update-rc.d: warning:  start runlevel arguments (none) do not match open-iscsi Default-Start values (S)<br>update-rc.d: warning:  stop runlevel arguments (none) do not match open-iscsi Default-Stop values (0 1 6)<br> Enabling system startup links for /etc/init.d/open-iscsi …<br> Removing any system startup links for /etc/init.d/open-iscsi …<br>   /etc/rc0.d/K20open-iscsi<br>   /etc/rc1.d/K20open-iscsi<br>   /etc/rc2.d/S20open-iscsi<br>   /etc/rc3.d/S20open-iscsi<br>   /etc/rc4.d/S20open-iscsi<br>   /etc/rc5.d/S20open-iscsi<br>   /etc/rc6.d/K20open-iscsi<br> Adding system startup for /etc/init.d/open-iscsi …<br>   /etc/rc0.d/K20open-iscsi -&gt; ../init.d/open-iscsi<br>   /etc/rc1.d/K20open-iscsi -&gt; ../init.d/open-iscsi<br>   /etc/rc6.d/K20open-iscsi -&gt; ../init.d/open-iscsi<br>   /etc/rc2.d/S20open-iscsi -&gt; ../init.d/open-iscsi<br>   /etc/rc3.d/S20open-iscsi -&gt; ../init.d/open-iscsi<br>   /etc/rc4.d/S20open-iscsi -&gt; ../init.d/open-iscsi<br>   /etc/rc5.d/S20open-iscsi -&gt; ../init.d/open-iscsi<br>hua@node1:~$ sudo sysv-rc-conf –list open-iscsi<br>open-iscsi   0:off    1:off    2:on    3:on    4:on    5:on    6:off</p>
<p>vi /etc/iscsi/iscsid.conf<br>node.startup = automatic<br>node.session.auth.authmethod = CHAP<br>node.session.auth.username = username<br>node.session.auth.password = password</p>
<p>hua@node1:~$ sudo iscsiadm -m discovery -t st -p 192.168.99.122<br>192.168.99.122:3260,1 iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13<br>10.8.0.1:3260,1 iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13</p>
<p>hua@node1:~$ sudo cat /etc/iscsi/initiatorname.iscsi<br>InitiatorName=iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13</p>
<p>hua@node1:~$ sudo iscsiadm -m node -T iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13 -p 192.168.99.122 –op new<br>New iSCSI node [tcp:[hw=,ip=,net_if=,iscsi_if=default] 192.168.99.122,3260,-1 iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13] added</p>
<p>hua@node1:~$ sudo iscsiadm -m node -T iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13 -p 192.168.99.122 –login<br>Logging in to [iface: default, target: iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13, portal: 192.168.99.122,3260] (multiple)<br>Login to [iface: default, target: iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13, portal: 192.168.99.122,3260] successful.<br>hua@node1:~$ sudo iscsiadm -m node -T iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13 -p 192.168.99.122 –op update -n node.startup -v automatic</p>
<p>hua@node1:~$ ls -l /dev/disk/by-path/ip-192.168.99.122\:3260-iscsi-iqn.2004-04.com.qnap\:ts-212p\:iscsi.quqiscsi.d5ad13-lun-0<br>lrwxrwxrwx 1 root root 9 Dec 28 10:22 /dev/disk/by-path/ip-192.168.99.122:3260-iscsi-iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13-lun-0 -&gt; ../../sdb</p>
<p>hua@node1:~$ cat /sys/class/iscsi_host/host16/device/session11/iscsi_session/session11/targetname<br>iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13</p>
<p>hua@node1:~$ sudo cat /etc/udev/rules.d/55-openiscsi.rules<br>KERNEL==”sd*”, BUS==”scsi”, PROGRAM=”/etc/udev/scripts/iscsidev.sh %b”,SYMLINK+=”iscsi/%c”</p>
<p>hua@node1:~$ sudo cat /etc/udev/scripts/iscsidev.sh</p>
<p>#!/bin/sh</p>
<h1 id="FILE-etc-udev-scripts-iscsidev-sh"><a href="#FILE-etc-udev-scripts-iscsidev-sh" class="headerlink" title="FILE: /etc/udev/scripts/iscsidev.sh"></a>FILE: /etc/udev/scripts/iscsidev.sh</h1><p>BUS=${1}<br>HOST=${BUS%%:<em>}<br>[ -e /sys/class/iscsi_host ] || exit 1<br>file=”/sys/class/iscsi_host/host${HOST}/device/session</em>/iscsi_session*/targetname”<br>target_name=$(cat ${file})</p>
<h1 id="This-is-not-an-open-scsi-drive"><a href="#This-is-not-an-open-scsi-drive" class="headerlink" title="This is not an open-scsi drive"></a>This is not an open-scsi drive</h1><p>if [ -z “${target_name}” ]; then<br>  exit 1<br>fi<br>echo “${target_name##*.}”</p>
<p>hua@node1:~$ sudo fdisk -l<br>…<br>Disk /dev/sdb: 859.0 GB, 858993459200 bytes<br>255 heads, 63 sectors/track, 104433 cylinders, total 1677721600 sectors<br>Units = sectors of 1 * 512 = 512 bytes<br>Sector size (logical/physical): 512 bytes / 512 bytes<br>I/O size (minimum/optimal): 1048576 bytes / 1048576 bytes<br>Disk identifier: 0x00000000</p>
<p>Disk /dev/sdb doesn’t contain a valid partition table</p>
<p>hua@node1:~$ sudo mkfs.ext4 /dev/sdb<br>mke2fs 1.42.9 (4-Feb-2014)<br>/dev/sdb is entire device, not just one partition!<br>Proceed anyway? (y,n) y<br>Discarding device blocks: done<br>Filesystem label=<br>OS type: Linux<br>Block size=4096 (log=2)<br>Fragment size=4096 (log=2)<br>Stride=256 blocks, Stripe width=256 blocks<br>52428800 inodes, 209715200 blocks<br>10485760 blocks (5.00%) reserved for the super user<br>First data block=0<br>Maximum filesystem blocks=4294967296<br>6400 block groups<br>32768 blocks per group, 32768 fragments per group<br>8192 inodes per group<br>Superblock backups stored on blocks:<br>    32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,<br>    4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968,<br>    102400000</p>
<p>Allocating group tables: done<br>Writing inode tables: done<br>Creating journal (32768 blocks): done<br>Writing superblocks and filesystem accounting information: done</p>
<p>hua@node1:~$ sudo mkdir /qnap<br>hua@node1:~$ sudo mount -t ext4 /dev/sdb /qnap<br>hua@node1:~$ df -h |grep sdb<br>/dev/sdb        788G   69M  748G   1% /qnap</p>
<p>hua@node1:~$ sudo tune2fs -l /dev/sdb |grep UUID<br>Filesystem UUID:          30aa3ef9-698b-41c2-9637-dc79be91eb79</p>
<p>hua@node1:~$ sudo cat /etc/fstab |grep sdb<br>/dev/sdb    /qnap    ext4    defaults,auto,_netdev    0    0</p>
<p>hua@hua-ThinkPad-T440p:/bak$ sudo mount -t ext4 /dev/sdb /qnap<br>mount: /dev/sdb is already mounted or /qnap busy<br>hua@hua-ThinkPad-T440p:/bak$ sudo multipath -ll<br>36001405abdafa5bd00cbd4d1ad9498d1 dm-0 QNAP,iSCSI Storage<br>size=800G features=’0’ hwhandler=’0’ wp=rw<br><code>-+- policy=&#39;round-robin 0&#39; prio=1 status=active</code>- 6:0:0:0 sdb 8:16 active ready running<br>hua@hua-ThinkPad-T440p:/bak$ sudo multipath -F<br>hua@hua-ThinkPad-T440p:/bak$ sudo multipath -ll<br>hua@hua-ThinkPad-T440p:/bak$ sudo mount -t ext4 /dev/sdb /qnap<br>自动挂载<br>可参考： <a href="https://blog.csdn.net/bing_g/article/details/51279427" target="_blank" rel="external">https://blog.csdn.net/bing_g/article/details/51279427</a></p>
<p>写了个脚本如下：</p>
<p>cat /etc/auto.iscsi</p>
<p>#!/bin/bash</p>
<p>IQN=”iqn.2018-05.com.quqi99:sdb_for_t440p”<br>IP=”192.168.99.1”<br>PORT=”3260”<br>LUN=”1”<br>OPTS=”-nocheck,ro,fstype=ext4”</p>
<p>if iscsiadm -m session | grep “$IQN”; then<br>  echo node exists<br>else<br>  iscsiadm -m node -T “$IQN” -p “$IP” –login 1&gt;&amp;2<br>fi</p>
<h1 id="Let-udev-settle-before-we-try-to-look-up-the-block-device"><a href="#Let-udev-settle-before-we-try-to-look-up-the-block-device" class="headerlink" title="Let udev settle before we try to look up the block device."></a>Let udev settle before we try to look up the block device.</h1><p>sleep 1</p>
<h1 id="Figure-out-the-block-device-using-the-udev-symlink"><a href="#Figure-out-the-block-device-using-the-udev-symlink" class="headerlink" title="Figure out the block device using the udev symlink."></a>Figure out the block device using the udev symlink.</h1><p>DEV_LINK=<code>readlink /dev/disk/by-path/ip-${IP}:${PORT}-iscsi-${IQN}-lun-${LUN}</code><br>DEV_NODE=<code>basename $DEV_LINK</code><br>DEV_FULL=”/dev/$DEV_NODE”</p>
<h1 id="Return-the-magic-to-automount"><a href="#Return-the-magic-to-automount" class="headerlink" title="Return the magic to automount"></a>Return the magic to automount</h1><p>echo “${OPTS} :${DEV_FULL}${LUN}”<br>sudo mount -t ext4 ${DEV_FULL} /bak_openwrt/</p>
<p>遇到的问题<br>1, 多出来的10.8.0.1:3260这个Portal连不上会导致开机时超时非常慢，得删除它。</p>
<p>hua@node2:~$ sudo iscsiadm -m discoverydb -P1<br>SENDTARGETS:<br>DiscoveryAddress: 192.168.99.122,3260<br>Target: iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13<br>Portal: 192.168.99.122:3260,1<br>Iface Name: default<br>Portal: 10.8.0.1:3260,1<br>Iface Name: default<br>DiscoveryAddress: aa.quqi.com,3260<br>iSNS:<br>No targets found.<br>STATIC:<br>Target: iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13<br>Portal: aa.quqi.com:3260,-1<br>Iface Name: default<br>FIRMWARE:<br>No targets found.</p>
<p>hua@node2:~$ sudo iscsiadm -m discoverydb -t st -p 10.8.0.1,3260 -o delete<br>iscsiadm: Discovery record [10.8.0.1,3260] not found.<br>hua@node2:~$ sudo iscsiadm -m discoverydb -t st -p 10.8.0.1:3260 -o delete<br>iscsiadm: Discovery record [10.8.0.1,3260] not found</p>
<p>hua@node2:~$ sudo iscsiadm -m discoverydb -P1<br>SENDTARGETS:<br>DiscoveryAddress: 192.168.99.122,3260<br>Target: iqn.2004-04.com.qnap:ts-212p:iscsi.quqiscsi.d5ad13<br>Portal: 192.168.99.122:3260,1<br>Iface Name: default<br>DiscoveryAddress: veryhua2006.myqnapcloud.com,3260<br>iSNS:<br>No targets found.<br>STATIC:<br>No targets found.<br>FIRMWARE:<br>No targets found.</p>
<p>2, 遇到这个类似的问题（<a href="http://forum.open-e.com/showthread.php?1262-Problem-synchronize-data-using-2-initiator-1-target），即多个iscsi" target="_blank" rel="external">http://forum.open-e.com/showthread.php?1262-Problem-synchronize-data-using-2-initiator-1-target），即多个iscsi</a> client连向同一个iscsi server时，在共享存储上写入新内容了，iscsi client需要先umount再mount才能更新。似乎没有自动地sync</p>
<p>20200413更新<br>遇到一个问题，ssh与nfs都可以用，但是访问portal登录时输入用户名和密码后无反应无跳转也不报错。下载固件( <a href="https://www.qnap.com.cn/zh-cn/download?model=ts-212p&amp;category=firmware" target="_blank" rel="external">https://www.qnap.com.cn/zh-cn/download?model=ts-212p&amp;category=firmware</a> ) 使用qfinder pro重刷即可。多等一会，不会损坏nfs数据。</p>
<p>20201108更新 - 群晖上使用iscsi<br>sudo apt install open-iscsi -y<br>sudo iscsiadm -m discoverydb -P1</p>
<h1 id="etc-iscsi-iscsid-conf-for-CHAP-username-password"><a href="#etc-iscsi-iscsid-conf-for-CHAP-username-password" class="headerlink" title="/etc/iscsi/iscsid.conf for CHAP username/password"></a>/etc/iscsi/iscsid.conf for CHAP username/password</h1><p>#sudo bash -c ‘cat &gt; /etc/iscsi/initiatorname.iscsi’ &lt;&lt; EOF<br>cat &lt;&lt; EOF | sudo tee /etc/iscsi/initiatorname.iscsi<br>InitiatorName=iqn.2020-11.com.quqi:catdisk.share<br>EOF</p>
<h1 id="discover-target"><a href="#discover-target" class="headerlink" title="discover target"></a>discover target</h1><p>sudo iscsiadm -m discovery -t sendtargets -p 192.168.2.108</p>
<h1 id="confirm-status-after-discovery"><a href="#confirm-status-after-discovery" class="headerlink" title="confirm status after discovery"></a>confirm status after discovery</h1><p>sudo iscsiadm -m node -o show</p>
<h1 id="login-to-the-target"><a href="#login-to-the-target" class="headerlink" title="login to the target"></a>login to the target</h1><h1 id="sudo-iscsiadm-m-node-–login"><a href="#sudo-iscsiadm-m-node-–login" class="headerlink" title="sudo iscsiadm -m node –login"></a>sudo iscsiadm -m node –login</h1><p>sudo iscsiadm -m node -T iqn.2020-11.com.quqi:catdisk.share -p 192.168.2.108 –login</p>
<h1 id="confirm-the-established-session"><a href="#confirm-the-established-session" class="headerlink" title="confirm the established session"></a>confirm the established session</h1><p>sudo iscsiadm -m session -o show<br>cat /proc/partitions  #or fdisk -l<br>sudo gdisk /dev/sdc  #n,[enter],[enter],[enter],[enter],t,8300,p,w,Y<br>sudo parted /dev/sdc print<br>sudo mkfs.ext4 /dev/sdc1<br>sudo mount /dev/sdc1 /mnt/share/<br>sudo lsblk<br>sudo blkid</p>
<h1 id="use-autofs"><a href="#use-autofs" class="headerlink" title="use autofs"></a>use autofs</h1><p>cat &lt;&lt; EOF | sudo tee -a /etc/auto.master<br>/-      auto.direct<br>EOF<br>cat &lt;&lt; EOF | sudo tee -a /etc/auto.direct</p>
<p>#/cat    -fstype=nfs,rw,rsize=32768,wsize=32768,vers=3,username=admin,password=xxx     192.168.2.108:/volume1/bak</p>
<p>#/nas    -fstype=nfs4,rsize=32768,wsize=32768     192.168.2.103:/Publicj</p>
<h1 id="DEV-NAME-readlink-dev-disk-by-path-ip-192-168-2-108-3260-iscsi-iqn-2020-11-com-quqi-catdisk-share-lun-1-xargs-basename"><a href="#DEV-NAME-readlink-dev-disk-by-path-ip-192-168-2-108-3260-iscsi-iqn-2020-11-com-quqi-catdisk-share-lun-1-xargs-basename" class="headerlink" title="DEV_NAME=$(readlink /dev/disk/by-path/ip-192.168.2.108\:3260-iscsi-iqn.2020-11.com.quqi\:catdisk.share-lun-1 |xargs basename)"></a>DEV_NAME=$(readlink /dev/disk/by-path/ip-192.168.2.108\:3260-iscsi-iqn.2020-11.com.quqi\:catdisk.share-lun-1 |xargs basename)</h1><p>/mnt/share    -fstype=ext4,rw,nosuid,nodev    :/dev/sdc1<br>EOF</p>
<h1 id="or-use-fstab"><a href="#or-use-fstab" class="headerlink" title="or use fstab"></a>or use fstab</h1><p>sudo bash -c ‘cat &gt;&gt;/etc/fstab’ &lt;&lt;EOF<br>/dev/sdc1     /mnt/share   ext4  defaults,auto,_netdev  0  0<br>EOF<br>sudo mount -a</p>
<p>sudo iscsiadm -m node -T iqn.2020-11.com.quqi:catdisk.share -p 192.168.2.108 –op update -n node.startup -v automatic<br>sudo systemctl enable open-iscsi</p>
<p>20201109更新　- 使用samba<br>openwrt上的samba server<br>1, set samba share - <a href="http://192.168.99.1/cgi-bin/luci/admin/nas/samba" target="_blank" rel="external">http://192.168.99.1/cgi-bin/luci/admin/nas/samba</a><br>2, smbpasswd -a root  #root/password<br>3, cat /etc/samba/smb.conf  &amp;&amp; cat /etc/samba/smbpasswd &amp;&amp; ps |grep -E ‘smbd|nmbd’<br>4, root@OpenWrt:~# cat /etc/samba/smb.conf<br>[global]<br>    netbios name = OpenWrt<br>    display charset = UTF-8</p>
<pre><code>#interfaces = lo br-lan
server string = OpenWrt
unix charset = UTF-8
workgroup = WORKGROUP
bind interfaces only = yes
deadtime = 30
enable core files = no
local master = yes
map to guest = Bad User
max protocol = SMB2
min receivefile size = 16384
null passwords = yes
passdb backend = smbpasswd
    # run &apos;smbpasswd root&apos; to set pass for &apos;root&apos;, we also need to comment &apos;invalid users=root&apos;
    #invalid users = root
security = user
    smb passwd file = /etc/samba/smbpasswd
socket options = TCP_NODELAY IPTOS_LOWDELAY
use sendfile = yes
</code></pre><p>[share]<br>    path = /mnt/sdc2<br>    read only = no<br>    guest ok = yes<br>    create mask = 0666<br>    directory mask = 0777<br>    browseable = yes</p>
<p>5, windows client: \192.168.2.47\share, 开机自动挂载可: 计算机-&gt;映射网络驱动器<br>6, Linux client<br>smbclient -L //192.168.2.47 -U root%password<br>smbclient //192.168.2.47/share -U root%password -c “ls”<br>sudo apt install cifs-utils -y<br>dmesg  #To use the less secure SMB1 dialect to access old servers which do not support SMB3 (or SMB2.1) specify vers=1.0 on mount<br>sudo mount -t cifs -o username=root,password=password,vers=1.0 //192.168.2.47/share /mnt/smb<br>autofs: /mnt/smb    -fstype=cifs,rw,username=root,password=password,vers=1.0,file_mode=0777,dir_mode=0777 ://192.168.2.47/share</p>
<p>目前规划<br>QNAP NAS(2T)的nfs仍通过autofs作工作之用，且做为照片主备分入口<br>OpenWrt上的4T硬盘，划分出/dev/sdc1(2T)定期通过crontab用rsync从QNAP同步备份数据.<br>OpenWrt上的/dev/sdc2(1T)做SMB在win与android电视之间共享孩子要看的视频<br>OpenWRT上的tgt iscsi server不work, 其余sdc3(500G)与sdc4(226G)备用<br>CATDISK(1T)上刷了群晖，做了两块iSCSI, iqn.2020-11.com.quqi:catdisk.share(500G)给Linux用，iqn.2020-10.com.quqi:catdisk.win(200G)给win10用<br>CATDIST(100G)上还有一个windisk nfs (\192.168.2.108\volume1\windisk)<br>CATDIST也通过ssh备份一些极其重要的小数据如毕业证</p>
<p>/nas    -fstype=nfs4,rsize=32768,wsize=32768     192.168.2.103:/Public</p>
<h1 id="DEV-NAME-readlink-dev-disk-by-path-ip-192-168-2-108-3260-iscsi-iqn-2020-11-com-quqi-catdisk-share-lun-1-xargs-basename-1"><a href="#DEV-NAME-readlink-dev-disk-by-path-ip-192-168-2-108-3260-iscsi-iqn-2020-11-com-quqi-catdisk-share-lun-1-xargs-basename-1" class="headerlink" title="DEV_NAME=$(readlink /dev/disk/by-path/ip-192.168.2.108\:3260-iscsi-iqn.2020-11.com.quqi\:catdisk.share-lun-1 |xargs basename)"></a>DEV_NAME=$(readlink /dev/disk/by-path/ip-192.168.2.108\:3260-iscsi-iqn.2020-11.com.quqi\:catdisk.share-lun-1 |xargs basename)</h1><p>/mnt/nasbak    -fstype=ext4,rw,nosuid,nodev    :/dev/sdc1<br>/mnt/smbwin    -fstype=cifs,rw,username=root,password=password,vers=1.0,file_mode=0777,dir_mode=0777 ://192.168.2.47/share<br>/nfswin    -fstype=nfs,rw,rsize=32768,wsize=32768,vers=3,username=quqi99,password=xxx     192.168.2.108:/volume1/windisk</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/04/使用rsync同步数据/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/04/使用rsync同步数据/" itemprop="url">使用rsync同步数据</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-04T21:15:27+08:00">
                2020-11-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华  发表于：2015-12-28<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )<br>急需使用rsync在家里的多台机器上同步相片。</p>
<p>sudo apt-get  install  rsync<br>sudo update-rc.d -f rsync remove<br>sudo update-rc.d rsync start 20 2 3 4 5 . stop 20 0 1 6 .<br>sudo update-rc.d rsync enable<br>hua@node1:~$ sudo sysv-rc-conf –list rsync<br>rsync        0:off    1:off    2:on    3:on    4:on    5:on    6:off</p>
<p>这时候就可以在一台机器上同步目录了(rsync server运行在qnap上，统一在qnap上修改，rsync client定时运行下列命令即可让客户端的文件夹与服务端同步，以服务端为准）:</p>
<p>rsync -avzur –progress –delete /bak/tmp/qnap/ /qnap/<br>rsync -avzur –progress –delete –password-file=/etc/rsync.secret  /bak/tmp/qnap/ /qnap/</p>
<p>在多台机器上同步目录：</p>
<p>rsync -rave “ssh -p 22 -l hua” -avzur –progress –delete 192.168.99.124:/qnap/ /qnap/<br>等价于：rsync -avzur –progress –delete hua@192.168.99.124:/qnap/ /qnap/</p>
<p>也可以配置使用::qnap使用下列配置文件/etc/rsyncd.conf中的[qnap]模块：</p>
<p>sudo rsync -avrzP hua@192.168.99.124::qnap qnap</p>
<p>hua@node1:~$ sudo rsync –list-only hua@192.168.99.124::<br>++++++++++++++++++++++++++++++++++++++++++++++<br>Welcome to use quqi rsync services!<br>++++++++++++++++++++++++++++++++++++++++++++++</p>
<p>qnap               This is qnap backup data</p>
<p>配置[qnap]模块的步骤如下：</p>
<p>sudo touch /etc/rsyncd.conf<br>sudo touch /etc/rsyncd.motd<br>hua@node1:~$ sudo cat /etc/rsyncd.motd<br>++++++++++++++++++++++++++++++++++++++++++++++<br>Welcome to use quqi rsync services!<br>++++++++++++++++++++++++++++++++++++++++++++++<br>sudo touch /etc/rsyncd.secrets<br>hua@node1:~$ sudo cat /etc/rsyncd.secrets<br>hua:Passw0rd<br>sudo chmod 600 /etc/rsyncd.secrets<br>sudo chown root:root /etc/rsyncd.secrets<br>hua@node1:~$ sudo cat /etc/default/rsync |grep ‘RSYNC_ENABLE’<br>RSYNC_ENABLE=true</p>
<p>sudo /etc/init.d/rsync restart<br>sudo iptables -A INPUT -p tcp -m state –state NEW  -m tcp –dport 873 -j ACCEPT<br>vi /etc/rsyncd.conf<br>pid file = /var/run/rsyncd.pid<br>port = 873<br>address = 192.168.99.124</p>
<p>#usermod -g root hua<br>uid = hua<br>gid = root<br>use chroot = yes<br>read only = yes<br>hosts allow=192.168.99.0/255.255.255.0 10.0.1.0/255.255.255.0<br>hosts deny=*<br>max connections = 5<br>motd file = /etc/rsyncd.motd<br>log file = /var/log/rsync.log</p>
<p>#transfer logging = yes<br>log format = %t %a %m %f %b<br>syslog facility = local3<br>timeout = 300</p>
<p>[qnap]<br>path = /qnap<br>list=yes             # 可以使用rsync –list-only hua@192.168.99.124::命令列出目录<br>ignore errors<br>auth users = hua,root<br>secrets file = /etc/rsyncd.secrets<br>comment = This is qnap backup data<br>exclude = tmp/  test/</p>
<p>最后，我实际上是这样处理的，我有一个qnap，一个台式机，一个笔记本，对于一些相片啥的想多存储几份别一个机器哪天坏了丢了。</p>
<p>1, 由于iscsi上一个bug，一个client对qnap上的iscsi server写了之后，没法实时更新在另一个client上（必须先umount再mount一下才行）， 并且qnap的iscsi采用一个大的虚拟文件存储的，这都不是我想要的。所以最后只使用了qnap上的nfs将相片存储了一份。</p>
<p>2, 台式机因为IP固定开机自动mount (sudo mount -t nfs -o vers=3 192.168.99.122:/Public /bak/qnap), 另外直接复制了一份到/bak/qnap_local目录防止rsync操作失误毁坏数据。</p>
<p>3, 笔记本因为经常外出IP不固定，外出时使用/bak/qnap_local目录的内容，在家需要同步时手工同步：</p>
<p>   sudo mount -t nfs -o vers=3 192.168.99.122:/Public /bak/qnap<br>   cd ~ &amp;&amp; rsync -avzurP –exclude ‘doc’ –exclude ‘photo’ –exclude ‘media’  –progress –delete /bak/qnap/ /bak/qnap_local</p>
<p>4, 平时在家办公统一从台式机上写/bak/qnap目录将数据直接写到qnap上。手机等移动设备通过qnap ftp访问数据。</p>
<p>20171031更新：</p>
<p>最后的方案是：</p>
<p>1， 使用autos先将nas上的nfs共享目录共享到台式机（注： nfs共离目录无法使用inotify)：</p>
<p>hua@node1:~$ grep -r ‘auto.direct’ /etc/auto.master<br>/-      auto.direct        –timeout 60<br>hua@node1:~$ cat /etc/auto.direct<br>/nas    -fstype=nfs4,rsize=32768,wsize=32768     192.168.99.122:/Public</p>
<p>2, 然后直接手动运行下列两个命令：</p>
<p>rsync -avztur –progress –delete  /nas/doc/  /bak/doc<br>rsync -avztur –progress –delete  /nas/photo/  /bak/photo</p>
<p>20201104更新 - nas往openwrt同步数据</p>
<p>rsync can work in two different mode, rsync over rsync (873, rsyncd) and rsync over ssh, see <a href="https://serverfault.com/questions/827633/confused-about-rsync-port-873-and-nas" target="_blank" rel="external">https://serverfault.com/questions/827633/confused-about-rsync-port-873-and-nas</a><br>we use rsync over ssh mode here.</p>
<h1 id="openwrt-192-168-2-47-192-168-99-1"><a href="#openwrt-192-168-2-47-192-168-99-1" class="headerlink" title="openwrt (192.168.2.47, 192.168.99.1)"></a>openwrt (192.168.2.47, 192.168.99.1)</h1><p>opkg install rsync</p>
<h1 id="nas-192-168-2-103-push-data-into-openwrt-192-168-2-47"><a href="#nas-192-168-2-103-push-data-into-openwrt-192-168-2-47" class="headerlink" title="nas (192.168.2.103) push data into openwrt(192.168.2.47)"></a>nas (192.168.2.103) push data into openwrt(192.168.2.47)</h1><h1 id="don’t-use-rsync"><a href="#don’t-use-rsync" class="headerlink" title="don’t use rsync"></a>don’t use rsync</h1><p>cd /share/HDA_DATA/Public/test &amp;&amp; touch test1 &amp;&amp; touch test2 &amp;&amp; touch test3<br>ssh root@192.168.2.47 “tee -a /etc/dropbear/authorized_keys” &lt; ~/.ssh/id_rsa.pub<br>rsync -avztur –progress –delete –exclude ‘test/test1’ –exclude ‘test/test2’ /share/HDA_DATA/Public/test root@192.168.2.47:/mnt/sdb1/<br>rsync -avztur –progress –delete –exclude ‘test1’ –exclude ‘test2’ /share/HDA_DATA/Public/test/ root@192.168.2.47:/mnt/sdb1/<br>rsync -avztur –progress –delete –exclude ‘windisk’ –exclude ‘media’ /share/HDA_DATA/Public/ root@192.168.2.47:/mnt/sdb1/<br>contab -e<br>0 1 <em> </em> * /usr/bin/rsync -avztur –progress –delete –exclude ‘windisk’ –exclude ‘media’ /share/HDA_DATA/Public/ root@192.168.2.47:/mnt/sdb1/</p>
<p>或者在openwrt端做:</p>
<p>mkdir ~/.ssh<br>dropbearkey -t rsa -f ~/.ssh/id_rsa<br>dropbearkey -y -f ~/.ssh/id_rsa |sed -n 2p &gt; ~/.ssh/id_rsa.pub</p>
<h1 id="https-community-onion-io-topic-2538-resolved-ssh-from-omega-to-linux-server-without-password-9"><a href="#https-community-onion-io-topic-2538-resolved-ssh-from-omega-to-linux-server-without-password-9" class="headerlink" title="https://community.onion.io/topic/2538/resolved-ssh-from-omega-to-linux-server-without-password/9"></a><a href="https://community.onion.io/topic/2538/resolved-ssh-from-omega-to-linux-server-without-password/9" target="_blank" rel="external">https://community.onion.io/topic/2538/resolved-ssh-from-omega-to-linux-server-without-password/9</a></h1><p>ln -s ~/.ssh/id_rsa ~/.ssh/id_dropbear<br>chmod 700 ~/.ssh<br>touch ~/.ssh/authorized_keys &amp;&amp; chmod 644 ~/.ssh/authorized_keys<br>ssh admin@192.168.2.103 “tee -a /root/.ssh/authorized_keys” &lt; ~/.ssh/id_rsa.pub<br>/usr/bin/rsync -avztur –progress –delete –exclude ‘windisk’ –exclude ‘media’ admin@192.168.2.103:/share/HDA_DATA/Public/ /mnt/sdb1/</p>
<p>但这无论从nas还是从openwrt运行rsync都由于nas太慢大概只有4M左右，可能提升速度最好的办法是先将nas通过nfs映射到openwrt然后再拷．这样速度能达到30M</p>
<p>opkg install nfs-utils<br>showmount -e 192.168.2.103<br>/usr/bin/mount -t nfs 192.168.2.103:Public /mnt/nas -o proto=tcp -o nolock -o vers=4<br>/usr/bin/rsync -avztur –progress –delete –exclude ‘windisk’ –exclude ‘media’ /mnt/nas/ /mnt/sdc1/<br>但是我担心将mount放到fstab时当nas有问题时也影响openwrt的启动，所以我将上面命令放在rc.local中.<br>root@OpenWrt:~# cat /etc/init.d/done</p>
<p>#!/bin/sh /etc/rc.common</p>
<h1 id="Copyright-C-2006-OpenWrt-org"><a href="#Copyright-C-2006-OpenWrt-org" class="headerlink" title="Copyright (C) 2006 OpenWrt.org"></a>Copyright (C) 2006 OpenWrt.org</h1><p>START=95<br>boot() {<br>    mount_root done<br>    rm -f /sysupgrade.tgz &amp;&amp; sync</p>
<pre><code># process user commands
[ -f /etc/rc.local ] &amp;&amp; {
    sh /etc/rc.local
}

# set leds to normal state
. /etc/diag.sh
set_state done
</code></pre><p>}<br>并且crontab也在openwrt这边做．这样速度达到了30M</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/04/Install-OpenStack-on-MAAS-Nodes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/04/Install-OpenStack-on-MAAS-Nodes/" itemprop="url">Install OpenStack on MAAS Nodes</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-04T16:23:01+08:00">
                2020-11-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>作者：张华 发表于：2016-11-09<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )</em></p>
<p>1, Prepare MAAS Nodes</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install -y maas-cli</span><br><span class="line">echo &apos;&lt;key-in-&lt;MAASIP&gt;/MAAS/r/account/prefs/api-keys&gt;&apos; &gt; ~/maas-apikey</span><br><span class="line">maas login admin http://10.230.56.2/MAAS `cat ~/maas-apikey`</span><br><span class="line">maas admin tags read</span><br><span class="line"></span><br><span class="line"># https://juju.is/docs/maas-cloud</span><br><span class="line">sudo bash -c &apos;cat &gt; /tmp/mymmas.yaml&apos; &lt;&lt; EOF</span><br><span class="line">clouds:</span><br><span class="line">   mymmas:</span><br><span class="line">      type: maas</span><br><span class="line">      auth-types: [oauth1]</span><br><span class="line">      endpoint: http://&lt;MAASIP&gt;/MAAS</span><br><span class="line">EOF</span><br><span class="line">juju remove-cloud --local mymmas</span><br><span class="line">juju add-cloud --local mymmas /tmp/mymmas.yaml</span><br><span class="line">juju list-clouds</span><br><span class="line">juju show-cloud mymmas --local</span><br><span class="line">sudo bash -c &apos;cat &gt; /tmp/credential.yaml&apos; &lt;&lt; EOF</span><br><span class="line">credentials:</span><br><span class="line">  mymmas:</span><br><span class="line">    zhhuabj:</span><br><span class="line">      auth-type: oauth1</span><br><span class="line">      maas-oauth: &lt;key-in-&lt;MAASIP&gt;/MAAS/r/account/prefs/api-keys&gt;</span><br><span class="line">EOF</span><br><span class="line">juju remove-credential --local mymmas zhhuabj</span><br><span class="line">juju add-credential --local mymmas -f /tmp/credential.yaml</span><br><span class="line">juju credentials --local</span><br><span class="line">juju show-credential --local mymmas zhhuabj</span><br><span class="line">cat ~/.local/share/juju/credentials.yaml</span><br><span class="line"></span><br><span class="line">juju bootstrap --debug segmaas --no-gui --config image-stream=daily --config default-series=focal --constraints=&quot;tags=virtual&quot;</span><br><span class="line"># hit this error https://discourse.maas.io/t/bootstrap-instance-started-but-did-not-change-to-deployed-state-instance-8epqeq-failed-to-deploy/2237/3</span><br><span class="line"># workaround for above issue,</span><br><span class="line"># first change rootfs to use lvm for the virutal host z-rotomvm14, then add a tag &apos;z-rotomvm14&apos; for it as well</span><br><span class="line"># finnally use the following two tags to try this host again.</span><br><span class="line">juju bootstrap --debug segmaas --no-gui --config image-stream=daily --config default-series=focal --constraints=&quot;tags=virtual,z-rotomvm14&quot;</span><br><span class="line">this is the bug - https://bugs.launchpad.net/curtin/+bug/1876258</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line">series:                     &amp;series                    bionic</span><br><span class="line">debug:                      &amp;debug                     True</span><br><span class="line">verbose:                    &amp;verbose                   True</span><br><span class="line">openstack_origin:           &amp;openstack_origin          cloud:bionic-ussuri</span><br><span class="line">source:                     &amp;source                    cloud:bionic-ussuri</span><br><span class="line"></span><br><span class="line">machines:</span><br><span class="line">  &apos;0&apos;:</span><br><span class="line">    constraints: &quot;tags=shuckle&quot;</span><br><span class="line">    series: *series</span><br><span class="line">  &apos;1&apos;:</span><br><span class="line">    constraints: &quot;tags=virtual&quot;</span><br><span class="line">    series: *series</span><br><span class="line"></span><br><span class="line">series: *series</span><br><span class="line">applications:</span><br><span class="line">  ntp:</span><br><span class="line">    charm: cs:ntp</span><br><span class="line">    num_units: 0</span><br><span class="line">  mysql:</span><br><span class="line">    charm: cs:~openstack-charmers-next/percona-cluster</span><br><span class="line">    num_units: 1</span><br><span class="line">    bindings:</span><br><span class="line">        &quot;&quot;: oam</span><br><span class="line">    options:</span><br><span class="line">      source: *source</span><br><span class="line">      root-password: ChangeMe123</span><br><span class="line">      sst-password: ChangeMe123</span><br><span class="line">      innodb-buffer-pool-size: 2G</span><br><span class="line">      max-connections: 2000</span><br><span class="line">    to:</span><br><span class="line">      - lxd:0</span><br><span class="line">  rabbitmq-server:</span><br><span class="line">    charm: cs:~openstack-charmers-next/rabbitmq-server</span><br><span class="line">    num_units: 1</span><br><span class="line">    bindings:</span><br><span class="line">        &quot;&quot;: oam</span><br><span class="line">    options:</span><br><span class="line">      source: *source</span><br><span class="line">    to:</span><br><span class="line">      - lxd:0</span><br><span class="line">  cinder:</span><br><span class="line">    num_units: 1</span><br><span class="line">    charm: cs:~openstack-charmers-next/cinder-428</span><br><span class="line">    bindings:</span><br><span class="line">        &quot;&quot;: oam</span><br><span class="line">    options:</span><br><span class="line">      debug: *debug</span><br><span class="line">      verbose: *verbose</span><br><span class="line">      block-device: /dev/sdb</span><br><span class="line">      overwrite: &quot;true&quot;</span><br><span class="line">      glance-api-version: 2</span><br><span class="line">      openstack-origin: *openstack_origin</span><br><span class="line">    to:</span><br><span class="line">      - lxd:0</span><br><span class="line">  glance:</span><br><span class="line">    charm: cs:~openstack-charmers-next/glance</span><br><span class="line">    num_units: 1</span><br><span class="line">    bindings:</span><br><span class="line">        &quot;&quot;: oam</span><br><span class="line">    options:</span><br><span class="line">      debug: *debug</span><br><span class="line">      verbose: *verbose</span><br><span class="line">      openstack-origin: *openstack_origin</span><br><span class="line">    to:</span><br><span class="line">      - lxd:0</span><br><span class="line">  keystone:</span><br><span class="line">    charm: cs:~openstack-charmers-next/keystone</span><br><span class="line">    num_units: 1</span><br><span class="line">    bindings:</span><br><span class="line">        &quot;&quot;: oam</span><br><span class="line">    options:</span><br><span class="line">      debug: *debug</span><br><span class="line">      verbose: *verbose</span><br><span class="line">      openstack-origin: *openstack_origin</span><br><span class="line">      admin-password: openstack</span><br><span class="line">    to:</span><br><span class="line">      - lxd:0</span><br><span class="line">  nova-cloud-controller:</span><br><span class="line">    charm: cs:~openstack-charmers-next/nova-cloud-controller</span><br><span class="line">    num_units: 1</span><br><span class="line">    bindings:</span><br><span class="line">        &quot;&quot;: oam</span><br><span class="line">    options:</span><br><span class="line">      debug: *debug</span><br><span class="line">      verbose: *verbose</span><br><span class="line">      openstack-origin: *openstack_origin</span><br><span class="line">      network-manager: Neutron</span><br><span class="line">    to:</span><br><span class="line">      - lxd:0</span><br><span class="line">  nova-compute:</span><br><span class="line">    charm: cs:~openstack-charmers-next/nova-compute</span><br><span class="line">    num_units: 1</span><br><span class="line">    bindings:</span><br><span class="line">        &quot;&quot;: oam</span><br><span class="line">    options:</span><br><span class="line">      debug: *debug</span><br><span class="line">      verbose: *verbose</span><br><span class="line">      openstack-origin: *openstack_origin</span><br><span class="line">      enable-resize: True</span><br><span class="line">      enable-live-migration: True</span><br><span class="line">      migration-auth-type: ssh</span><br><span class="line">    to:</span><br><span class="line">      - 0</span><br><span class="line">  neutron-openvswitch:</span><br><span class="line">    charm: cs:~openstack-charmers-next/neutron-openvswitch</span><br><span class="line">    num_units: 0</span><br><span class="line">    options:</span><br><span class="line">      debug: *debug</span><br><span class="line">      verbose: *verbose</span><br><span class="line">      prevent-arp-spoofing: false</span><br><span class="line">      flat-network-providers: physnet1</span><br><span class="line">  neutron-api:</span><br><span class="line">    charm: cs:~openstack-charmers-next/neutron-api</span><br><span class="line">    num_units: 1</span><br><span class="line">    bindings:</span><br><span class="line">        &quot;&quot;: oam</span><br><span class="line">    options:</span><br><span class="line">      debug: *debug</span><br><span class="line">      verbose: *verbose</span><br><span class="line">      openstack-origin: *openstack_origin</span><br><span class="line">      vlan-ranges: &apos;&apos;  # prevent creating defaults since we don&apos;t need them</span><br><span class="line">      neutron-security-groups: True</span><br><span class="line">      enable-ml2-port-security: False</span><br><span class="line">      flat-network-providers: &apos;physnet1&apos;</span><br><span class="line">    to:</span><br><span class="line">      - lxd:0</span><br><span class="line">  neutron-gateway:</span><br><span class="line">    charm: cs:~openstack-charmers-next/neutron-gateway</span><br><span class="line">    num_units: 1</span><br><span class="line">    options:</span><br><span class="line">      debug: *debug</span><br><span class="line">      verbose: *verbose</span><br><span class="line">      openstack-origin: *openstack_origin</span><br><span class="line">      bridge-mappings: physnet1:br-data</span><br><span class="line">      worker-multiplier: 0.5</span><br><span class="line">      dns-servers: 10.230.56.2</span><br><span class="line">    to:</span><br><span class="line">      - 1</span><br><span class="line">  placement:</span><br><span class="line">    charm: cs:~openstack-charmers-next/placement-36</span><br><span class="line">    num_units: 1</span><br><span class="line">    bindings:</span><br><span class="line">        &quot;&quot;: oam</span><br><span class="line">    options:</span><br><span class="line">      worker-multiplier: 0.5</span><br><span class="line">      openstack-origin: *openstack_origin</span><br><span class="line">    to:</span><br><span class="line">    - lxd:0</span><br><span class="line">relations:</span><br><span class="line">  - [ keystone, mysql ]</span><br><span class="line">  - [ &quot;nova-cloud-controller:shared-db&quot;, &quot;mysql:shared-db&quot; ]</span><br><span class="line">  - [ &quot;nova-cloud-controller:amqp&quot;, &quot;rabbitmq-server:amqp&quot; ]</span><br><span class="line">  - [ nova-cloud-controller, glance ]</span><br><span class="line">  - [ nova-cloud-controller, keystone ]</span><br><span class="line">  - [ nova-compute, nova-cloud-controller ]</span><br><span class="line">  - [ nova-compute, &apos;rabbitmq-server:amqp&apos; ]</span><br><span class="line">  - [ nova-compute, glance ]</span><br><span class="line">  - [ glance, mysql ]</span><br><span class="line">  - [ glance, keystone ]</span><br><span class="line">  - [ glance, &quot;cinder:image-service&quot; ]</span><br><span class="line">  - [ glance, rabbitmq-server ]</span><br><span class="line">  - [ cinder, mysql ]</span><br><span class="line">  - [ cinder, rabbitmq-server ]</span><br><span class="line">  - [ cinder, nova-cloud-controller ]</span><br><span class="line">  - [ cinder, keystone ]</span><br><span class="line">  - [ ntp, nova-compute ]</span><br><span class="line">  - [ neutron-api, mysql ]</span><br><span class="line">  - [ neutron-api, rabbitmq-server ]</span><br><span class="line">  - [ neutron-api, nova-cloud-controller ]</span><br><span class="line">  - [ neutron-api, neutron-openvswitch ]</span><br><span class="line">  - [ neutron-api, keystone ]</span><br><span class="line">  - [ neutron-openvswitch, nova-compute ]</span><br><span class="line">  - [ neutron-openvswitch, rabbitmq-server ]</span><br><span class="line">  - [ neutron-gateway, neutron-api ]</span><br><span class="line">  - [ neutron-gateway, nova-cloud-controller ]</span><br><span class="line">  - [ &quot;rabbitmq-server:amqp&quot;, &quot;neutron-gateway:amqp&quot; ]</span><br><span class="line">  - [ placement, mysql ]</span><br><span class="line">  - [ placement, keystone ]</span><br><span class="line">  - [ placement, nova-cloud-controller ]</span><br></pre></td></tr></table></figure>
<p>2, Upload the image</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://api.jujucharms.com/charmstore/v5/openstack-base/archive</span><br><span class="line">source novarc</span><br><span class="line">wget http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-1511.qcow2</span><br><span class="line">glance image-create --name centos --disk-format qcow2 --container-format bare --file ./CentOS-7-x86_64-GenericCloud-1511.qcow2 --progress</span><br></pre></td></tr></table></figure>
<p>3, Create the network<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># NOTE: can use seg-bundles instead</span><br><span class="line">neutron net-create private --provider:network_type gre --provider:segmentation_id 1012</span><br><span class="line">neutron subnet-create --allocation-pool start=192.168.21.22,end=192.168.21.122 --gateway 192.168.21.1 private 192.168.21.0/24 --enable_dhcp=True --name private_subnet</span><br><span class="line"></span><br><span class="line">neutron net-create ext_net -- --router:external=True --provider:network_type flat --provider:physical_network physnet1</span><br><span class="line">neutron subnet-create --allocation-pool start=10.230.56.100,end=10.230.56.104 --gateway 10.230.56.1 ext_net 10.230.56.100/21 --enable_dhcp=False --name ext_net_subnet</span><br><span class="line"></span><br><span class="line">neutron router-create provider-router</span><br><span class="line">EXT_NET_ID=$(neutron net-list |grep &apos; ext_net &apos; |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">ROUTER_ID=$(neutron router-list |grep &apos; provider-router &apos; |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">SUBNET_ID=$(neutron subnet-list |grep &apos;192.168.21.0/24&apos; |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">neutron router-interface-add $ROUTER_ID $SUBNET_ID</span><br><span class="line">neutron router-gateway-set $ROUTER_ID $EXT_NET_ID</span><br><span class="line">#neutron router-gateway-clear provider-router</span><br><span class="line">#neutron router-interface-delete provider-router private_subnet</span><br><span class="line">#nova floating-ip-delete 10.230.56.101</span><br><span class="line">#neutron subnet-delete ext_net_subnet</span><br><span class="line">#neutron subnet-delete private_subnet</span><br><span class="line">#neutron net-delete ext_net</span><br><span class="line">#neutron net-delete private</span><br><span class="line">#neutron router-delete provider-router</span><br></pre></td></tr></table></figure></p>
<p>4, Boot the VM<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">neutron security-group-rule-create --protocol icmp --direction ingress default</span><br><span class="line">neutron security-group-rule-create --protocol tcp --port-range-min 22 --port-range-max 22 --direction ingress default</span><br><span class="line">nova keypair-add --pub-key ~/.ssh/id_rsa.pub mykey</span><br><span class="line">nova service-list</span><br><span class="line">nova hypervisor-list</span><br><span class="line">nova boot --poll --key-name mykey --image centos --flavor 2 --nic net-id=$(neutron net-list |grep &apos;private&apos; |awk &apos;&#123;print $2&#125;&apos;) --availability-zone nova:node2 i2</span><br><span class="line">nova floating-ip-create</span><br><span class="line">nova floating-ip-associate i2 10.230.56.104</span><br><span class="line">ssh -i mykey centos@10.230.56.104 -v #dd if=/dev/urandom of=/var/tmp/live_mig_test bs=4M count=1000  #~/.local/share/juju/ssh/juju_id_rsa</span><br><span class="line">nova live-migration --block-migrate i2 voltorb</span><br><span class="line">#sudo virsh --connect qemu+ssh://node2/system list</span><br><span class="line">#LIBVIRT_DEBUG=1 virsh migrate --verbose --live --copy-storage-all instance-0000000d qemu+ssh://voltorb/system 2&gt;&amp;1 &gt; debug.log</span><br></pre></td></tr></table></figure></p>
<h2 id="Appendix-sriov-yaml"><a href="#Appendix-sriov-yaml" class="headerlink" title="Appendix - sriov yaml"></a>Appendix - sriov yaml</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line">juju export-bundle --filename /tmp/sriov.yaml   #seg-bundles</span><br><span class="line">series: bionic</span><br><span class="line">applications:</span><br><span class="line">  glance:</span><br><span class="line">    charm: cs:~openstack-charmers-next/glance-423</span><br><span class="line">    num_units: 1</span><br><span class="line">    to:</span><br><span class="line">    - lxd:0</span><br><span class="line">    options:</span><br><span class="line">      debug: true</span><br><span class="line">      verbose: true</span><br><span class="line">      worker-multiplier: 0.25</span><br><span class="line">  keystone:</span><br><span class="line">    charm: cs:~openstack-charmers-next/keystone-508</span><br><span class="line">    num_units: 1</span><br><span class="line">    to:</span><br><span class="line">    - lxd:0</span><br><span class="line">    options:</span><br><span class="line">      admin-password: openstack</span><br><span class="line">      debug: true</span><br><span class="line">      verbose: true</span><br><span class="line">      worker-multiplier: 0.25</span><br><span class="line">  mysql:</span><br><span class="line">    charm: cs:~openstack-charmers-next/percona-cluster-376</span><br><span class="line">    num_units: 1</span><br><span class="line">    to:</span><br><span class="line">    - lxd:0</span><br><span class="line">    options:</span><br><span class="line">      innodb-buffer-pool-size: 512M</span><br><span class="line">      max-connections: 1000</span><br><span class="line">      root-password: admin</span><br><span class="line">      sst-password: admin</span><br><span class="line">  neutron-api:</span><br><span class="line">    charm: local:bionic/neutron-api-0</span><br><span class="line">    num_units: 1</span><br><span class="line">    to:</span><br><span class="line">    - lxd:0</span><br><span class="line">    options:</span><br><span class="line">      debug: true</span><br><span class="line">      enable-sriov: true</span><br><span class="line">      flat-network-providers: physnet1</span><br><span class="line">      neutron-security-groups: true</span><br><span class="line">      supported-pci-vendor-devs: 14e4:16af 8086:1515 8086:1528</span><br><span class="line">      verbose: true</span><br><span class="line">      vlan-ranges: &quot;&quot;</span><br><span class="line">      worker-multiplier: 0.25</span><br><span class="line">  neutron-openvswitch:</span><br><span class="line">    charm: local:bionic/neutron-openvswitch-0</span><br><span class="line">    options:</span><br><span class="line">      debug: true</span><br><span class="line">      enable-local-dhcp-and-metadata: true</span><br><span class="line">      enable-sriov: true</span><br><span class="line">      sriov-device-mappings: physnet1:eno50</span><br><span class="line">      verbose: true</span><br><span class="line">      vlan-ranges: &quot;&quot;</span><br><span class="line">  nova-cloud-controller:</span><br><span class="line">    charm: local:bionic/nova-cloud-controller-501</span><br><span class="line">    num_units: 1</span><br><span class="line">    to:</span><br><span class="line">    - lxd:0</span><br><span class="line">    options:</span><br><span class="line">      debug: true</span><br><span class="line">      network-manager: Neutron</span><br><span class="line">      verbose: true</span><br><span class="line">      worker-multiplier: 0.25</span><br><span class="line">  nova-compute:</span><br><span class="line">    charm: local:bionic/nova-compute-133</span><br><span class="line">    num_units: 1</span><br><span class="line">    to:</span><br><span class="line">    - &quot;1&quot;</span><br><span class="line">    options:</span><br><span class="line">      debug: true</span><br><span class="line">      pci-passthrough-whitelist: &apos;[&#123;&quot;devname&quot;:&quot;eno50&quot;, &quot;physical_network&quot;:&quot;physnet1&quot;&#125;]&apos;</span><br><span class="line">      verbose: true</span><br><span class="line">  ntp:</span><br><span class="line">    charm: cs:ntp-41</span><br><span class="line">  rabbitmq-server:</span><br><span class="line">    charm: cs:~openstack-charmers-next/rabbitmq-server-379</span><br><span class="line">    num_units: 1</span><br><span class="line">    to:</span><br><span class="line">    - lxd:0</span><br><span class="line">machines:</span><br><span class="line">  &quot;0&quot;:</span><br><span class="line">    constraints: tags=buneary</span><br><span class="line">  &quot;1&quot;:</span><br><span class="line">    constraints: tags=duduo</span><br><span class="line">  &quot;2&quot;:</span><br><span class="line">    constraints: tags=virtual</span><br><span class="line">    series: bionic</span><br><span class="line">relations:</span><br><span class="line">- - keystone:shared-db</span><br><span class="line">  - mysql:shared-db</span><br><span class="line">- - nova-cloud-controller:identity-service</span><br><span class="line">  - keystone:identity-service</span><br><span class="line">- - glance:identity-service</span><br><span class="line">  - keystone:identity-service</span><br><span class="line">- - neutron-api:identity-service</span><br><span class="line">  - keystone:identity-service</span><br><span class="line">- - neutron-openvswitch:neutron-plugin-api</span><br><span class="line">  - neutron-api:neutron-plugin-api</span><br><span class="line">- - neutron-api:shared-db</span><br><span class="line">  - mysql:shared-db</span><br><span class="line">- - neutron-api:amqp</span><br><span class="line">  - rabbitmq-server:amqp</span><br><span class="line">- - glance:shared-db</span><br><span class="line">  - mysql:shared-db</span><br><span class="line">- - glance:amqp</span><br><span class="line">  - rabbitmq-server:amqp</span><br><span class="line">- - nova-cloud-controller:image-service</span><br><span class="line">  - glance:image-service</span><br><span class="line">- - nova-cloud-controller:amqp</span><br><span class="line">  - rabbitmq-server:amqp</span><br><span class="line">- - neutron-openvswitch:amqp</span><br><span class="line">  - rabbitmq-server:amqp</span><br><span class="line">- - nova-cloud-controller:shared-db</span><br><span class="line">  - mysql:shared-db</span><br><span class="line">- - nova-cloud-controller:neutron-api</span><br><span class="line">  - neutron-api:neutron-api</span><br><span class="line">- - nova-compute:amqp</span><br><span class="line">  - rabbitmq-server:amqp</span><br><span class="line">- - nova-compute:image-service</span><br><span class="line">  - glance:image-service</span><br><span class="line">- - nova-cloud-controller:cloud-compute</span><br><span class="line">  - nova-compute:cloud-compute</span><br><span class="line">- - nova-compute:neutron-plugin</span><br><span class="line">  - neutron-openvswitch:neutron-plugin</span><br><span class="line">- - ntp:juju-info</span><br><span class="line">  - nova-compute:juju-info</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">张华</p>
              <p class="site-description motion-element" itemprop="description">blog.csdn.net/quqi99</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">100</span>
                  <span class="site-state-item-name">Artikel</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张华</span>

  
</div>


  <div class="powered-by">Erstellt mit  <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>

<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="blog.csdn.net/quqi99">
<meta property="og:type" content="website">
<meta property="og:title" content="技术并艺术着">
<meta property="og:url" content="http://yoursite.com/page/7/index.html">
<meta property="og:site_name" content="技术并艺术着">
<meta property="og:description" content="blog.csdn.net/quqi99">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="技术并艺术着">
<meta name="twitter:description" content="blog.csdn.net/quqi99">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/7/"/>





  <title>技术并艺术着</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">技术并艺术着</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">张华的技术博客 - blog.csdn.net/quqi99</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/18/Set-ip-IPv6-env/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/18/Set-ip-IPv6-env/" itemprop="url">Set ip IPv6 env</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-18T13:44:20+08:00">
                2019-01-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="基础-ND协议的三个位"><a href="#基础-ND协议的三个位" class="headerlink" title="基础 - ND协议的三个位"></a>基础 - ND协议的三个位</h2><p>ND协议包中有三个位(Auto, Managed, Other)：</p>
<ul>
<li>M bit (Managed Address Configuration), M bit如果是1,表示Clients要另外再去跟DHCPv6要IPv6 Prefix</li>
<li>O bit (Other Configuration), O bit如果是1,表示Clients要去跟DHCPv6要DNS(RDNSS)等其他信息<br>这样：</li>
<li>slaas, Stateless autoconfiguration, A=1, M=0, O=0, 主机將只得到Router給的 Prefix, 无法取得DNS等资讯, 其他必须自己填写.</li>
<li>dhcpv6-stateful, A=0, M=1, O=1, 所有信息（IPv6 prefix, DNS等)都通过DHCPv6获得,客戶端主要使用UDP port 546, 而服務器端使用 UDP port 547</li>
<li>dhcpv6-stateless,A=1, M=0, O=1, 除了使用RA裡面的Prefix,其他如DNS等等信息会由DHCPv6 取得.<h2 id="基础-Neutron-IPv6"><a href="#基础-Neutron-IPv6" class="headerlink" title="基础 - Neutron IPv6"></a>基础 - Neutron IPv6</h2>Neutron中有两个重要属性来支持IPv6 (ipv6_address_mode 与 ipv6_ra_mode):</li>
<li>ipv6_ra_mode, 如果设置表示由Neutron来使用radvd来模拟软件IPv6路由器, 如果不设置表示使用外部IPv6路由器</li>
<li>ipv6_address_mode, 对应上述ND协议中的三个位(Auto, Managed, Other), 例如: 对于dhcpv6-stateless, 3比特应该是: A=1, M=0, O=1.</li>
</ul>
<p>下面是创建一个使用外部IPv6路由器并使用dhcpv6-stateless的例子:<br>neutron net-create –provider:network_type flat –provider:physical_network physnet1 –router:external=True ext_net<br>neutron subnet-create ext_net –name external-subnet-v6 –ip_version 6 –ipv6_address_mode dhcpv6-stateless –allocation-pool start=2001:db8:0:1::2,end=2001:db8:0:1:ffff:ffff:ffff:ffff 2001:db8:0:1::/64</p>
<h2 id="基础-Ubuntu中手工配置IPv6的注意点"><a href="#基础-Ubuntu中手工配置IPv6的注意点" class="headerlink" title="基础 - Ubuntu中手工配置IPv6的注意点"></a>基础 - Ubuntu中手工配置IPv6的注意点</h2><p>Ubuntu中配置IPv6可以采用network-manager, 也可采用在/etc/network/interface中手工配置, 也可以使用最新的netplan. 这里描述的是采用手工配置的方法.<br>先看一个遇到的实际问题:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">下面配置不work</span><br><span class="line">iface eth0 inet6 auto</span><br><span class="line">   # use SLAAC to get global IPv6 address from the router</span><br><span class="line">   # we may not enable ipv6 forwarding, otherwise SLAAC gets disabled</span><br><span class="line">   up sleep 5</span><br><span class="line">   dhcp 1</span><br><span class="line">   autoconf 1</span><br><span class="line">   accept_ra 2</span><br><span class="line"></span><br><span class="line">下列配置work</span><br><span class="line">iface eth0 inet6 static</span><br><span class="line">address 2001:192:168:99::135</span><br><span class="line">   gateway 2001:192:168:99::1</span><br><span class="line">   netmask 64</span><br><span class="line">且改成network-manager也work, 这是为什么呢?</span><br><span class="line"></span><br><span class="line">测试方法是:</span><br><span class="line">#it will flush link-local address as well</span><br><span class="line">#ip addr flush br-eth0</span><br><span class="line"># avoid the error: can&apos;t get a link-local address</span><br><span class="line">sudo ip link set dev eth0 down</span><br><span class="line">sudo ip link set dev eth0 up</span><br><span class="line">ifdown br-eth0</span><br><span class="line">ifup --force --verbose br-eth0</span><br></pre></td></tr></table></figure></p>
<p>采用”ifup –force –verbose br-eth0”命令看到的错误是”can’t get a link-local address”.<br>为什么static模式与network-manager模式没有这个错误呢? 原来是这两者默认执行了:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 0 &gt; /proc/sys/net/ipv6/conf/$IFACE/disable_ipv6</span><br></pre></td></tr></table></figure></p>
<p>并且之前Linux网桥br-eth0上一直没有IPv6地址的原因也是这个, 且上面”sudo ip link set dev eth0 up”这句也会自动设置disable_ipv6=0, 但不会对br-eth0作同样的设置.<br>所以添加”up echo 0 &gt; /proc/sys/net/ipv6/conf/$IFACE/disable_ipv6”后问题解决, 完整配置如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">root@node1:~# cat /etc/network/interfaces</span><br><span class="line">auto lo</span><br><span class="line">iface lo inet loopback</span><br><span class="line">auto eth0</span><br><span class="line">iface eth0 inet manual</span><br><span class="line">auto br-eth0</span><br><span class="line">iface br-eth0 inet static</span><br><span class="line">    address 192.168.99.124/24</span><br><span class="line">    gateway 192.168.99.1</span><br><span class="line">    bridge_ports eth0</span><br><span class="line">    dns-nameservers 192.168.99.1</span><br><span class="line">    bridge_stp on</span><br><span class="line">    bridge_fd 0</span><br><span class="line">    bridge_maxwait 0</span><br><span class="line">    up echo -n 0 &gt; /sys/devices/virtual/net/$IFACE/bridge/multicast_snooping</span><br><span class="line"># for stateless it&apos;s &apos;inet6 auto&apos;, for stateful it&apos;s &apos;inet6 dhcp&apos;</span><br><span class="line">iface br-eth0 inet6 auto</span><br><span class="line">    #iface eth0 inet6 static</span><br><span class="line">    #address 2001:192:168:99::135</span><br><span class="line">    #gateway 2001:192:168:99::1</span><br><span class="line">    #netmask 64</span><br><span class="line">    # use SLAAC to get global IPv6 address from the router</span><br><span class="line">    # we may not enable ipv6 forwarding, otherwise SLAAC gets disabled</span><br><span class="line">    # sleep 5 is due a bug and &apos;dhcp 1&apos; indicates that info should be obtained from dhcpv6 server for stateless</span><br><span class="line">    up echo 0 &gt; /proc/sys/net/ipv6/conf/$IFACE/disable_ipv6</span><br><span class="line">    up sleep 5</span><br><span class="line">    autoconf 1</span><br><span class="line">    accept_ra 2</span><br><span class="line">    dhcp 1</span><br></pre></td></tr></table></figure></p>
<p>此外, 最好设置accept_ra=2, 因为经常会遇到自动配置的IPv6地址丢失或者不能获取的问题。一般情况是都是启用了IPv6转发功能(sudo sysctl -w net.ipv6.conf.all.forwarding=1)引起的。<br>为了配置IPv6 address和default gateway, client/host都会默认去listen或者solicit RA广播, 并且host作为router时会忽略RA, 这由accept_ra设置:</p>
<ul>
<li>0 Do not accept RouterAdvertisements.</li>
<li>1 Accept Router Advertisements if forwarding is disabled.</li>
<li>2 Overrule forwarding behavior. Accept Router Advertisements  even if forwarding is enabled.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -w net.ipv6.conf.all.forwarding=1</span><br><span class="line">sudo sysctl -w net.ipv6.conf.all.accept_ra=2</span><br><span class="line">sudo sysctl -w net.ipv6.conf.br-lan.disable_ipv6=0</span><br><span class="line">#echo 0 &gt; /proc/sys/net/ipv6/conf/$IFACE/disable_ipv6</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="IPv6中的防火墙"><a href="#IPv6中的防火墙" class="headerlink" title="IPv6中的防火墙"></a>IPv6中的防火墙</h2><p>IPv6 Router端:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># Clear all ip6tables rules</span><br><span class="line">ip6tables -t nat -X</span><br><span class="line">ip6tables -t nat -P PREROUTING ACCEPT</span><br><span class="line">ip6tables -t nat -P POSTROUTING ACCEPT</span><br><span class="line">ip6tables -t nat -P OUTPUT ACCEPT</span><br><span class="line">ip6tables -t mangle -F</span><br><span class="line">ip6tables -t mangle -X</span><br><span class="line">ip6tables -t mangle -P PREROUTING ACCEPT</span><br><span class="line">ip6tables -t mangle -P INPUT ACCEPT</span><br><span class="line">ip6tables -t mangle -P FORWARD ACCEPT</span><br><span class="line">ip6tables -t mangle -P OUTPUT ACCEPT</span><br><span class="line">ip6tables -t mangle -P POSTROUTING ACCEPT</span><br><span class="line">ip6tables -F</span><br><span class="line">ip6tables -X</span><br><span class="line">ip6tables -P FORWARD ACCEPT</span><br><span class="line">ip6tables -P INPUT ACCEPT</span><br><span class="line">ip6tables -P OUTPUT ACCEPT</span><br><span class="line">ip6tables -t raw -F</span><br><span class="line">ip6tables -t raw -X</span><br><span class="line">ip6tables -t raw -P PREROUTING ACCEPT</span><br><span class="line">ip6tables -t raw -P OUTPUT ACCEPT</span><br><span class="line"></span><br><span class="line"># Default DROP rules</span><br><span class="line">ip6tables -P INPUT   DROP</span><br><span class="line">ip6tables -P OUTPUT  ACCEPT</span><br><span class="line">ip6tables -P FORWARD DROP</span><br><span class="line"></span><br><span class="line"># Allow established connections</span><br><span class="line">ip6tables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">ip6tables -A FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"># For IPv6</span><br><span class="line"># it&apos;s not required due to ipv6-icmp</span><br><span class="line"># sudo ip6tables -A INPUT -p udp --dport 547 -j ACCEPT</span><br><span class="line">#ip6tables -A INPUT -p icmpv6 --icmpv6-type echo-request -j ACCEPT</span><br><span class="line">ip6tables -A INPUT -p ipv6-icmp -j ACCEPT</span><br><span class="line">ip6tables -A FORWARD -p ipv6-icmp -j ACCEPT</span><br><span class="line"></span><br><span class="line"># Ajust MTU</span><br><span class="line">ip6tables -t mangle -A POSTROUTING -o pppoe-wan -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br></pre></td></tr></table></figure></p>
<p>IPv6 Client端:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ip6tables -t filter -A INPUT -p udp -m udp --sport 547 --dport 546 -j ACCEPT</span><br><span class="line">ip6tables -t filter -A INPUT -p ipv6-icmp -j ACCEPT</span><br><span class="line"></span><br><span class="line"># or security group</span><br><span class="line">https://bugs.launchpad.net/neutron/+bug/1335984</span><br><span class="line">openstack security group rule create $secgroup --protocol udp --dst-port 546 --ethertype IPv6</span><br><span class="line">openstack security group rule create $secgroup --protocol icmpv6 --ethertype IPv6</span><br><span class="line"></span><br><span class="line"># Flow based firewall</span><br><span class="line">hard_timeout=0,idle_timeout=0,priority=4,udp,tp_dst=546/0xffff,table=32,tp_src=547/0xffff,nw_src=fe80::f816:3eff:fea3:ec40,actions=learn(table=33,priority=5,hard_timeout=120,eth_type=0x800,nw_proto=17,NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],NXM_OF_IP_SRC[]=NXM_OF_IP_DST[],NXM_OF_UDP_SRC[]=NXM_OF_UDP_DST[], NXM_OF_UDP_DST[]=NXM_OF_UDP_SRC[],output:NXM_OF_IN_PORT[]),normal</span><br></pre></td></tr></table></figure></p>
<p>另外, 别忘了禁用掉ufw或者SELinux之类的.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ufw disable</span><br></pre></td></tr></table></figure></p>
<h2 id="Statefull-DHCPv6"><a href="#Statefull-DHCPv6" class="headerlink" title="Statefull DHCPv6"></a>Statefull DHCPv6</h2><p>采用isc-dhcp-server搭建DHCPv6 Server:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">https://jochen.kirstaetter.name/dhcpv6-ipv6-in-your-local-network/</span><br><span class="line">hua@t440p:~$ ip addr show eth0 |grep inet6 |grep global</span><br><span class="line">    inet6 2001:192:168:99::430/128 scope global</span><br><span class="line">echo &apos;Acquire::ForceIPv4 &quot;true&quot;;&apos; | sudo tee /etc/apt/apt.conf.d/99force-ipv4</span><br><span class="line">sudo apt install isc-dhcp-server</span><br><span class="line">grep -v ^# /etc/dhcp/dhcpd6.conf</span><br><span class="line">sudo cp /etc/dhcp/dhcpd6.conf /etc/dhcp/dhcpd6.conf_bak</span><br><span class="line">sudo bash -c &apos;cat &gt;/etc/dhcp/dhcpd6.conf&apos; &lt;&lt;EOF</span><br><span class="line">authoritative;</span><br><span class="line">default-lease-time 14400;</span><br><span class="line">max-lease-time 86400;</span><br><span class="line">log-facility local7;</span><br><span class="line">subnet6 2001:192:168:99::/64 &#123;</span><br><span class="line">    option dhcp6.name-servers 2001:4860:4860::8888, 2001:4860:4860::8844;</span><br><span class="line">    option dhcp6.domain-search &quot;quqi.com&quot;;</span><br><span class="line">    range6 2001:192:168:99::100 2001:192:168:99::199;</span><br><span class="line">    range6 2001:192:168:99::/64 temporary;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sudo touch /var/lib/dhcp/dhcpd6.leases</span><br><span class="line">sudo /usr/sbin/dhcpd -6 -d -cf /etc/dhcp/dhcpd6.conf eth0</span><br><span class="line">sudo chown dhcpd:dhcpd /var/lib/dhcp/dhcpd6.leases</span><br><span class="line">sudo service isc-dhcp-server6 restart</span><br></pre></td></tr></table></figure></p>
<p>然后记得照上节说的设置DHCPv6 Server与Client上的防火墙规则. 接着在另一台机器上作client测试:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># need to use &apos;inet6 dhcp&apos; in client side for statefull DHCPv6</span><br><span class="line">iface br-eth0 inet6 dhcp</span><br><span class="line">    #iface eth0 inet6 static</span><br><span class="line">    #address 2001:192:168:99::135</span><br><span class="line">    #gateway 2001:192:168:99::1</span><br><span class="line">    #netmask 64</span><br><span class="line">    # use SLAAC to get global IPv6 address from the router</span><br><span class="line">    # we may not enable ipv6 forwarding, otherwise SLAAC gets disabled</span><br><span class="line">    # sleep 5 is due a bug and &apos;dhcp 1&apos; indicates that info should be obtained from dhcpv6 server for stateless</span><br><span class="line">    up echo 0 &gt; /proc/sys/net/ipv6/conf/$IFACE/disable_ipv6</span><br><span class="line">    up sleep 5</span><br><span class="line">    autoconf 1</span><br><span class="line">    accept_ra 2</span><br><span class="line">    dhcp 1</span><br><span class="line"></span><br><span class="line"># test command</span><br><span class="line">dhclient -6 -d br-eth0</span><br><span class="line"></span><br><span class="line"># verity</span><br><span class="line">hua@node1:~$ sudo tcpdump -ni eth0 ip6 host fe80::d5a3:10a3:6161:5b2e</span><br><span class="line">12:44:00.868609 IP6 fe80::d5a3:10a3:6161:5b2e.547 &gt; fe80::fa32:e4ff:febe:87cd.546: dhcp6 advertise</span><br><span class="line">12:44:01.946548 IP6 fe80::d5a3:10a3:6161:5b2e.547 &gt; fe80::fa32:e4ff:febe:87cd.546: dhcp6 reply</span><br><span class="line">root@node1:~# cat /etc/resolv.conf</span><br><span class="line"># Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)</span><br><span class="line">#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN</span><br><span class="line">nameserver 192.168.99.1</span><br><span class="line">#nameserver 211.136.17.107</span><br><span class="line">#nameserver 114.114.114.114</span><br><span class="line">#nameserver 223.5.5.5</span><br><span class="line">nameserver 2001:4860:4860::8888</span><br><span class="line">nameserver 2001:4860:4860::8844</span><br><span class="line">nameserver 2001:db8::1:6a1:51ff:fe8a:2ca7</span><br><span class="line">search quqi.com lan</span><br></pre></td></tr></table></figure>
<p>另外, 使用BIND9的例子可参见 - <a href="https://jochen.kirstaetter.name/enabling-dns-for-ipv6-infrastructure/" target="_blank" rel="external">https://jochen.kirstaetter.name/enabling-dns-for-ipv6-infrastructure/</a></p>
<h2 id="SLAAC-Stateless-Address-Auto-Configuration"><a href="#SLAAC-Stateless-Address-Auto-Configuration" class="headerlink" title="SLAAC (Stateless Address Auto Configuration)"></a>SLAAC (Stateless Address Auto Configuration)</h2><p>radvd来提供RA部分, SLAAC只有RA部分. RA只能设置IPv6 prefix与DNS (RDNSS).<br>Historically the software package radvd was commonly used for just the RA-part of this. But dnsmasq offers a more complete setup.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -w net.ipv6.conf.all.forwarding=1</span><br><span class="line">sudo ip addr add 2001:db8:0:1::1/64 dev eth0</span><br><span class="line">sudo apt-get install radvd</span><br><span class="line">$ cat /etc/radvd.conf</span><br><span class="line">    interface eth0</span><br><span class="line">    &#123;</span><br><span class="line">       AdvSendAdvert on;</span><br><span class="line">       prefix 2001:db8:0:1::/64</span><br><span class="line">       &#123;</span><br><span class="line">            AdvOnLink on;</span><br><span class="line">            AdvAutonomous on;</span><br><span class="line">       &#125;;</span><br><span class="line">       #Send DNS Server setting</span><br><span class="line">       #RDNSS fd5d:12c9:2201:1::2&#123;</span><br><span class="line">    &#125;;</span><br><span class="line">sudo /etc/init.d/radvd restart</span><br><span class="line">sudo ip6tables -F</span><br><span class="line"></span><br><span class="line">neutron subnet-create --ip-version=6 --name=ext-v6-subnet --gateway 2001:db8:0:1::1 --allocation-pool start=2001:db8:0:1::5,end=2001:db8:0:1:ffff:ffff:ffff:fffe --disable-dhcp ext_net 2001:db8:0:1::/64</span><br><span class="line">neutron net-create private</span><br><span class="line">neutron subnet-create --ip-version=6 --name=private_v6_subnet --ipv6-address-mode=slaac --ipv6-ra-mode=slaac private 2001:db8:0:2::/64</span><br><span class="line">neutron router-interface-add provider-router private_v6_subnet</span><br></pre></td></tr></table></figure></p>
<h2 id="SLAAS-with-Stateless-DHCPv6"><a href="#SLAAS-with-Stateless-DHCPv6" class="headerlink" title="SLAAS with Stateless DHCPv6"></a>SLAAS with Stateless DHCPv6</h2><p>Stateless意味着:</p>
<ul>
<li>radvd提供RA (AdvManagedFlag=off)</li>
<li>client使用radvd RA提供的IPv6 prefix配置IPv6 address</li>
<li>client的其他信息如DNS等从DHCPv6获得<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">sudo bash -c &apos;cat &gt; /etc/dhcp/dhcpd6.conf&apos; &lt;&lt;EOF</span><br><span class="line">interface eth0</span><br><span class="line">&#123;</span><br><span class="line">    AdvSendAdvert on;</span><br><span class="line">    MinRtrAdvInterval 30;</span><br><span class="line">    MaxRtrAdvInterval 100;</span><br><span class="line">    AdvManagedFlag off;</span><br><span class="line">    AdvOtherConfigFlag on;</span><br><span class="line">    prefix 2001:192:168:99::/64</span><br><span class="line">    &#123;</span><br><span class="line">        AdvOnLink on;</span><br><span class="line">        AdvAutonomous on;</span><br><span class="line">        AdvRouterAddr off;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo bash -c &apos;cat &gt; /etc/radvd.conf&apos; &lt;&lt;EOF</span><br><span class="line">default-lease-time 600;</span><br><span class="line">max-lease-time 7200;</span><br><span class="line">log-facility local7;</span><br><span class="line">option dhcp6.name-servers 2001:4860:4860::8888;</span><br><span class="line">option dhcp6.domain-search &quot;&quot;;</span><br><span class="line">subnet6 2001:192:168:99::/64 &#123;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="SLAAS-with-Statefull-DHCPv6"><a href="#SLAAS-with-Statefull-DHCPv6" class="headerlink" title="SLAAS with Statefull DHCPv6"></a>SLAAS with Statefull DHCPv6</h2><p>Statefull意味着:</p>
<ul>
<li>radvd不提供RA (AdvManagedFlag=on)</li>
<li>client使用DHCPv6去配置IPv6 address</li>
<li>client的其他信息如DNS等也从DHCPv6获得<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">sudo bash -c &apos;cat &gt; /etc/dhcp/dhcpd6.conf&apos; &lt;&lt;EOF</span><br><span class="line">interface eth0</span><br><span class="line">&#123;</span><br><span class="line">    AdvSendAdvert on;</span><br><span class="line">    MinRtrAdvInterval 30;</span><br><span class="line">    MaxRtrAdvInterval 100;</span><br><span class="line">    AdvManagedFlag on;</span><br><span class="line">    AdvOtherConfigFlag on;</span><br><span class="line">    prefix 2001:192:168:99::/64</span><br><span class="line">    &#123;</span><br><span class="line">        AdvOnLink on;</span><br><span class="line">        AdvAutonomous on;</span><br><span class="line">        AdvRouterAddr off;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sudo bash -c &apos;cat &gt;/etc/dhcp/dhcpd6.conf&apos; &lt;&lt;EOF</span><br><span class="line">authoritative;</span><br><span class="line">default-lease-time 14400;</span><br><span class="line">max-lease-time 86400;</span><br><span class="line">log-facility local7;</span><br><span class="line">subnet6 2001:192:168:99::/64 &#123;</span><br><span class="line">    option dhcp6.name-servers 2001:4860:4860::8888, 2001:4860:4860::8844;</span><br><span class="line">    option dhcp6.domain-search &quot;quqi.com&quot;;</span><br><span class="line">    range6 2001:192:168:99::100 2001:192:168:99::199;</span><br><span class="line">    range6 2001:192:168:99::/64 temporary;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题"></a>实际问题</h2><p>例如, 使用OpenStack根据外部IPv6 stateless router定义的IPv6网络, 虚机分配了IP, 但是网卡上去没配置, 一般地, 理论上问题出在:<br>1, 既然有外部路由器, Openstack CLI中不应该定义ipv6_ra_mode, 不指定ipv6_ra_mode, neutron就不会创建radvd, 那样就会直接使用外部的IPv6路由器<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">neutron net-create IPv6 (vlan1809) --shared --provider:physical_network physnet2 --provider:network_type vlan --provider:segmentation_id 1809 --router:external True</span><br><span class="line">neutron subnet-create ipv6-pd --name external-subnet-v6 --ip_version 6 --ipv6_address_mode dhcp-stateless --allocation-pool start=2001:1284:ff:18::2,end=2001:1284:ff:18:ffff:ffff:ffff:ffff --dns-nameserver 2001:1284:ff02::243 2001:1284:ff:18::/64</span><br></pre></td></tr></table></figure></p>
<p>2, 外部IPv6路由器的防火墙规则打开了没:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo ip6tables -A INPUT -p icmpv6 -j ACCEPT</span><br><span class="line">sudo ip6tables -A OUTPUT -p icmpv6 -j ACCEPT</span><br><span class="line">sudo ip6tables -A FORWARD -p icmpv6 -j ACCEPT</span><br><span class="line">sudo ufw allow proto udp from fe80::/64 to any port 547</span><br><span class="line">sudo ufw disable</span><br></pre></td></tr></table></figure></p>
<p>3, 虚机这边虚机定义security group rule将其所有的计算节点上的防火墙规则打开:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">https://bugs.launchpad.net/neutron/+bug/1335984</span><br><span class="line"></span><br><span class="line">openstack security group rule create $secgroup --protocol udp --dst-port 546 --ethertype IPv6</span><br><span class="line">openstack security group rule create $secgroup --protocol icmpv6 --ethertype IPv6</span><br><span class="line"></span><br><span class="line">For iptables driver, above equals:</span><br><span class="line">ip6tables -t filter -A INPUT -p udp -m udp --sport 547 --dport 546 -j ACCEPT</span><br><span class="line">ip6tables -t filter -A INPUT -p ipv6-icmp -j ACCEPT</span><br><span class="line"></span><br><span class="line">For ovs flow based driver, above equals:</span><br><span class="line">hard_timeout=0,idle_timeout=0,priority=4,udp,tp_dst=546/0xffff,table=32,tp_src=547/0xffff,nw_src=fe80::f816:3eff:fea3:ec40,actions=learn(table=33,priority=5,hard_timeout=120,eth_type=0x800,nw_proto=17,NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],NXM_OF_IP_SRC[]=NXM_OF_IP_DST[],NXM_OF_UDP_SRC[]=NXM_OF_UDP_DST[], NXM_OF_UDP_DST[]=NXM_OF_UDP_SRC[],output:NXM_OF_IN_PORT[]),normal</span><br></pre></td></tr></table></figure></p>
<p>3, 外部路由器因为启用了forwarding应该设置accept_ra=2而不是accept_ra=1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -w net.ipv6.conf.all.accept_ra=2</span><br></pre></td></tr></table></figure></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="http://asdkda.github.io/2016/02/05/ipv6/" target="_blank" rel="external">http://asdkda.github.io/2016/02/05/ipv6/</a><br>[2] <a href="http://tldp.org/HOWTO/html_single/Linux+IPv6-HOWTO/#idp57787920" target="_blank" rel="external">http://tldp.org/HOWTO/html_single/Linux+IPv6-HOWTO/#idp57787920</a><br>[3] <a href="https://jochen.kirstaetter.name/dhcpv6-ipv6-in-your-local-network/" target="_blank" rel="external">https://jochen.kirstaetter.name/dhcpv6-ipv6-in-your-local-network/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/02/Using-kubeadm-to-deploy-k8s/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/02/Using-kubeadm-to-deploy-k8s/" itemprop="url">Using kubeadm to deploy k8s</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-02T11:08:22+08:00">
                2019-01-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (作者：张华 发表于：2018-07-13)</strong></p>
<h2 id="Bootstrapping-master-with-kubeadm"><a href="#Bootstrapping-master-with-kubeadm" class="headerlink" title="Bootstrapping master with kubeadm"></a>Bootstrapping master with kubeadm</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">https://kubernetes.io/docs/setup/independent/install-kubeadm/</span><br><span class="line">Create a VM (ubuntu 18.04) joshuazhang1c.mylabserver.com in linuxacademy.com, then run:</span><br><span class="line"></span><br><span class="line"># Reset env</span><br><span class="line">kubectl drain joshuazhang1c.mylabserver.com --delete-local-data --force --ignore-daemonsets</span><br><span class="line">kubectl get node</span><br><span class="line">kubectl delete node joshuazhang1c.mylabserver.com</span><br><span class="line">sudo kubeadm reset</span><br><span class="line"></span><br><span class="line"># Installing kubeadm, kubelet, kubectl, docker</span><br><span class="line">sudo apt update &amp;&amp; sudo apt install -y apt-transport-https curl</span><br><span class="line">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -</span><br><span class="line">sudo bash -c &apos;cat &gt;/etc/apt/sources.list.d/kubernetes.list&apos; &lt;&lt;EOF</span><br><span class="line">deb https://apt.kubernetes.io/ kubernetes-xenial main</span><br><span class="line">EOF</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install -y kubelet kubeadm kubectl</span><br><span class="line">sudo apt-mark hold kubelet kubeadm kubectl</span><br><span class="line">cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">sudo apt install -y docker.io</span><br><span class="line">sudo systemctl enable docker.service</span><br><span class="line"></span><br><span class="line"># Creating a single master cluster with kubeadm</span><br><span class="line"># For flannle to work correctly, you must pass --pod-network-cidr=10.244.0.0/16</span><br><span class="line">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/</span><br><span class="line">sudo kubeadm init --pod-network-cidr=10.244.0.0/16</span><br><span class="line">kubectl describe ds kube-flannel-ds-amd64 --namespace kube-system</span><br><span class="line">cat /etc/cni/net.d/10-flannel.conflist</span><br><span class="line"></span><br><span class="line"># the following commands come from the output of &apos;kubeadm init&apos;</span><br><span class="line"># export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">rm -rf ~/.kube/config &amp;&amp; sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line">kubectl cluster-info</span><br><span class="line"></span><br><span class="line"># configure kubectl completion</span><br><span class="line">kubectl completion bash | sudo tee /etc/bash_completion.d/k8s</span><br><span class="line"></span><br><span class="line"># Installing a pod network add-on</span><br><span class="line">sudo bash -c &apos;cat &gt;&gt; /etc/sysctl.conf&apos; &lt;&lt; EOF</span><br><span class="line">net.bridge.bridge-nf-call-iptables=1</span><br><span class="line">EOF</span><br><span class="line">sysctl -p</span><br><span class="line">sysctl net.bridge.bridge-nf-call-iptables=1</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml</span><br><span class="line">kubectl get daemonsets --all-namespaces</span><br><span class="line">kubectl get nodes --all-namespaces -o wide</span><br><span class="line"></span><br><span class="line"># Joining your nodes with kubeadm way, we will aslo try tls bootstrap way below</span><br><span class="line">ssh cloud_user@joshuazhang3c.mylabserver.com -v</span><br><span class="line">sudo -i</span><br><span class="line">apt install -y docker.io</span><br><span class="line">systemctl enable docker.service</span><br><span class="line">apt update &amp;&amp; apt install -y apt-transport-https curl</span><br><span class="line">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -</span><br><span class="line">cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">deb https://apt.kubernetes.io/ kubernetes-xenial main</span><br><span class="line">EOF</span><br><span class="line">apt update</span><br><span class="line">apt install -y kubeadm</span><br><span class="line"></span><br><span class="line"># kubeadm token list</span><br><span class="line"># openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &apos;s/^.* //&apos;</span><br><span class="line"># kubectl get secrets --namespace=kube-system bootstrap-token-9k9xoo -o jsonpath=&apos;&#123;.data.token-id&#125;&apos; |base64 -d</span><br><span class="line"># kubectl get secrets --namespace=kube-system bootstrap-token-9k9xoo -o jsonpath=&apos;&#123;.data.token-secret&#125;&apos; |base64 -d</span><br><span class="line">kubeadm join 172.31.19.84:6443 --token 0c4fdy.xptpmgh4eqihxh66 --discovery-token-ca-cert-hash sha256:b227cfd35c9d1ad42d8692576c0a453271741f59e5052c98674bc075b0789a17</span><br></pre></td></tr></table></figure>
<h2 id="Bootstrapping-workerwith-tls-bootstrapping"><a href="#Bootstrapping-workerwith-tls-bootstrapping" class="headerlink" title="Bootstrapping workerwith tls bootstrapping"></a>Bootstrapping workerwith tls bootstrapping</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line">https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/</span><br><span class="line">Just need to copy two files(/var/lib/kubelet/config.yaml and ca.crt) to new worker node, then use bootstrap token (temporary auth) and ca.crt to generate kubeconfig, finally restart kubelet with &apos;kubelet --bootstrap-kubeconfig=&quot;/etc/kubelet/bootstrap-kubelet.conf&quot; --kubeconfig=&quot;/etc/kubelet/kubeconfig.conf&quot; --config=&quot;/var/lib/kubelet/config.yaml&quot;&apos;, then master will create the certificate(for non-temporary auth) for worker and auto approve them.</span><br><span class="line"></span><br><span class="line">a, Principle behind. kubeadm has created bootstrap token with the auth-extra-groups &apos;system:bootstrappers:kubeadm:default-node-token&apos; for us. You can change the group name &apos;system:bootstrappers:kubeadm:default-node-token&apos; to another, eg: system:bootstrappers:myworkers</span><br><span class="line"></span><br><span class="line">$ kubectl get --namespace=kube-system secrets bootstrap-token-9k9xoo -o jsonpath=&apos;&#123;.data.auth-extra-groups&#125;&apos; |base64 -d</span><br><span class="line">system:bootstrappers:kubeadm:default-node-token</span><br><span class="line"></span><br><span class="line">sudo bash -c &apos;cat &gt; bootstrap-token.yaml&apos; &lt;&lt;EOF</span><br><span class="line"># https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  # Name MUST be of form &quot;bootstrap-token-&lt;token id&gt;&quot;</span><br><span class="line">  name: bootstrap-token-07401b</span><br><span class="line">  namespace: kube-system</span><br><span class="line"># Type MUST be &apos;bootstrap.kubernetes.io/token&apos;</span><br><span class="line">type: bootstrap.kubernetes.io/token</span><br><span class="line">stringData:</span><br><span class="line">  # Human readable description. Optional.</span><br><span class="line">  description: &quot;The default bootstrap token generated by &apos;kubeadm init&apos;.&quot;</span><br><span class="line">  # Token ID and secret. Required.</span><br><span class="line">  token-id: 07401b</span><br><span class="line">  token-secret: f395accd246ae52d</span><br><span class="line">  # Expiration. Optional.</span><br><span class="line">  expiration: 2019-03-10T03:22:11Z</span><br><span class="line">  # Allowed usages.</span><br><span class="line">  usage-bootstrap-authentication: &quot;true&quot;</span><br><span class="line">  usage-bootstrap-signing: &quot;true&quot;</span><br><span class="line">  # Extra groups to authenticate the token as. Must start with &quot;system:bootstrappers:&quot;</span><br><span class="line">  auth-extra-groups: system:bootstrappers:kubeadm:default-node-token</span><br><span class="line">EOF</span><br><span class="line">kubectl create -f bootstrap-token.yaml</span><br><span class="line">kubectl describe secrets --namespace=kube-system bootstrap-token-07401b</span><br><span class="line">kubectl get secrets --namespace=kube-system bootstrap-token-07401b -o jsonpath=&#123;.data.token-id&#125; |base64 -d</span><br><span class="line">kubectl get secrets --namespace=kube-system bootstrap-token-07401b -o jsonpath=&#123;.data.token-secret&#125; |base64 -d</span><br><span class="line"></span><br><span class="line">So the following &apos;kubeadm token create&apos; will create a new token with the auth-extra-groups &apos;system:bootstrappers:kubeadm:default-node-token&apos;.</span><br><span class="line">$ kubeadm token create</span><br><span class="line">iate9c.v9qhw2dyngxfcsig</span><br><span class="line">TOKEN_ID=$(kubectl get secrets --namespace=kube-system bootstrap-token-iate9c -o jsonpath=&apos;&#123;.data.token-id&#125;&apos; |base64 -d)</span><br><span class="line">TOKEN_SECRET=$(kubectl get secrets --namespace=kube-system bootstrap-token-iate9c -o jsonpath=&apos;&#123;.data.token-secret&#125;&apos; |base64 -d)</span><br><span class="line"></span><br><span class="line">b, Principle behind. kubeadm has also create the following 3 clusterrolebindings to map the group &apos;system:bootstrappers:kubeadm:default-node-token&apos; for us. If you are using the new group &apos;system:bootstrappers:myworkers&apos;, here you need to change to &apos;system:bootstrappers:myworkers&apos; or &apos;system:bootstrappers&apos;.</span><br><span class="line"></span><br><span class="line">#Authorize kubelet to create CSR by mapping the clusterrole &apos;system:node-bootstrapper&apos; to the group &apos;system:bootstrappers&apos; or &apos;system:bootstrappers:joshuazhang2c.mylabserver.com&apos;</span><br><span class="line">kubectl create clusterrolebinding create-csrs-for-bootstrapping --group=system:bootstrappers:kubeadm:default-node-token --clusterrole=system:node-bootstrapper</span><br><span class="line"></span><br><span class="line">#Auto approve all CSRs by mapping the clusterrole &apos;selfnodeclient&apos; to the group &quot;system:bootstrappers&quot; or &apos;system:bootstrappers:joshuazhang2c.mylabserver.com&apos;</span><br><span class="line">kubectl create clusterrolebinding auto-approve-csrs-for-group --group=system:bootstrappers:kubeadm:default-node-token --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient</span><br><span class="line"></span><br><span class="line">#Auto approve renewal CSRs by mapping the clusterrole &apos;selfnodeclient&apos; to the group &quot;system:nodes&quot;</span><br><span class="line">kubectl create clusterrolebinding auto-approve-renewals-csrs-for-group --group=system:nodes --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</span><br><span class="line"></span><br><span class="line">So we can use the following CertificateSigningRequest to self-test it.</span><br><span class="line"></span><br><span class="line">openssl genrsa -out joshuazhang2c.mylabserver.com.key</span><br><span class="line">openssl req -new -key joshuazhang2c.mylabserver.com.key -out joshuazhang2c.mylabserver.com.csr -subj &quot;/CN=system:node:joshuazhang2c.mylabserver.com/O=system:nodes&quot;</span><br><span class="line">openssl x509 -req -in joshuazhang2c.mylabserver.com.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out joshuazhang4c.mylabserver.com.crt -days 45</span><br><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: certificates.k8s.io/v1beta1</span><br><span class="line">kind: CertificateSigningRequest</span><br><span class="line">metadata:</span><br><span class="line">  name: system:node:joshuazhang2c.mylabserver.com</span><br><span class="line">spec:</span><br><span class="line">  groups:</span><br><span class="line">    - system:nodes</span><br><span class="line">  request: $(cat joshuazhang2c.mylabserver.com.csr | base64 | tr -d &apos;\n&apos;)</span><br><span class="line">  usages:</span><br><span class="line">    - key encipherment</span><br><span class="line">    - digital signature</span><br><span class="line">    - client auth</span><br><span class="line">EOF</span><br><span class="line">kubectl get csr</span><br><span class="line"></span><br><span class="line">c, Generate kubeconfig with bootstrap token and ca</span><br><span class="line">ENDPOINT=$(kubectl describe service kubernetes |grep -i endpoints |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">CLUSTER=$(kubectl config view |grep &apos;  cluster:&apos; |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">TOKEN_ID=$(kubectl get secrets --namespace=kube-system bootstrap-token-iate9c -o jsonpath=&apos;&#123;.data.token-id&#125;&apos; |base64 -d)</span><br><span class="line">TOKEN_SECRET=$(kubectl get secrets --namespace=kube-system bootstrap-token-iate9c -o jsonpath=&apos;&#123;.data.token-secret&#125;&apos; |base64 -d)</span><br><span class="line">sudo kubectl config --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf set-cluster $&#123;CLUSTER&#125; --server=https://$&#123;ENDPOINT&#125; --certificate-authority=/etc/kubernetes/pki/ca.crt</span><br><span class="line">sudo kubectl config --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf set-credentials kubelet-bootstrap --token=$&#123;TOKEN_ID&#125;.$&#123;TOKEN_SECRET&#125;</span><br><span class="line">sudo kubectl config --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf set-context bootstrap --user=kubelet-bootstrap --cluster=$&#123;CLUSTER&#125;</span><br><span class="line">sudo kubectl config --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf use-context bootstrap</span><br><span class="line"></span><br><span class="line"># Copy the following files from master to the new node</span><br><span class="line">scp joshuazhang1c.mylabserver.com:/etc/kubernetes/bootstrap-kubelet.conf . &amp;&amp; sudo mv bootstrap-kubelet.conf /etc/kubernetes/</span><br><span class="line">scp joshuazhang1c.mylabserver.com:/etc/kubernetes/pki/ca.crt . &amp;&amp; sudo mv ca.crt /etc/kubernetes/pki/</span><br><span class="line">scp joshuazhang1c.mylabserver.com:/var/lib/kubelet/config.yaml . &amp;&amp; sudo mv config.yaml /var/lib/kubelet/</span><br><span class="line"></span><br><span class="line">c, Install kubelet in the new node joshuazhang2c.mylabserver.com</span><br><span class="line"></span><br><span class="line">sudo apt update &amp;&amp; sudo apt install -y apt-transport-https curl</span><br><span class="line">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -</span><br><span class="line">sudo bash -c &apos;cat &gt;/etc/apt/sources.list.d/kubernetes.list&apos; &lt;&lt;EOF</span><br><span class="line">deb https://apt.kubernetes.io/ kubernetes-xenial main</span><br><span class="line">EOF</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install -y kubelet kubeadm kubectl</span><br><span class="line">sudo apt-mark hold kubelet kubeadm kubectl</span><br><span class="line">cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">sudo apt install -y docker.io</span><br><span class="line">sudo systemctl enable docker.service</span><br><span class="line"></span><br><span class="line">sudo bash -c &apos;cat &gt;/lib/systemd/system/kubelet.service&apos; &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=kubelet: The Kubernetes Node Agent</span><br><span class="line">Documentation=https://kubernetes.io/docs/home/</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line">[Service]</span><br><span class="line">#ExecStart=/usr/bin/kubelet --bootstrap-kubeconfig=&quot;/etc/kubernetes/bootstrap-kubelet.conf&quot; --kubeconfig=&quot;/etc/kubernetes/kubeconfig.conf&quot; --config=&quot;/var/lib/kubernetes/config.yaml&quot;</span><br><span class="line">ExecStart=/usr/bin/kubelet --bootstrap-kubeconfig=&quot;/etc/kubernetes/bootstrap-kubelet.conf&quot; --kubeconfig=&quot;/etc/kubernetes/kubeconfig.conf&quot; --pod-manifest-path=&quot;/etc/kubernetes/manifests/&quot; --feature-gates=RotateKubeletClientCertificate=true</span><br><span class="line">Restart=always</span><br><span class="line">StartLimitInterval=0</span><br><span class="line">RestartSec=10</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line">sudo systemctl daemon-reload; sudo systemctl restart kubelet</span><br><span class="line">sudo systemctl status kubelet</span><br><span class="line"></span><br><span class="line"># Verify</span><br><span class="line">kubectl get nodes joshuazhang2c.mylabserver.com</span><br><span class="line"># kubectl get csr</span><br><span class="line">NAME                                                   AGE   REQUESTOR                 CONDITION</span><br><span class="line">node-csr-mdou0axSg2vlk5wx2_a1uA0-buvaC-PsiF69Jvjg110   87s   system:bootstrap:iate9c   Approved,Issued</span><br><span class="line"># ls /var/lib/kubelet/pki/</span><br><span class="line">kubelet-client-2018-12-04-08-50-51.pem  kubelet-client-current.pem  kubelet.crt  kubelet.key</span><br><span class="line"># openssl x509 -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem |grep system:node</span><br><span class="line">        Subject: O = system:nodes, CN = system:node:joshuazhang4c.mylabserver.com</span><br></pre></td></tr></table></figure>
<h2 id="附件-RBAC-authentication"><a href="#附件-RBAC-authentication" class="headerlink" title="附件 - RBAC authentication"></a>附件 - RBAC authentication</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">kubectl create ns development</span><br><span class="line">kubectl create ns production</span><br><span class="line">$ kubectl config get-contexts</span><br><span class="line">CURRENT   NAME           CLUSTER        AUTHINFO   NAMESPACE</span><br><span class="line">*         juju-context   juju-cluster   admin</span><br><span class="line"></span><br><span class="line">sudo useradd -s /bin/bash DevHua</span><br><span class="line">sudo passwd DevHua</span><br><span class="line"></span><br><span class="line"># Generate a private key, then Certificate Signing Request (CSR) for DevHua</span><br><span class="line">openssl genrsa -out DevHua.key</span><br><span class="line">openssl req -new -key DevHua.key -out DevHua.csr -subj &quot;/CN=DevHua/O=development&quot;</span><br><span class="line"># Using the newly created request generate a self-signed certificate using the x509 protocol</span><br><span class="line">openssl x509 -req -in DevHua.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out DevHua.crt -days 45</span><br><span class="line"></span><br><span class="line">kubectl config view</span><br><span class="line">kubectl config set-credentials --help</span><br><span class="line">kubectl config set-credentials DevHua --client-certificate=./DevHua.crt --client-key=./DevHua.key</span><br><span class="line">kubectl config set-context --help</span><br><span class="line">kubectl config set-context DevHua-context --cluster=juju-cluster --namespace=development --user=DevHua</span><br><span class="line">kubectl --context=DevHua-context get pods</span><br><span class="line">#kubectl config use-context DevHua-context</span><br><span class="line">kubectl config get-contexts</span><br><span class="line"></span><br><span class="line">sudo bash -c &apos;cat &gt; role-dev.yaml&apos; &lt;&lt;EOF</span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: development</span><br><span class="line">  name: developer</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;, &quot;extensions&quot;, &quot;apps&quot;]</span><br><span class="line">  resources: [&quot;deployments&quot;, &quot;replicasets&quot;, &quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;list&quot;, &quot;get&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]</span><br><span class="line">EOF</span><br><span class="line">kubectl create -f role-dev.yaml</span><br><span class="line">kubectl -n development describe roles developer</span><br><span class="line"></span><br><span class="line">sudo bash -c &apos;cat &gt; rolebind.yaml&apos; &lt;&lt;EOF</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: developer-role-binding</span><br><span class="line">  namespace: development</span><br><span class="line">subjects:</span><br><span class="line">  - kind: User</span><br><span class="line">    name: DevHua</span><br><span class="line">    apiGroup: &quot;&quot;</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: developer</span><br><span class="line">  apiGroup: &quot;&quot;</span><br><span class="line">EOF</span><br><span class="line">kubectl apply -f rolebind.yaml</span><br><span class="line">kubectl -n development describe rolebinding developer-role-binding</span><br><span class="line"></span><br><span class="line">kubectl --context=DevHua-context run nginx --image=nginx</span><br><span class="line">kubectl --context=DevHua-context get pods</span><br><span class="line">kubectl --context=DevHua-context delete deploy nginx</span><br><span class="line"></span><br><span class="line">sudo bash -c &apos;cat &gt; adminrolebind.yaml&apos; &lt;&lt;EOF</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: developer-adminrole-binding</span><br><span class="line">  namespace: development</span><br><span class="line">subjects:</span><br><span class="line">  - kind: User</span><br><span class="line">    name: DevHua</span><br><span class="line">    apiGroup: &quot;&quot;</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">  apiGroup: &quot;&quot;</span><br><span class="line">EOF</span><br><span class="line">kubectl apply -f adminrolebind.yaml</span><br><span class="line">kubectl --context=DevHua-context get pods</span><br><span class="line"></span><br><span class="line">kubectl apply -f role-prod.yaml</span><br><span class="line">vim role-prod.yaml</span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: production #&lt;&lt;- This line</span><br><span class="line">  name: dev-prod #&lt;&lt;- and this line</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;, &quot;extensions&quot;, &quot;apps&quot;]</span><br><span class="line">  resources: [&quot;deployments&quot;, &quot;replicasets&quot;, &quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] #&lt;&lt;- and this one</span><br><span class="line"></span><br><span class="line">kubectl apply -f rolebindprod.yaml</span><br><span class="line">vim rolebindprod.yaml</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: production-role-binding</span><br><span class="line">  namespace: production</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: DevDan</span><br><span class="line">  apiGroup: &quot;&quot;</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: dev-prod</span><br><span class="line">  apiGroup: &quot;&quot;</span><br><span class="line"></span><br><span class="line">kubectl config set-context ProdHua-context --cluster=kubernetes --namespace=production --user=DevHua</span><br><span class="line">kubectl --context=ProdHua-context run nginx --image=nginx</span><br></pre></td></tr></table></figure>
<h2 id="附件-RBAC-authentication-in-Dashboard"><a href="#附件-RBAC-authentication-in-Dashboard" class="headerlink" title="附件 - RBAC authentication in Dashboard"></a>附件 - RBAC authentication in Dashboard</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># Use default anonymous user</span><br><span class="line"># generate client-certificate-data</span><br><span class="line">grep &apos;client-certificate-data&apos; /var/run/kubernetes/admin.kubeconfig | head -n 1 | awk &apos;&#123;print $2&#125;&apos; | base64 -d &gt;&gt; kubecfg.crt</span><br><span class="line"># generate client-key-data</span><br><span class="line">grep &apos;client-key-data&apos; /var/run/kubernetes/admin.kubeconfig | head -n 1 | awk &apos;&#123;print $2&#125;&apos; | base64 -d &gt;&gt; kubecfg.key</span><br><span class="line"># generate p12</span><br><span class="line">openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name &quot;kubernetes-client&quot;</span><br><span class="line"></span><br><span class="line">kubectl get secret -n kube-system | grep dashboard</span><br><span class="line">kubectlh -n kube-system  get secret kubernetes-dashboard-token-kglhd -o jsonpath=&#123;.data.token&#125;| base64 -d</span><br><span class="line"></span><br><span class="line"># Use admin user</span><br><span class="line">cat &gt; /tmp/admin-user.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line">cat &gt; /tmp/admin-user-role-binding.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line">kubectl create -f /tmp/admin-user.yaml</span><br><span class="line">kubectl create -f /tmp/admin-user-role-binding.yaml</span><br><span class="line">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin | awk &apos;&#123;print $1&#125;&apos;)</span><br></pre></td></tr></table></figure>
<h2 id="附件-TLS-bootstrapping"><a href="#附件-TLS-bootstrapping" class="headerlink" title="附件 - TLS bootstrapping"></a>附件 - TLS bootstrapping</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/</span><br><span class="line">https://www.codercto.com/a/23740.html</span><br><span class="line">Workers must use a certificate issued by masters to communicatate with masters. To save the workload of creating certificates each time the worker is added, kubelet in worker will use a predefined certificate bootstrap-kubelet.conf to request masters to apply for cerfificate for this worker dynamically.</span><br><span class="line">kubelet has two ports, one is 10250 used to provide read/write tls private api, one is 10255 used to provide read-only non-tls private api.</span><br><span class="line">Bootstrap Token Secret (kubectl describe secrets --namespace=kube-system bootstrap-signer-token-8xsmh) will replace the previous token.csv.</span><br><span class="line"></span><br><span class="line">kube-apiserver side receives the requests for certificates from the kubelet and authenticates those requests:</span><br><span class="line">a, Recognizing CA that signs the client certificate</span><br><span class="line">   kube-apiserver --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-bootstrap-token-auth=true ...</span><br><span class="line">b, Authenticating the bootstrapping kubelet to the system:bootstrappers group</span><br><span class="line"># Create Bootstrap Token</span><br><span class="line">echo &quot;$(head -c 6 /dev/urandom | md5sum | head -c 6)&quot;.&quot;$(head -c 16 /dev/urandom | md5sum | head -c 16)&quot;</span><br><span class="line">vdb9xb.jiqhz35y355g1ngx</span><br><span class="line">vdb9xb.jiqhz35y355g1ngx,kubelet-bootstrap,10001,&quot;system:bootstrappers&quot;  #token.csv</span><br><span class="line">c, Authorize the bootstrapping kubelet to create a certificate signing request (CSR)</span><br><span class="line">kubectl describe roles.rbac.authorization.k8s.io --namespace=kube-system system:controller:bootstrap-signer</span><br><span class="line">sudo bash -c &apos;cat &lt; rolebinding.yaml&apos; &lt;&lt;EOF</span><br><span class="line"># enable bootstrapping nodes to create CSR</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: create-csrs-for-bootstrapping</span><br><span class="line">subjects:</span><br><span class="line">- kind: Group</span><br><span class="line">  name: system:bootstrappers</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:node-bootstrapper</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kube-controller-manager side is responsible for issuing actual signed certificates:</span><br><span class="line">a, access to the “kuberetes CA key and certificate” that you created and distributed</span><br><span class="line">kube-controller-manager --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key ...</span><br><span class="line">b, approve CSR signing automatically</span><br><span class="line">sudo bash -c &apos;cat &lt; certificatesigningrequests.yaml&apos; &lt;&lt;EOF</span><br><span class="line"># Approve all CSRs for the group &quot;system:bootstrappers&quot;</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: auto-approve-csrs-for-group</span><br><span class="line">subjects:</span><br><span class="line">- kind: Group</span><br><span class="line">  name: system:bootstrappers</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">EOF</span><br><span class="line">sudo bash -c &apos;cat &lt; renewal.yaml&apos; &lt;&lt;EOF</span><br><span class="line"># Approve renewal CSRs for the group &quot;system:nodes&quot;</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: auto-approve-renewals-for-nodes</span><br><span class="line">subjects:</span><br><span class="line">- kind: Group</span><br><span class="line">  name: system:nodes</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubelet side:</span><br><span class="line">kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf ...</span><br><span class="line"># cat /etc/kubernetes/bootstrap-kubelet.conf</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: [xxx]</span><br><span class="line">    server: https://172.31.43.252:6443</span><br><span class="line">  name: kubernetes</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: kubernetes</span><br><span class="line">    user: tls-bootstrap-token-user</span><br><span class="line">  name: tls-bootstrap-token-user@kubernetes</span><br><span class="line">current-context: tls-bootstrap-token-user@kubernetes</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: tls-bootstrap-token-user</span><br><span class="line">  user:</span><br><span class="line">    token: vdb9xb.jiqhz35y355g1ngx</span><br><span class="line"></span><br><span class="line">In Summary:</span><br><span class="line">kubectl get secrets -n kube-system |grep -i bootstrap</span><br><span class="line">kubectl -n kube-system get secret bootstrap-signer-token-8xsmh -o jsonpath=&#123;.data.token&#125;| base64 -d</span><br></pre></td></tr></table></figure>
<h2 id="附件-microk8s"><a href="#附件-microk8s" class="headerlink" title="附件 - microk8s"></a>附件 - microk8s</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#sudo snap install microk8s --beta --classic</span><br><span class="line">sudo snap install microk8s --edge --classic</span><br><span class="line">snap list</span><br><span class="line">journalctl -u snap.microk8s.daemon-apiserver.service</span><br><span class="line">microk8s.kubectl get no</span><br><span class="line">microk8s.enable dns dashboard</span><br><span class="line">microk8s.kubectl get all --all-namespaces</span><br><span class="line">lynx http://xxxx</span><br></pre></td></tr></table></figure>
<h2 id="附件-其他"><a href="#附件-其他" class="headerlink" title="附件 - 其他"></a>附件 - 其他</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br></pre></td><td class="code"><pre><span class="line"># How to generate yaml template</span><br><span class="line">kubectl run --restart=Always # creates a Deployment</span><br><span class="line">kubectl run --restart=Never # creates bare pod</span><br><span class="line">kubectl run --restart=OnFailure # creates a Job.</span><br><span class="line"></span><br><span class="line">https://kubernetes.io/docs/reference/kubectl/conventions/#generators</span><br><span class="line">pod template:</span><br><span class="line">  kubectl run --help |grep generator</span><br><span class="line">  kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml</span><br><span class="line">service and deployment template:</span><br><span class="line">  kubectl run nginx --service-generator=&apos;service/v2&apos; --image=nginx --dry-run --expose --port 80 -o yaml</span><br><span class="line">job template:</span><br><span class="line">  kubectl run --generator=job/v1 nginx --image=nginx --dry-run -o yaml</span><br><span class="line"></span><br><span class="line"># jq, jsonpath, sort-by, kubectl top etc</span><br><span class="line">kubectl delete pods,services -l name=myLabel --include-uninitialized</span><br><span class="line">kubectl get pods --field-selector=status.phase=Running</span><br><span class="line">kubectl get pod ubuntu -o yaml |sed &apos;s/\(image: ubuntu\):.*$/\1:18.04/&apos; |kubectl replace -f -</span><br><span class="line">kubectl top pod -l name=nginx-ingress-kubernetes-worker</span><br><span class="line">kubectl get pods --sort-by=.metadata.name</span><br><span class="line"></span><br><span class="line">kubectl get -o template pod/web-pod-13je7 --template=&#123;&#123;.status.phase&#125;&#125;</span><br><span class="line"></span><br><span class="line">kubectl get nodes -o jsonpath=&apos;&#123;.items[*].metadata.name&#125;&apos; equals kubectl get nodes -o jsonpath=&apos;&#123;.items..metadata.name&#125;&apos;</span><br><span class="line">kubectl get nodes -o jsonpath=&apos;&#123;.items[].metadata.name&#125;&apos; equals kubectl get nodes -o jsonpath=&apos;&#123;.items[0].metadata.name&#125;&apos;</span><br><span class="line">kubectl get nodes -o jsonpath=&apos;&#123;.items[*].status.addresses[?(@.type==&quot;InternalIP&quot;)].address&#125;&apos;</span><br><span class="line"></span><br><span class="line">kubectl get pods -o json |jq &apos;.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name&apos; |grep -v null |sort |uniq</span><br><span class="line"></span><br><span class="line"># just get pod&apos;s names</span><br><span class="line">kubectl get pod -l app=nginx -o json |jq &apos;.items[].metadata.name&apos;</span><br><span class="line">kubectl get pods -l app=nginx -o=custom-columns=NAME:.metadata.name</span><br><span class="line">kubectl get pods -l app=nginx -o=name</span><br><span class="line"></span><br><span class="line"># dont&apos;s forget --record</span><br><span class="line">#kubectl rollout pause deployment/scale-deploy</span><br><span class="line">#kubectl set resources deployment/scale-deploy -c=nginx --limits=cpu=200m,memory=512Mi</span><br><span class="line">#kubectl rollout resume deployment/scale-deploy</span><br><span class="line">deployment.apps/nginx-deployment resource requirements updated</span><br><span class="line">kubectl set image deploy scale-deploy nginx=nginx:1.9.1 --record</span><br><span class="line">kubectl rollout history deployment/scale-deploy</span><br><span class="line">kubectl rollout history deployment/scale-deploy --revision=1</span><br><span class="line">kubectl rollout undo deployment/scale-deploy</span><br><span class="line">kubectl rollout undo deployment/scale-deploy --to-revision=2</span><br><span class="line">kubectl scale deployment/scale-deploy --replicas=2</span><br><span class="line">kubectl autoscale deployment/scale-deploy --min=3 --max=4 --cpu-percent=80</span><br><span class="line"></span><br><span class="line"># volume template</span><br><span class="line">cat &gt; test_pod.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pod</span><br><span class="line">spec:</span><br><span class="line">  #initContainers:</span><br><span class="line">  containers:</span><br><span class="line">  - image: nginx</span><br><span class="line">    name: nginx</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /test</span><br><span class="line">      name: secret-volume</span><br><span class="line">    env:</span><br><span class="line">    - name: PASS</span><br><span class="line">      valueFrom:</span><br><span class="line">        secretKeyRef:</span><br><span class="line">          name: test-secret</span><br><span class="line">          key: passwd</span><br><span class="line">  volumes:</span><br><span class="line">  - name: hostpath-volume</span><br><span class="line">    hostPath:</span><br><span class="line">      path: /data</span><br><span class="line">  - name: emptydir-volume</span><br><span class="line">    emptyDir: &#123;&#125;</span><br><span class="line">  - name: secret-volume</span><br><span class="line">    secret:</span><br><span class="line">      secretName: test-secret</span><br><span class="line">EOF</span><br><span class="line">kubectl create --save-config -f test_pod.yaml</span><br><span class="line">kubectl apply --record -f test_pod.yaml</span><br><span class="line"></span><br><span class="line"># initcontainers</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: init-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: workdir</span><br><span class="line">      mountPath: /usr/share/nginx/html</span><br><span class="line">  initContainers:</span><br><span class="line">  - name: touch</span><br><span class="line">    image: busybox:1.28</span><br><span class="line">    command: [&apos;touch&apos;, &apos;/work-dir/index.html&apos;]</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: workdir</span><br><span class="line">      mountPath: &quot;/work-dir&quot;</span><br><span class="line">  volumes:</span><br><span class="line">  - name: workdir</span><br><span class="line">    emptyDir: &#123;&#125;</span><br><span class="line"></span><br><span class="line"># Create a pod that uses secrets</span><br><span class="line">kubectl create secret generic test-secret --from-literal=usename=hua --from-literal=passwd=password --dry-run -o yaml</span><br><span class="line">kubectl create secret generic test-secret --from-literal=usename=hua --from-literal=passwd=password</span><br><span class="line">kubectl get pods --namespace=kube-system kube-flannel-ds-amd64-4mt82 -o yaml &gt; pod_template.yaml</span><br><span class="line">cat &gt; pod-secret.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: question1</span><br><span class="line">  name: pod-secret</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: nginx</span><br><span class="line">    name: nginx</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /mnt/secret</span><br><span class="line">      name: test-secret-vol</span><br><span class="line">  volumes:</span><br><span class="line">  - name: test-secret-vol</span><br><span class="line">    secret:</span><br><span class="line">      secretName: test-secret</span><br><span class="line">EOF</span><br><span class="line">kubectl exec pod-secret -- cat /mnt/secret/passwd</span><br><span class="line">cat &gt; pod_secret_env.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: question1</span><br><span class="line">  name: pod-secret-env</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: nginx</span><br><span class="line">    name: nginx</span><br><span class="line">    env:</span><br><span class="line">    - name: PASS</span><br><span class="line">      valueFrom:</span><br><span class="line">        secretKeyRef:</span><br><span class="line">          name: test-secret</span><br><span class="line">          key: passwd</span><br><span class="line">EOF</span><br><span class="line">kubectl exec pod-secret-env -- env |grep PASS</span><br><span class="line"></span><br><span class="line"># etcd 3</span><br><span class="line">ETCDCTL_API=3 etcdctl --help |grep snap</span><br><span class="line">ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt --key=/etc/kubernetes/pki/apiserver-etcd-client.key member list</span><br><span class="line">ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt --key=/etc/kubernetes/pki/apiserver-etcd-client.key snapshot save snapshot.db</span><br><span class="line">etcdctl --endpoints=https://[127.0.0.1]:2379 --ca-file=/etc/kubernetes/pki/etcd/ca.crt --cert-file=/etc/kubernetes/pki/apiserver-etcd-client.crt --key-file=/etc/kubernetes/pki/apiserver-etcd-client.key cluster-health</span><br><span class="line"></span><br><span class="line"># PV &amp; PVC &amp; Pod</span><br><span class="line">cat &gt; pv.yaml &lt;&lt;EOF</span><br><span class="line">pv + hostpath</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: task-pv-volume</span><br><span class="line">  labels:</span><br><span class="line">    type: local</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 2Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  hostPath:</span><br><span class="line">    path: &quot;/mnt/data&quot;</span><br><span class="line">EOF</span><br><span class="line">cat &gt; pv.yaml &lt;&lt;EOF</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: task-pv-claim</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">EOF</span><br><span class="line">cat &gt; pv.yaml &lt;&lt;EOF</span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: task-pv-pod</span><br><span class="line">spec:</span><br><span class="line">  volumes:</span><br><span class="line">    - name: task-pv-storage</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">       claimName: task-pv-claim</span><br><span class="line">  containers:</span><br><span class="line">    - name: task-pv-container</span><br><span class="line">      image: nginx</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: &quot;http-server&quot;</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - mountPath: &quot;/usr/share/nginx/html&quot;</span><br><span class="line">          name: task-pv-storage</span><br><span class="line">EOF</span><br><span class="line"># NOTE: the following command should be runned in the pod which ship pod</span><br><span class="line">echo &apos;hello&apos; &gt; /mnt/data/index.html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># custom install master (TBD)</span><br><span class="line"># install kube* and etcd binary - https://kubernetes.io/docs/setup/scratch/</span><br><span class="line">wget https://github.com/kubernetes/kubernetes/releases/download/v1.13.0/kubernetes.tar.gz</span><br><span class="line">tar -xf kubernetes.tar.gz</span><br><span class="line">./kubernetes/cluster/get-kube-binaries.sh</span><br><span class="line">tar -xf kubernetes/server/kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">sudo cp kubernetes/server/bin/&#123;kube-apiserver,kube-scheduler,kube-controller-manager,kube-proxy,kubectl,kubelet&#125; /usr/bin/</span><br><span class="line">wget https://github.com/etcd-io/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz</span><br><span class="line">tar -xf etcd-v3.3.10-linux-amd64.tar.gz</span><br><span class="line">sudo cp etcd-v3.3.10-linux-amd64/&#123;etcd,etcdctl&#125; /usr/bin/</span><br><span class="line"></span><br><span class="line"># create cert</span><br><span class="line">sudo -i</span><br><span class="line">mkdir -p /etc/kubernetes &amp;&amp; cd /etc/kubernetes</span><br><span class="line">openssl genrsa -out ca.key</span><br><span class="line">openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=quqi.cluster&quot; -days 5000 -out ca.crt</span><br><span class="line"># openssl x509 -in ca.crt -out ca.pem  #convert CRT to PEM</span><br><span class="line"></span><br><span class="line">#Create key pair for kube-master. NOTE: kube-master should be the same as it&apos;s hostname</span><br><span class="line">openssl genrsa -out server.key</span><br><span class="line">openssl req -new -key server.key -out server.csr -subj &quot;/CN=system:node:172.31.20.224/O=system:nodes&quot;</span><br><span class="line">openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 5000</span><br><span class="line">openssl x509  -noout -text -in ./server.crt</span><br><span class="line"></span><br><span class="line"># Create key pair for every kube-worker, here we will just create one for all-workers</span><br><span class="line">openssl genrsa -out client.key</span><br><span class="line">openssl req -new -key client.key -out client.csr -subj &quot;/CN=system:node:worker/O=system:nodes&quot;</span><br><span class="line">openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 5000</span><br><span class="line">openssl x509  -noout -text -in ./client.crt</span><br><span class="line"></span><br><span class="line"># static pod way</span><br><span class="line">sudo apt install -y docker.io</span><br><span class="line">sudo mkdir -p /etc/kubernetes/manifests</span><br><span class="line">sudo kubelet --register-node=false --pod-manifest-path=/etc/kubernetes/manifests</span><br><span class="line">https://medium.com/containerum/4-ways-to-bootstrap-a-kubernetes-cluster-de0d5150a1e4</span><br><span class="line"></span><br><span class="line"># systemd way - https://medium.com/containerum/4-ways-to-bootstrap-a-kubernetes-cluster-de0d5150a1e4</span><br><span class="line">git clone https://github.com/kubernetes/contrib.git</span><br><span class="line">cd ./contrib/init/systemd/</span><br><span class="line">sudo useradd kube</span><br><span class="line">sudo mv *.service /etc/systemd/system/</span><br><span class="line">sudo mv ./environ/* /etc/kubernetes/</span><br><span class="line">sudo mkdir -p  /var/run/kubernetes</span><br><span class="line">sudo systemctl enable kube-apiserver</span><br><span class="line">sudo systemctl restart kube-apiserver</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"># install etcd - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/</span><br><span class="line">sudo touch /var/log/etcd.log &amp;&amp; sudo chown -R $(id -u) /var/log/etcd.log</span><br><span class="line">sudo etcd --listen-client-urls=http://0.0.0.0:2379 --advertise-client-urls=http://172.31.20.224:2379 &gt;&gt; /var/log/etcd.log 2&gt;&amp;1 &amp;</span><br><span class="line">sudo ETCDCTL_API=2 etcdctl --endpoints=http://172.31.20.224:2379 cluster-health</span><br><span class="line">sudo ETCDCTL_API=3 etcdctl --endpoints=http://172.31.20.224:2379 member list</span><br><span class="line">sudo ETCDCTL_API=3 etcdctl --endpoints=http://172.31.20.224:2379 snapshot save snapshot.save</span><br><span class="line"></span><br><span class="line"># run kube-apiserver</span><br><span class="line">sudo touch /var/log/kube-apiserver.log &amp;&amp; sudo chown -R $(id -u) /var/log/kube-apiserver.log</span><br><span class="line">#sudo kube-apiserver --logtostderr --v=0 --etcd-servers=http://172.31.20.224:2379 --insecure-bind-address=0.0.0.0 --insecure-port=8080 --service-cluster-ip-range=10.244.0.0/16 --admission-control=ServiceAccount,LimitRanger,ResourceQuota --bind-address=0.0.0.0 --secure-port=6443 --client-ca-file=/etc/kubernetes/ca.crt --tls-private-key-file=/etc/kubernetes/server.key --tls-cert-file=/etc/kubernetes/server.crt &gt;&gt; /var/log/kube-apiserver.log 2&gt;&amp;1 &amp;</span><br><span class="line">sudo kube-apiserver --logtostderr --v=0 --etcd-servers=http://172.31.20.224:2379 --insecure-bind-address=0.0.0.0 --insecure-port=8080 --service-cluster-ip-range=10.244.0.0/16 --admission-control=ServiceAccount,LimitRanger,ResourceQuota &gt;&gt; /var/log/kube-apiserver.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"># run kube-controller-manager</span><br><span class="line">sudo touch /var/log/kube-controller-manager.log &amp;&amp; sudo chown -R $(id -u) /var/log/kube-controller-manager.log</span><br><span class="line">#sudo kube-controller-manager --logtostderr --v=0 --master=https://172.31.20.224:6443 --service-account-private-key-file=/etc/kubernetes/server.key --root-ca-file=/etc/kubernetes/ca.crt</span><br><span class="line">sudo kube-controller-manager --logtostderr --v=0 --master=http://172.31.20.224:8080  &gt;&gt; /var/log/kube-controller-manager.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"># run kube-scheduler</span><br><span class="line">sudo touch /var/log/kube-scheduler.log &amp;&amp; sudo chown -R $(id -u) /var/log/kube-scheduler.log</span><br><span class="line">sudo kube-scheduler --logtostderr --v=0 --master=http://172.31.20.224:8080  &gt;&gt; /var/log/kube-scheduler.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"># verity master</span><br><span class="line">kubectl -s http://172.31.20.224:8080 get componentstatus</span><br><span class="line">kubectl -s http://172.31.20.224:8080 get node</span><br><span class="line"></span><br><span class="line"># run kube-proxy, docker and kubelet</span><br><span class="line">sudo touch /var/log/kube-proxy.log &amp;&amp; sudo chown -R $(id -u) /var/log/kube-proxy.log</span><br><span class="line">sudo kube-proxy --logtostderr --v=0 --master=http://172.31.20.224:8080  &gt;&gt; /var/log/kube-proxy.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">sudo apt install docker.io</span><br><span class="line">sudo systemctl enable docker</span><br><span class="line"></span><br><span class="line">kubectl config view</span><br><span class="line">kubectl config set-credentials admin --username=admin --password=password</span><br><span class="line">kubectl config set-cluster quqi.cluster --insecure-skip-tls-verify=true --server=http://172.31.20.224:8080</span><br><span class="line">kubectl config set-context quqi.context --user=admin --namespace=default --cluster=quqi.cluster</span><br><span class="line">kubectl config use-context quqi.context</span><br><span class="line">sudo cp .kube/config /etc/kubernetes/kubeconfig</span><br><span class="line"></span><br><span class="line">sudo touch /var/log/kubelet.log &amp;&amp; sudo chown -R $(id -u) /var/log/kubelet.log</span><br><span class="line">sudo kubelet --logtostderr --v=0 --kubeconfig=/etc/kubernetes/kubeconfig  &gt;&gt; /var/log/kubelet.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"># self-hosted way</span><br><span class="line"># create kubeconfig</span><br><span class="line">NOTE: Put the kubeconfig(s) on every node. eg: in /var/lib/kube-proxy/kubeconfig and /var/lib/kubelet/kubeconfig.</span><br><span class="line">CLUSTER_NAME=quqicluster</span><br><span class="line">CA_CERT=/etc/kubernetes/pki/ca.crt</span><br><span class="line">CLI_CERT=/etc/kubernetes/pki/client.crt</span><br><span class="line">CLI_KEY=/etc/kubernetes/pki/client.key</span><br><span class="line">TOKEN=$(dd if=/dev/urandom bs=128 count=1 2&gt;/dev/null | base64 | tr -d &quot;=+/[:space:]&quot; | dd bs=32 count=1 2&gt;/dev/null)</span><br><span class="line">USER=admin</span><br><span class="line">CONTEXT_NAME=admin_context</span><br><span class="line">MASTER_IP=172.31.29.147</span><br><span class="line">sudo kubectl config set-cluster $CLUSTER_NAME --certificate-authority=$CA_CERT --embed-certs=true --server=https://$MASTER_IP</span><br><span class="line">sudo kubectl config set-credentials $USER --client-certificate=$CLI_CERT --client-key=$CLI_KEY --embed-certs=true --token=$TOKEN</span><br><span class="line">sudo kubectl config set-context $CONTEXT_NAME --cluster=$CLUSTER_NAME --user=$USER</span><br><span class="line">sudo kubectl config use-context $CONTEXT_NAME</span><br><span class="line"></span><br><span class="line"># install docker</span><br><span class="line">#iptables -t nat -F</span><br><span class="line">#ip link set docker0 down</span><br><span class="line">#ip link delete docker0</span><br><span class="line">sudo apt install -y docker.io</span><br><span class="line">sudo systemctl enable docker</span><br><span class="line"></span><br><span class="line"># Install kubelet</span><br><span class="line">sudo mkdir -p /var/lib/kubelet</span><br><span class="line">sudo cp ~/.kube/config /var/lib/kubelet/kubeconfig</span><br><span class="line">sudo kubelet --kubeconfig=/var/lib/kubelet/kubeconfig</span><br><span class="line"></span><br><span class="line">kubelet --kubeconfig=/var/lib/kubelet/kubeconfig --cgroup-driver=cgroupfs --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1</span><br><span class="line"></span><br><span class="line"># Install kube-proxy</span><br><span class="line">sudo mkdir -p /var/lib/kube-proxy</span><br><span class="line">sudo cp ~/.kube/config /var/lib/kube-proxy/kubeconfig</span><br><span class="line">sudo kube-proxy --master=https://$MASTER_IP --kubeconfig=/var/lib/kube-proxy/kubeconfig</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="http://blog.spider.im/2018/06/26/cka-exam/" target="_blank" rel="external">http://blog.spider.im/2018/06/26/cka-exam/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/31/Use-Octavia-to-Implement-HTTPS-Health-Monitors/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/31/Use-Octavia-to-Implement-HTTPS-Health-Monitors/" itemprop="url">Use Octavia to Implement HTTPS Health Monitors</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-31T16:29:50+08:00">
                2018-12-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="几张图感性认识ocatvia"><a href="#几张图感性认识ocatvia" class="headerlink" title="几张图感性认识ocatvia"></a>几张图感性认识ocatvia</h2><p><img src="https://img-blog.csdnimg.cn/20200216105554797.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F1cWk5OQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200216105645469.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F1cWk5OQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200624104629442.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F1cWk5OQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>采用Neutron LBaaS v2实现HTTPS Health Monitors时的配置如下(步骤见附件 - Neutron LBaaS v2)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">backend 52112201-05ce-4f4d-b5a8-9e67de2a895a</span><br><span class="line">    mode tcp</span><br><span class="line">    balance leastconn</span><br><span class="line">    timeout check 10s</span><br><span class="line">    option httpchk GET /</span><br><span class="line">    http-check expect rstatus 200</span><br><span class="line">    option ssl-hello-chk</span><br><span class="line">    server 37a1f5a8-ec7e-4208-9c96-27d2783a594f 192.168.21.13:443 weight 1 check inter 5s fall 2</span><br><span class="line">    server 8e722b4b-08b8-4089-bba5-8fa5dd26a87f 192.168.21.8:443 weight 1 check inter 5s fall 2</span><br></pre></td></tr></table></figure></p>
<p>这种配置会有一个问题, 当使用自定义签名证书时一切正常, 但使用机构颁发的证书时反而有问题.<br>1, 对于ssl check, 严格一点的是check-ssl, 但haproxy没有证书不支持严格的客户端认证, 所以需添加”check check-ssl verify none”参数禁止对客户端参数进行验证. lbaasv2由于久远不支持(那时都还是haproxy 1.7以前必须不支持), ocatavia则有对ssl check的支持.(<a href="https://github.com/openstack/octavia/blob/master/octavia/common/jinja/haproxy/templates/macros.j2" target="_blank" rel="external">https://github.com/openstack/octavia/blob/master/octavia/common/jinja/haproxy/templates/macros.j2</a>)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">154         &#123;% if pool.health_monitor.type == constants.HEALTH_MONITOR_HTTPS %&#125;</span><br><span class="line">155             &#123;% set monitor_ssl_opt = &quot; check-ssl verify none&quot; %&#125;</span><br></pre></td></tr></table></figure></p>
<p>下面的配置works<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">backend 79024d4d-4de4-492c-a3e2-21730b096a37</span><br><span class="line">    mode tcp</span><br><span class="line">    balance roundrobin</span><br><span class="line">    timeout check 10s</span><br><span class="line">    option httpchk GET /</span><br><span class="line">    http-check expect rstatus 200</span><br><span class="line">    fullconn 1000000</span><br><span class="line">    option allbackups</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout server 50000</span><br><span class="line">    server 317e4dea-5a62-4df0-a2f1-ea7bad4a9c5d 192.168.21.7:443 weight 1 check check-ssl verify none inter 5s fall 3 rise 4</span><br></pre></td></tr></table></figure></p>
<p>但是似乎ocatavia的client有点问题, 它设置出来的是:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    option httpchk None None</span><br><span class="line">    http-check expect rstatus None</span><br><span class="line">    server 317e4dea-5a62-4df0-a2f1-ea7bad4a9c5d 192.168.21.7:443 weight 1 check check-ssl verify none inter 5s fall 3 rise 4</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~/ca3$ openstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type HTTPS --name https-monitor --http-method GET --url-path / --expected-codes 200 pool1</span><br><span class="line">http_method is not a valid option for health monitors of type HTTPS (HTTP 400) (Request-ID: req-2d81bafa-1240-4f73-8e2e-cb0dd7691fdb)</span><br></pre></td></tr></table></figure></p>
<p>2, ssl backend side采用了严格的客户端认证的话, 需改用TLS-HELLO check (<a href="https://docs.openstack.org/octavia/latest/user/guides/basic-cookbook.html)其实已经有说明" target="_blank" rel="external">https://docs.openstack.org/octavia/latest/user/guides/basic-cookbook.html)其实已经有说明</a>, 如下:<br>HTTPS health monitors operate exactly like HTTP health monitors, but with ssl back-end servers. Unfortunately, this causes problems if the servers are performing client certificate validation, as HAProxy won’t have a valid cert. In this case, using TLS-HELLO type monitoring is an alternative.<br>TLS-HELLO health monitors simply ensure the back-end server responds to SSLv3 client hello messages. It will not check any other health metrics, like status code or body contents.<br><a href="https://review.openstack.org/#/c/475944/" target="_blank" rel="external">https://review.openstack.org/#/c/475944/</a></p>
<p>实际上客户并未使用客户端认证, 所以不是上面的原因, 应该是SNI所致. 因为后端有SNI认证, haproxy端需传入hostname, 但haproxy端无法传入hostname, 所以出错.<strong>但octavia-worker应该可以传SNI到后端</strong>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@zhhuabj-bastion:~/ca3$ curl --cacert ca.crt https://10.5.150.5</span><br><span class="line">curl: (51) SSL: certificate subject name (www.server1.com) does not match target host name &apos;10.5.150.5&apos;</span><br><span class="line">ubuntu@zhhuabj-bastion:~/ca3$ curl --resolve www.server1.com:443:10.5.150.5 -k https://www.server1.com</span><br><span class="line">test1</span><br></pre></td></tr></table></figure></p>
<p>什么是SNI, 就是ssl server端可能会根据每个节点的hostname生成不同的cert, 并启用SNI. 这样ssl client访问ssl server端时也应该将hostname也传过去. (注: SNIProxy是一个适用于 HTTPS 和 HTTP 的类似于透明代理的反向代理工具。它可以在 TCP 层直接将流量在不解包的情况下转发出来，实现不需要在代理服务器配置证书就能反向代理 HTTPS 网站的功能)<br>‘curl -k’的方式测试无法很好的测试SNI, 最好是通过’openstack s_client’测试:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#Ideal Test to connect with SSLv3/No SNI</span><br><span class="line">openssl s_client -ssl3 -connect 103.245.215.4:443</span><br><span class="line">#openssl s_client -connect 192.168.254.214:9443 | openssl x509 -noout -text | grep DNS</span><br><span class="line">#We can also send SNI using -servername:</span><br><span class="line">openssl s_client -ssl3 -servername CERT_HOSTNAME -connect 103.245.215.4:443</span><br></pre></td></tr></table></figure></p>
<p>这个文档 (<a href="https://cbonte.github.io/haproxy-dconv/1.7/configuration.html#option%20ssl-hello-chk" target="_blank" rel="external">https://cbonte.github.io/haproxy-dconv/1.7/configuration.html#option%20ssl-hello-chk</a> )指出ssl-hello-chk只检查SSLv3不检查HTTP, 事实上, HTTPS health check也不检查HTTP只是多了个SSL negotiation. check-ssl check似乎能做更多.<br>LBaas v2模板目前只支持”httpchk”与”ssl-hello-chk”, 这只有SSL check, 没有HTTP check. 所以问题很可能是出在SSLv3 hello (without SNI)有问题.做个SNI相关的实验验证一下:<br>首先, charm应该将ca传给octavia, ocavia应该根据ca再去创建SNI证书, 并且传SNI证书到backend, octavia-worker的相关处理代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">    LOG.debug(&quot;request url %s&quot;, path)</span><br><span class="line">    _request = getattr(self.session, method.lower())</span><br><span class="line">    _url = self._base_url(amp.lb_network_ip) + path</span><br><span class="line">    LOG.debug(&quot;request url %s&quot;, _url)</span><br><span class="line">    reqargs = &#123;</span><br><span class="line">        &apos;verify&apos;: CONF.haproxy_amphora.server_ca,</span><br><span class="line">        &apos;url&apos;: _url,</span><br><span class="line">        &apos;timeout&apos;: (req_conn_timeout, req_read_timeout), &#125;</span><br><span class="line">    reqargs.update(kwargs)</span><br><span class="line">    headers = reqargs.setdefault(&apos;headers&apos;, &#123;&#125;)</span><br><span class="line">    headers[&apos;User-Agent&apos;] = OCTAVIA_API_CLIENT</span><br><span class="line">    self.ssl_adapter.uuid = amp.id</span><br><span class="line">    exception = None</span><br><span class="line">    # Keep retrying</span><br><span class="line"></span><br><span class="line">def get_create_amphora_flow(self):</span><br><span class="line">    &quot;&quot;&quot;Creates a flow to create an amphora.</span><br><span class="line"></span><br><span class="line">    :returns: The flow for creating the amphora</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    create_amphora_flow = linear_flow.Flow(constants.CREATE_AMPHORA_FLOW)</span><br><span class="line">    create_amphora_flow.add(database_tasks.CreateAmphoraInDB(</span><br><span class="line">                            provides=constants.AMPHORA_ID))</span><br><span class="line">    create_amphora_flow.add(lifecycle_tasks.AmphoraIDToErrorOnRevertTask(</span><br><span class="line">        requires=constants.AMPHORA_ID))</span><br><span class="line">    if self.REST_AMPHORA_DRIVER:</span><br><span class="line">        create_amphora_flow.add(cert_task.GenerateServerPEMTask(</span><br><span class="line"></span><br><span class="line">        create_amphora_flow.add(</span><br><span class="line">            database_tasks.UpdateAmphoraDBCertExpiration(</span><br><span class="line">                requires=(constants.AMPHORA_ID, constants.SERVER_PEM)))</span><br><span class="line"></span><br><span class="line">        create_amphora_flow.add(compute_tasks.CertComputeCreate(</span><br><span class="line">            requires=(constants.AMPHORA_ID, constants.SERVER_PEM,</span><br><span class="line">                      constants.BUILD_TYPE_PRIORITY, constants.FLAVOR),</span><br><span class="line">            provides=constants.COMPUTE_ID))</span><br></pre></td></tr></table></figure></p>
<p>1, 两个证书, 略. lb_tls_secret_1的hostname是www.server1.com, lb_tls_secret_2的hostname是www.server2.com<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">secret1_id=$(openstack secret list |grep lb_tls_secret_1 |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">secret2_id=$(openstack secret list |grep lb_tls_secret_2 |awk &apos;&#123;print $2&#125;&apos;)</span><br></pre></td></tr></table></figure></p>
<p>2, 创建listener时使用( –sni-container-refs $secret1_id $secret2_id )加入了两个域名的SNI<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">IP=192.168.21.7</span><br><span class="line">secret1_id=$(openstack secret list |grep lb_tls_secret_1 |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">secret2_id=$(openstack secret list |grep lb_tls_secret_2 |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">subnetid=$(openstack subnet show private_subnet -f value -c id); echo $subnetid</span><br><span class="line">lb_id=$(openstack loadbalancer show lb3 -f value -c id)</span><br><span class="line">#lb_id=$(openstack loadbalancer create --name test_tls_termination --vip-subnet-id $subnetid -f value -c id); echo $lb_id</span><br><span class="line">listener_id=$(openstack loadbalancer listener create $lb_id --name https_listener --protocol-port 443 --protocol TERMINATED_HTTPS --default-tls-container=$secret1_id --sni-container-refs $secret1_id $secret2_id -f value -c id); echo $listener_id</span><br><span class="line">pool_id=$(openstack loadbalancer pool create --protocol HTTP --listener $listener_id --lb-algorithm ROUND_ROBIN -f value -c id); echo $pool_id</span><br><span class="line">openstack loadbalancer member create --address $&#123;IP&#125; --subnet-id $subnetid --protocol-port 80 $pool_id</span><br><span class="line"></span><br><span class="line">public_network=$(openstack network show ext_net -f value -c id)</span><br><span class="line">fip=$(openstack floating ip create $public_network -f value -c floating_ip_address)</span><br><span class="line">vip=$(openstack loadbalancer show $lb_id -c vip_address -f value)</span><br><span class="line">vip_port=$(openstack port list --fixed-ip ip-address=$vip -c ID -f value)</span><br><span class="line">openstack floating ip set $fip --fixed-ip-address $vip --port $vip_port</span><br></pre></td></tr></table></figure></p>
<p>3, 测试, client传入www.server1.com或www.server2.com两个域名时, server端能正常响应, 但传入一个域名www.server3.com时就报了这个错:’does not match target host name ‘www.server3.com’’</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@zhhuabj-bastion:~/ca3$ curl --resolve www.server1.com:443:10.5.150.5 --cacert ca.crt https://www.server1.com</span><br><span class="line">Hello World!</span><br><span class="line">ubuntu@zhhuabj-bastion:~/ca3$ curl --resolve www.server2.com:443:10.5.150.5 --cacert ca.crt https://www.server2.com</span><br><span class="line">Hello World!</span><br><span class="line">ubuntu@zhhuabj-bastion:~/ca3$ curl --resolve www.server3.com:443:10.5.150.5 --cacert ca.crt https://www.server3.com</span><br><span class="line">curl: (51) SSL: certificate subject name (www.server1.com) does not match target host name &apos;www.server3.com&apos;</span><br></pre></td></tr></table></figure>
<p>为什么会这样呢?</p>
<p>LBaaS v2中的ssl check将在haproxy中添加下列配置, 实际上有ssl-hello-chk时httpchk将被覆盖(haproxy忽略的). haproxy 1.7开始添加了更高级的check-ssl(xenial使用haproxy 1.6, 不支持), 估计就是早期的lbaas为ssl check添加ssl-hello-chk的原因<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mode tcp</span><br><span class="line">option httpchk GET /</span><br><span class="line">http-check expect rstatus 303</span><br><span class="line">option ssl-hello-chk</span><br></pre></td></tr></table></figure></p>
<p>haproxy(<a href="http://cbonte.github.io/haproxy-dconv/1.6/configuration.html#4-option%20ssl-hello-chk)将为ssl-hello-chk伪造SSLv3包" target="_blank" rel="external">http://cbonte.github.io/haproxy-dconv/1.6/configuration.html#4-option%20ssl-hello-chk)将为ssl-hello-chk伪造SSLv3包</a>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">When some SSL-based protocols are relayed in TCP mode through HAProxy, it is</span><br><span class="line">possible to test that the server correctly talks SSL instead of just testing</span><br><span class="line">that it accepts the TCP connection. When &quot;option ssl-hello-chk&quot; is set, pure</span><br><span class="line">SSLv3 client hello messages are sent once the connection is established to</span><br><span class="line">the server, and the response is analyzed to find an SSL server hello message.</span><br><span class="line">The server is considered valid only when the response contains this server</span><br><span class="line">hello message.</span><br><span class="line">All servers tested till there correctly reply to SSLv3 client hello messages,</span><br><span class="line">and most servers tested do not even log the requests containing only hello</span><br><span class="line">messages, which is appreciable.</span><br><span class="line">Note that this check works even when SSL support was not built into haproxy</span><br><span class="line">because it forges the SSL message. When SSL support is available, it is best</span><br><span class="line">to use native SSL health checks instead of this one.</span><br></pre></td></tr></table></figure></p>
<p>这是haproxy相关处理的源代码, 它没使用SSL libray, 先发硬编码的SSLv3 hello消息, 然后从response里找0x15 (SSL3_RT_ALERT) or 0x16 (SSL3_RT_HANDSHAKE), 若没找着就返回HCHK_STATUS_L6RSP(Layer6 invalid response) - <a href="https://github.com/haproxy/haproxy/blob/master/src/checks.c#L915" target="_blank" rel="external">https://github.com/haproxy/haproxy/blob/master/src/checks.c#L915</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">case PR_O2_SSL3_CHK:</span><br><span class="line">	if (!done &amp;&amp; b_data(&amp;check-&gt;bi) &lt; 5)</span><br><span class="line">		goto wait_more_data;</span><br><span class="line"></span><br><span class="line">	/* Check for SSLv3 alert or handshake */</span><br><span class="line">	if ((b_data(&amp;check-&gt;bi) &gt;= 5) &amp;&amp; (*b_head(&amp;check-&gt;bi) == 0x15 || *b_head(&amp;check-&gt;bi) == 0x16))</span><br><span class="line">		set_server_check_status(check, HCHK_STATUS_L6OK, NULL);</span><br><span class="line">	else</span><br><span class="line">		set_server_check_status(check, HCHK_STATUS_L6RSP, NULL);</span><br><span class="line">	break;</span><br></pre></td></tr></table></figure></p>
<p>错误’Layer6 invalid response’正是从客户日志中看到的:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Oct 25 04:50:34 neut002 haproxy[54990]: Server aaa0a533-073b-4b0f-8b81-777b6a8f3900/f2dc685f-58f7-4201-8060-3409d2d73a0d is DOWN, reason: Layer6 invalid response, check duration: 4ms. 0 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.</span><br></pre></td></tr></table></figure></p>
<p>所以backend ssl server端应该返回0x15, 客户究竟在haproxy之前运行什么ssl backend端, 我们不清楚. 假设它们运行的是apache2. 我们搭建一个测试环境, apache2采用默认的tls1.2, 而haproxy里还使用老的sslv3 hello时, apache2 ssl backend将返回下列的ssl协商错误:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@zhhuabj-bastion:~/ca3$ openssl s_client -ssl3 -connect www.server1.com:443</span><br><span class="line">140306875094680:error:140A90C4:SSL routines:SSL_CTX_new:null ssl method passed:ssl_lib.c:1878:</span><br><span class="line">ubuntu@zhhuabj-bastion:~/ca3$ openssl s_client -ssl3 -servername www.server1.com -connect www.server1.com:443</span><br><span class="line">139626113296024:error:140A90C4:SSL routines:SSL_CTX_new:null ssl method passed:ssl_lib.c:1878:</span><br><span class="line">ubuntu@zhhuabj-bastion:~/ca3$ openssl s_client -ssl3 -servername www.server2.com -connect www.server1.com:443</span><br><span class="line">140564176807576:error:140A90C4:SSL routines:SSL_CTX_new:null ssl method passed:ssl_lib.c:1878:</span><br></pre></td></tr></table></figure></p>
<p>为什么ssl backend端不返回0x15或0x16呢, 理论上可能有以下几个原因<br>a, SSLv3现在已经被废弃了, 主流http server已经禁用了SSLv3支持, apach2收到haproxy过来的SSLv3 hello包时, apache2的SSL实现可能会响应别的消息而不是0x15/0x15<br>b, 因为haproxy过来的SSLv3 hello请求里没有SNI, 这样若启用了SNI的backend端(如apache2)就会ssl协商失败了, 这样也就未返回0x15/0x16<br>c, 其他原因<br>具体原因还需继续在backend抓包(tcpdump -eni ens3 -w ssl-test.pcap -s 0 port 443 or port 8443)确认.</p>
<p><strong>更新, 原因已找到:</strong><br>octavia/0会创建amphorae service vm, octavia/0上的/usr/lib/python3/dist-packages/octavia/amphorae/drivers/haproxy/rest_api_driver.py采用python requests模块去连接service vm上的9443端口. 这块代码类似下面这句所以不work:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl --cacert /etc/octavia/certs/issuing_ca.pem 192.168.254.31:9443</span><br></pre></td></tr></table></figure></p>
<p>改试下面的都work , 其中26835e25-7c4f-4776-940f-209eb9a9e826是SNI (loadbalance-id).<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl -k 192.168.254.31:9443</span><br><span class="line">openssl s_client -connect 192.168.254.31:9443 -key /etc/octavia/certs/issuing_ca_key.pem</span><br><span class="line">curl --cacert /etc/octavia/certs/issuing_ca.pem https://26835e25-7c4f-4776-940f-209eb9a9e826:9443 --resolve 26835e25-7c4f-4776-940f-209eb9a9e826:9443:192.168.254.31</span><br><span class="line"></span><br><span class="line">#ipv6</span><br><span class="line">#tlsv13 alert certificate required, it shows ssh client verfication is required</span><br><span class="line">curl -6 -k https://[fc00:ea96:ae23:2d4f:f816:3eff:fe76:d87a]:9443/</span><br><span class="line">#no alternative certificate subject name matches target host name, it shows it&apos;s about sni</span><br><span class="line">curl -6 --cacert /etc/octavia/certs/issuing_ca.pem https://[fc00:ea96:ae23:2d4f:f816:3eff:fe76:d87a]:9443/</span><br><span class="line">curl -6 --cacert /etc/octavia/certs/issuing_ca.pem --resolve backend1.domain:9443:[fc00:ea96:ae23:2d4f:f816:3eff:fe76:d87a] https://backend1.domain:9443 -v</span><br></pre></td></tr></table></figure></p>
<p>在octavia/0上测试:<br>openssl s_client -connect 192.168.254.31:9443 -key /etc/octavia/certs/issuing_ca_key.pem<br>最后的原因是, 创建证书时未指明CN=$DOMAIN1:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SUBJECT=&quot;/C=CN/ST=BJ/L=BJ/O=STS/OU=Joshua/CN=$DOMAIN1&quot;</span><br><span class="line">openssl req -new -nodes -subj $SUBJECT -key $DOMAIN1.key -out $DOMAIN1.csr</span><br></pre></td></tr></table></figure>
<p>我们知道, python requests module中的assert_hostname用来往backend传递SNI, 见 - <a href="https://medium.com/@yzlin/python-requests-ssl-ip-binding-6df25a9a8f6a" target="_blank" rel="external">https://medium.com/@yzlin/python-requests-ssl-ip-binding-6df25a9a8f6a</a><br>而下列代码(<a href="https://github.com/openstack/octavia/blob/master/octavia/amphorae/drivers/haproxy/rest_api_driver.py#L542" target="_blank" rel="external">https://github.com/openstack/octavia/blob/master/octavia/amphorae/drivers/haproxy/rest_api_driver.py#L542</a>), 将传self.uuid(self.ssl_adapter.uuid = amp.id)到conn.asser_hostname.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class CustomHostNameCheckingAdapter(requests.adapters.HTTPAdapter):</span><br><span class="line">    def cert_verify(self, conn, url, verify, cert):</span><br><span class="line">        conn.assert_hostname = self.uuid</span><br><span class="line">        return super(CustomHostNameCheckingAdapter,</span><br><span class="line">                     self).cert_verify(conn, url, verify, cert)</span><br></pre></td></tr></table></figure></p>
<p>见设计文档: <a href="https://docs.openstack.org/octavia/ocata/specs/version0.5/tls-data-security.html" target="_blank" rel="external">https://docs.openstack.org/octavia/ocata/specs/version0.5/tls-data-security.html</a></p>
<h2 id="安装Octavia"><a href="#安装Octavia" class="headerlink" title="安装Octavia"></a>安装Octavia</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./generate-bundle.sh --name octavia --create-model --run --octavia -r stein --dvr-snat --num-compute 2   #can also use br-data:ens7 here</span><br><span class="line">juju config neutron-openvswitch data-port=&quot;br-data:ens7&quot;</span><br><span class="line">./bin/add-data-ports.sh                      #it will add another NIC ens7 for every nova-compute nodes</span><br></pre></td></tr></table></figure>
<p>或者:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># https://blog.ubuntu.com/2019/01/28/taking-octavia-for-a-ride-with-kubernetes-on-openstack</span><br><span class="line">sudo snap install --classic charm</span><br><span class="line">charm pull cs:openstack-base</span><br><span class="line">cd openstack-base/</span><br><span class="line">curl https://raw.githubusercontent.com/openstack-charmers/openstack-bundles/master/stable/overlays/loadbalancer-octavia.yaml -o loadbalancer-octavia.yaml</span><br><span class="line">juju deploy ./bundle.yaml --overlay loadbalancer-octavia.yaml</span><br></pre></td></tr></table></figure></p>
<p>或者<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">sudo bash -c &apos;cat &gt;overlays/octavia.yaml&quot; &lt;&lt;EOF</span><br><span class="line">debug:                      &amp;debug                     True</span><br><span class="line">openstack_origin:           &amp;openstack_origin          cloud:bionic-rocky</span><br><span class="line">applications:</span><br><span class="line">  octavia:</span><br><span class="line">    #series: bionic</span><br><span class="line">    charm: cs:octavia</span><br><span class="line">    num_units: 1</span><br><span class="line">    constraints: mem=2G</span><br><span class="line">    options:</span><br><span class="line">      debug: *debug</span><br><span class="line">      openstack-origin: *openstack_origin</span><br><span class="line">  octavia-dashboard:</span><br><span class="line">    charm: cs:octavia-dashboard</span><br><span class="line">relations:</span><br><span class="line">- - mysql:shared-db</span><br><span class="line">  - octavia:shared-db</span><br><span class="line">- - keystone:identity-service</span><br><span class="line">  - octavia:identity-service</span><br><span class="line">- - rabbitmq-server:amqp</span><br><span class="line">  - octavia:amqp</span><br><span class="line">- - neutron-api:neutron-load-balancer</span><br><span class="line">  - octavia:neutron-api</span><br><span class="line">- - neutron-openvswitch:neutron-plugin</span><br><span class="line">  - octavia:neutron-openvswitch</span><br><span class="line">- - openstack-dashboard:dashboard-plugin</span><br><span class="line">  - octavia-dashboard:dashboard</span><br><span class="line">EOF</span><br><span class="line">sudo bash -c &apos;cat &gt;overlays/octavia.yaml&quot; &lt;&lt;EOF</span><br><span class="line">debug:                      &amp;debug                     True</span><br><span class="line">verbose:                    &amp;verbose                   True</span><br><span class="line">openstack_origin:           &amp;openstack_origin</span><br><span class="line">applications:</span><br><span class="line">  barbican:</span><br><span class="line">    charm: cs:~openstack-charmers-next/barbican</span><br><span class="line">    num_units: 1</span><br><span class="line">    constraints: mem=1G</span><br><span class="line">    options:</span><br><span class="line">      debug: *debug</span><br><span class="line">      openstack-origin: *openstack_origin</span><br><span class="line">relations:</span><br><span class="line">  - [ barbican, rabbitmq-server ]</span><br><span class="line">  - [ barbican, mysql ]</span><br><span class="line">  - [ barbican, keystone ]</span><br><span class="line">EOF</span><br><span class="line">./generate-bundle.sh --series bionic --release rocky --barbican</span><br><span class="line">juju deploy ./b/openstack.yaml --overlay ./b/o/barbican.yaml --overlay overlays/octavia.yaml</span><br></pre></td></tr></table></figure></p>
<p>或者:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">juju add-model bionic-barbican-octavia</span><br><span class="line">./generate-bundle.sh --series bionic --barbican</span><br><span class="line">#./generate-bundle.sh --series bionic --release rocky --barbican</span><br><span class="line">juju deploy ./b/openstack.yaml --overlay ./b/o/barbican.yaml</span><br><span class="line">#https://github.com/openstack-charmers/openstack-bundles/blob/master/stable/overlays/loadbalancer-octavia.yaml</span><br><span class="line">#NOTE: need to comment to:lxd related lines from loadbalancer-octavia.yaml, and change nova-compute num to 3</span><br><span class="line">juju deploy ./b/openstack.yaml --overlay ./overlays/loadbalancer-octavia.yaml</span><br><span class="line"></span><br><span class="line"># Or we can:</span><br><span class="line"># 2018-12-25 03:30:39 DEBUG update-status fatal error: runtime: out of memory</span><br><span class="line">juju deploy octavia --config openstack-origin=cloud:bionic:queens --constraints mem=4G</span><br><span class="line">juju deploy octavia-dashboard</span><br><span class="line">juju add-relation octavia-dashboard openstack-dashboard</span><br><span class="line">juju add-relation octavia rabbitmq-server</span><br><span class="line">juju add-relation octavia mysql</span><br><span class="line">juju add-relation octavia keystone</span><br><span class="line">juju add-relation octavia neutron-openvswitch</span><br><span class="line">juju add-relation octavia neutron-api</span><br><span class="line"></span><br><span class="line"># Initialize and unseal vault</span><br><span class="line"># https://docs.openstack.org/project-deploy-guide/charm-deployment-guide/latest/app-vault.html</span><br><span class="line"># https://lingxiankong.github.io/2018-07-16-barbican-introduction.html</span><br><span class="line"># /snap/vault/1315/bin/vault server -config /var/snap/vault/common/vault.hcl</span><br><span class="line">sudo snap install vault</span><br><span class="line">export VAULT_ADDR=&quot;http://$(juju run --unit vault/0 unit-get private-address):8200&quot;</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ vault operator init -key-shares=5 -key-threshold=3</span><br><span class="line">Unseal Key 1: UB7XDri5FRcMLirKBIysdUb2PN7Ia5EVMP0Z9wD9Hyll</span><br><span class="line">Unseal Key 2: mD8Gnr3hdB2LjjNB4ugxvvsvb8+EQQ/0AXm2p+c2qYFT</span><br><span class="line">Unseal Key 3: vymYLAdou3qky24IEKDufYsZXAIPLWtErAKy/RkfgghS</span><br><span class="line">Unseal Key 4: xOwDbqgNLLipsZbp+FAmVhBc3ZxA8CI3DchRc4AClRyQ</span><br><span class="line">Unseal Key 5: nRlZ8WX6CS9nOw2ct5U9o0Za5jlUAtjN/6XLxjf62CnR</span><br><span class="line">Initial Root Token: s.VJKGhNvIFCTgHVbQ6WvL0OLe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vault operator unseal UB7XDri5FRcMLirKBIysdUb2PN7Ia5EVMP0Z9wD9Hyll</span><br><span class="line">vault operator unseal mD8Gnr3hdB2LjjNB4ugxvvsvb8+EQQ/0AXm2p+c2qYFT</span><br><span class="line">vault operator unseal vymYLAdou3qky24IEKDufYsZXAIPLWtErAKy/RkfgghS</span><br><span class="line">export VAULT_TOKEN=s.VJKGhNvIFCTgHVbQ6WvL0OLe</span><br><span class="line">vault token create -ttl=10m</span><br><span class="line">$ vault token create -ttl=10m</span><br><span class="line">Key                  Value</span><br><span class="line">---                  -----</span><br><span class="line">token                s.7ToXh9HqE6FiiJZybFhevL9v</span><br><span class="line">token_accessor       6dPkFpsPmx4D7g8yNJXvEpKN</span><br><span class="line">token_duration       10m</span><br><span class="line">token_renewable      true</span><br><span class="line">token_policies       [&quot;root&quot;]</span><br><span class="line">identity_policies    []</span><br><span class="line">policies             [&quot;root&quot;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Authorize vault charm to use a root token to be able to create secrets storage back-ends and roles to allow other app to access vault</span><br><span class="line">juju run-action vault/0 authorize-charm token=s.7ToXh9HqE6FiiJZybFhevL9v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># upload Amphora image</span><br><span class="line">source ~/stsstack-bundles/openstack/novarc</span><br><span class="line">http_proxy=http://squid.internal:3128 wget http://tarballs.openstack.org/octavia/test-images/test-only-amphora-x64-haproxy-ubuntu-xenial.qcow2</span><br><span class="line">#openstack image create --tag octavia-amphora --disk-format=qcow2 --container-format=bare --private amphora-haproxy-xenial --file ./test-only-amphora-x64-haproxy-ubuntu-xenial.qcow2</span><br><span class="line">glance image-create --tag octavia-amphora --disk-format qcow2 --name amphora-haproxy-xenial --file ./test-only-amphora-x64-haproxy-ubuntu-xenial.qcow2 --visibility public --container-format bare --progress</span><br><span class="line"></span><br><span class="line">cd stsstack-bundles/openstack/</span><br><span class="line">./configure</span><br><span class="line">./tools/sec_groups.sh</span><br><span class="line">./tools/instance_launch.sh 2 xenial</span><br><span class="line">neutron floatingip-create ext_net</span><br><span class="line">neutron floatingip-associate $(neutron floatingip-list |grep 10.5.150.4 |awk &apos;&#123;print $2&#125;&apos;) $(neutron port-list |grep &apos;192.168.21.3&apos; |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">or</span><br><span class="line">fix_ip=192.168.21.3</span><br><span class="line">public_network=$(openstack network show ext_net -f value -c id)</span><br><span class="line">fip=$(openstack floating ip create $public_network -f value -c floating_ip_address)</span><br><span class="line">openstack floating ip set $fip --fixed-ip-address $fix_ip --port $(openstack port list --fixed-ip ip-address=$fix_ip -c id -f value)</span><br><span class="line"></span><br><span class="line">cd ~/ca  #https://docs.openstack.org/project-deploy-guide/charm-deployment-guide/latest/app-octavia.html</span><br><span class="line">juju config octavia \</span><br><span class="line">    lb-mgmt-issuing-cacert=&quot;$(base64 controller_ca.pem)&quot; \</span><br><span class="line">    lb-mgmt-issuing-ca-private-key=&quot;$(base64 controller_ca_key.pem)&quot; \</span><br><span class="line">    lb-mgmt-issuing-ca-key-passphrase=foobar \</span><br><span class="line">    lb-mgmt-controller-cacert=&quot;$(base64 controller_ca.pem)&quot; \</span><br><span class="line">    lb-mgmt-controller-cert=&quot;$(base64 controller_cert_bundle.pem)&quot;</span><br><span class="line"></span><br><span class="line">注: 也遇到一个问题, 上述命令没有更新service vm里的cert, ssh登录service vm之后查看/etc/octavia/certs/目录发现证书不同. 证书不对, 会导致service vm里的amphora-agent在9443端口起不来. service vm是通过cloud-init来写的证书, 那错误出在哪个环节呢?</span><br></pre></td></tr></table></figure></p>
<p>配置资源:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># the code search &apos;configure_resources&apos;</span><br><span class="line">juju config octavia create-mgmt-network</span><br><span class="line">juju run-action --wait octavia/0 configure-resources</span><br><span class="line"></span><br><span class="line"># some deubg ways:</span><br><span class="line">openstack security group rule create $(openstack security group show lb-mgmt-sec-grp -f value -c id) --protocol udp --dst-port 546 --ethertype IPv6</span><br><span class="line">openstack security group rule create $(openstack security group show lb-mgmt-sec-grp -f value -c id) --protocol icmp --ethertype IPv6</span><br><span class="line">neutron security-group-rule-create --protocol icmpv6 --direction egress --ethertype IPv6 lb-mgmt-sec-grp</span><br><span class="line">neutron security-group-rule-create --protocol icmpv6 --direction ingress --ethertype IPv6 lb-mgmt-sec-grp</span><br><span class="line"></span><br><span class="line">neutron port-show octavia-health-manager-octavia-0-listen-port -f value -c status</span><br><span class="line">neutron port-update --admin-state-up True octavia-health-manager-octavia-0-listen-port</span><br><span class="line">AGENT=$(neutron l3-agent-list-hosting-router lb-mgmt -f value -c id)</span><br><span class="line">neutron l3-agent-router-remove $AGENT lb-mgmt</span><br><span class="line">neutron l3-agent-router-add $AGENT lb-mgmt</span><br></pre></td></tr></table></figure></p>
<p>上面configure-resources命令 (juju run-action –wait octavia/0 configure-resources)将会自动配置IPv6管理网段, 并且会配置一个binding:host在octavia/0节点上的名为octavia-health-manager-octavia-0-listen-port的port.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@zhhuabj-bastion:~$ neutron router-list |grep mgmt</span><br><span class="line">| 0a839377-6b19-419b-9868-616def4d749f | lb-mgmt         | null                                                                                                                                                                                    | False       | False |</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ neutron net-list |grep mgmt</span><br><span class="line">| ae580dc8-31d6-4ec3-9d44-4a9c7b9e80b6 | lb-mgmt-net | ea9c7d5c-d224-4dd3-b40c-3acae9690657 fc00:4a9c:7b9e:80b6::/64 |</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ neutron subnet-list |grep mgmt</span><br><span class="line">| ea9c7d5c-d224-4dd3-b40c-3acae9690657 | lb-mgmt-subnetv6 | fc00:4a9c:7b9e:80b6::/64 | &#123;&quot;start&quot;: &quot;fc00:4a9c:7b9e:80b6::2&quot;, &quot;end&quot;: &quot;fc00:4a9c:7b9e:80b6:ffff:ffff:ffff:ffff&quot;&#125; |</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ neutron port-list |grep fc00</span><br><span class="line">| 5cb6e3f3-ebe5-4284-9c05-ea272e8e599b |                                                      | fa:16:3e:9e:82:6a | &#123;&quot;subnet_id&quot;: &quot;ea9c7d5c-d224-4dd3-b40c-3acae9690657&quot;, &quot;ip_address&quot;: &quot;fc00:4a9c:7b9e:80b6::1&quot;&#125;                  |</span><br><span class="line">| 983c56d2-46dd-416c-abc8-5096d76f75e2 | octavia-health-manager-octavia-0-listen-port         | fa:16:3e:99:8c:ab | &#123;&quot;subnet_id&quot;: &quot;ea9c7d5c-d224-4dd3-b40c-3acae9690657&quot;, &quot;ip_address&quot;: &quot;fc00:4a9c:7b9e:80b6:f816:3eff:fe99:8cab&quot;&#125; |</span><br><span class="line">| af38a60d-a370-4ddb-80ac-517fda175535 |                                                      | fa:16:3e:5f:cd:ae | &#123;&quot;subnet_id&quot;: &quot;ea9c7d5c-d224-4dd3-b40c-3acae9690657&quot;, &quot;ip_address&quot;: &quot;fc00:4a9c:7b9e:80b6:f816:3eff:fe5f:cdae&quot;&#125; |</span><br><span class="line">| b65f90d1-2e1f-4994-a0e9-2bb13ead4cab |                                                      | fa:16:3e:10:34:84 | &#123;&quot;subnet_id&quot;: &quot;ea9c7d5c-d224-4dd3-b40c-3acae9690657&quot;, &quot;ip_address&quot;: &quot;fc00:4a9c:7b9e:80b6:f816:3eff:fe10:3484&quot;&#125; |</span><br></pre></td></tr></table></figure></p>
<p>并且在octavia/0上会创建一个名为o-hm0的接口, 此接口的IP地址与octavia-health-manager-octavia-0-listen-port port同.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@zhhuabj-bastion:~$ juju ssh octavia/0 -- ip addr show o-hm0 |grep global</span><br><span class="line">Connection to 10.5.0.110 closed.</span><br><span class="line">    inet6 fc00:4a9c:7b9e:80b6:f816:3eff:fe99:8cab/64 scope global dynamic mngtmpaddr noprefixroute</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ juju ssh octavia/0 -- sudo ovs-vsctl show</span><br><span class="line">490bbb36-1c7d-412d-8b44-31e6f796306a</span><br><span class="line">    Manager &quot;ptcp:6640:127.0.0.1&quot;</span><br><span class="line">        is_connected: true</span><br><span class="line">    Bridge br-tun</span><br><span class="line">        Controller &quot;tcp:127.0.0.1:6633&quot;</span><br><span class="line">            is_connected: true</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port &quot;gre-0a05006b&quot;</span><br><span class="line">            Interface &quot;gre-0a05006b&quot;</span><br><span class="line">                type: gre</span><br><span class="line">                options: &#123;df_default=&quot;true&quot;, in_key=flow, local_ip=&quot;10.5.0.110&quot;, out_key=flow, remote_ip=&quot;10.5.0.107&quot;&#125;</span><br><span class="line">        Port br-tun</span><br><span class="line">            Interface br-tun</span><br><span class="line">                type: internal</span><br><span class="line">        Port patch-int</span><br><span class="line">            Interface patch-int</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=patch-tun&#125;</span><br><span class="line">        Port &quot;gre-0a050016&quot;</span><br><span class="line">            Interface &quot;gre-0a050016&quot;</span><br><span class="line">                type: gre</span><br><span class="line">                options: &#123;df_default=&quot;true&quot;, in_key=flow, local_ip=&quot;10.5.0.110&quot;, out_key=flow, remote_ip=&quot;10.5.0.22&quot;&#125;</span><br><span class="line">    Bridge br-data</span><br><span class="line">        Controller &quot;tcp:127.0.0.1:6633&quot;</span><br><span class="line">            is_connected: true</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port phy-br-data</span><br><span class="line">            Interface phy-br-data</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=int-br-data&#125;</span><br><span class="line">        Port br-data</span><br><span class="line">            Interface br-data</span><br><span class="line">                type: internal</span><br><span class="line">    Bridge br-int</span><br><span class="line">        Controller &quot;tcp:127.0.0.1:6633&quot;</span><br><span class="line">            is_connected: true</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port patch-tun</span><br><span class="line">            Interface patch-tun</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=patch-int&#125;</span><br><span class="line">        Port &quot;o-hm0&quot;</span><br><span class="line">            tag: 1</span><br><span class="line">            Interface &quot;o-hm0&quot;</span><br><span class="line">                type: internal</span><br><span class="line">        Port br-int</span><br><span class="line">            Interface br-int</span><br><span class="line">                type: internal</span><br><span class="line">        Port int-br-data</span><br><span class="line">            Interface int-br-data</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=phy-br-data&#125;</span><br><span class="line">    Bridge br-ex</span><br><span class="line">        Port br-ex</span><br><span class="line">            Interface br-ex</span><br><span class="line">                type: internal</span><br><span class="line">    ovs_version: &quot;2.10.0&quot;</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ juju ssh neutron-gateway/0 -- sudo ovs-vsctl show</span><br><span class="line">ec3e2cb6-5261-4c22-8afd-5bacb0e8ce85</span><br><span class="line">    Manager &quot;ptcp:6640:127.0.0.1&quot;</span><br><span class="line">        is_connected: true</span><br><span class="line">    Bridge br-ex</span><br><span class="line">        Port br-ex</span><br><span class="line">            Interface br-ex</span><br><span class="line">                type: internal</span><br><span class="line">    Bridge br-int</span><br><span class="line">        Controller &quot;tcp:127.0.0.1:6633&quot;</span><br><span class="line">            is_connected: true</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port &quot;tap62c03d3b-b1&quot;</span><br><span class="line">            tag: 2</span><br><span class="line">            Interface &quot;tap62c03d3b-b1&quot;</span><br><span class="line">        Port &quot;tapb65f90d1-2e&quot;</span><br><span class="line">            tag: 3</span><br><span class="line">            Interface &quot;tapb65f90d1-2e&quot;</span><br><span class="line">        Port int-br-data</span><br><span class="line">            Interface int-br-data</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=phy-br-data&#125;</span><br><span class="line">        Port patch-tun</span><br><span class="line">            Interface patch-tun</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=patch-int&#125;</span><br><span class="line">        Port &quot;tap6f1478be-b1&quot;</span><br><span class="line">            tag: 1</span><br><span class="line">            Interface &quot;tap6f1478be-b1&quot;</span><br><span class="line">        Port &quot;tap01efd82b-53&quot;</span><br><span class="line">            tag: 2</span><br><span class="line">            Interface &quot;tap01efd82b-53&quot;</span><br><span class="line">        Port &quot;tap5cb6e3f3-eb&quot;</span><br><span class="line">            tag: 3</span><br><span class="line">            Interface &quot;tap5cb6e3f3-eb&quot;</span><br><span class="line">        Port br-int</span><br><span class="line">            Interface br-int</span><br><span class="line">                type: internal</span><br><span class="line">    Bridge br-data</span><br><span class="line">        Controller &quot;tcp:127.0.0.1:6633&quot;</span><br><span class="line">            is_connected: true</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port &quot;ens7&quot;</span><br><span class="line">            Interface &quot;ens7&quot;</span><br><span class="line">        Port br-data</span><br><span class="line">            Interface br-data</span><br><span class="line">                type: internal</span><br><span class="line">        Port phy-br-data</span><br><span class="line">            Interface phy-br-data</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=int-br-data&#125;</span><br><span class="line">    Bridge br-tun</span><br><span class="line">        Controller &quot;tcp:127.0.0.1:6633&quot;</span><br><span class="line">            is_connected: true</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port patch-int</span><br><span class="line">            Interface patch-int</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=patch-tun&#125;</span><br><span class="line">        Port &quot;gre-0a05007a&quot;</span><br><span class="line">            Interface &quot;gre-0a05007a&quot;</span><br><span class="line">                type: gre</span><br><span class="line">                options: &#123;df_default=&quot;true&quot;, in_key=flow, local_ip=&quot;10.5.0.22&quot;, out_key=flow, remote_ip=&quot;10.5.0.122&quot;&#125;</span><br><span class="line">        Port &quot;gre-0a05006b&quot;</span><br><span class="line">            Interface &quot;gre-0a05006b&quot;</span><br><span class="line">                type: gre</span><br><span class="line">                options: &#123;df_default=&quot;true&quot;, in_key=flow, local_ip=&quot;10.5.0.22&quot;, out_key=flow, remote_ip=&quot;10.5.0.107&quot;&#125;</span><br><span class="line">        Port br-tun</span><br><span class="line">            Interface br-tun</span><br><span class="line">                type: internal</span><br><span class="line">        Port &quot;gre-0a050079&quot;</span><br><span class="line">            Interface &quot;gre-0a050079&quot;</span><br><span class="line">                type: gre</span><br><span class="line">                options: &#123;df_default=&quot;true&quot;, in_key=flow, local_ip=&quot;10.5.0.22&quot;, out_key=flow, remote_ip=&quot;10.5.0.121&quot;&#125;</span><br><span class="line">        Port &quot;gre-0a05006e&quot;</span><br><span class="line">            Interface &quot;gre-0a05006e&quot;</span><br><span class="line">                type: gre</span><br><span class="line">                options: &#123;df_default=&quot;true&quot;, in_key=flow, local_ip=&quot;10.5.0.22&quot;, out_key=flow, remote_ip=&quot;10.5.0.110&quot;&#125;</span><br><span class="line">    ovs_version: &quot;2.10.0&quot;</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~⟫ juju ssh neutron-gateway/0 -- cat /var/lib/neutron/ra/0a839377-6b19-419b-9868-616def4d749f.radvd.conf</span><br><span class="line">interface qr-5cb6e3f3-eb</span><br><span class="line">&#123;</span><br><span class="line">   AdvSendAdvert on;</span><br><span class="line">   MinRtrAdvInterval 30;</span><br><span class="line">   MaxRtrAdvInterval 100;</span><br><span class="line">   AdvLinkMTU 1458;</span><br><span class="line">   prefix fc00:4a9c:7b9e:80b6::/64</span><br><span class="line">   &#123;</span><br><span class="line">        AdvOnLink on;</span><br><span class="line">        AdvAutonomous on;</span><br><span class="line">   &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ openstack security group rule list lb-health-mgr-sec-grp</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">| ID                                   | IP Protocol | IP Range | Port Range | Remote Security Group |</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">| 09a92cb2-9942-44d4-8a96-9449a6758967 | None        | None     |            | None                  |</span><br><span class="line">| 20daa06c-9de6-4c91-8a1e-59645f23953a | udp         | None     | 5555:5555  | None                  |</span><br><span class="line">| 8f7b9966-c255-4727-a172-60f22f0710f9 | None        | None     |            | None                  |</span><br><span class="line">| 90f86b27-12f8-4a9a-9924-37b31d26cbd8 | icmpv6      | None     |            | None                  |</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ openstack security group rule list lb-mgmt-sec-grp</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">| ID                                   | IP Protocol | IP Range | Port Range | Remote Security Group |</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">| 54f79f92-a6c5-411d-a309-a02b39cc384b | icmpv6      | None     |            | None                  |</span><br><span class="line">| 574f595e-3d96-460e-a3f2-329818186492 | None        | None     |            | None                  |</span><br><span class="line">| 5ecb0f58-f5dd-4d52-bdfa-04fd56968bd8 | tcp         | None     | 22:22      | None                  |</span><br><span class="line">| 7ead3a3a-bc45-4434-b7a2-e2a6c0dc3ce9 | None        | None     |            | None                  |</span><br><span class="line">| cf82d108-e0f8-4916-95d4-0c816b6eb156 | tcp         | None     | 9443:9443  | None                  |</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ source ~/novarc</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ openstack security group rule list default</span><br><span class="line">+--------------------------------------+-------------+-----------+------------+--------------------------------------+</span><br><span class="line">| ID                                   | IP Protocol | IP Range  | Port Range | Remote Security Group                |</span><br><span class="line">+--------------------------------------+-------------+-----------+------------+--------------------------------------+</span><br><span class="line">| 15b56abd-c2af-4c0a-8585-af68a8f09e3c | icmpv6      | None      |            | None                                 |</span><br><span class="line">| 2ad77fa3-32c7-4a20-a572-417bea782eff | icmp        | 0.0.0.0/0 |            | None                                 |</span><br><span class="line">| 2c2aec15-e4ad-4069-abd2-0191fe80f9bb | None        | None      |            | None                                 |</span><br><span class="line">| 3b775807-3c61-45a3-9677-aaf9631db677 | udp         | 0.0.0.0/0 | 3389:3389  | None                                 |</span><br><span class="line">| 3e9a6e7f-b9a2-47c9-97ca-042b22fbf308 | icmpv6      | None      |            | None                                 |</span><br><span class="line">| 42a3c09e-91c8-471d-b4a8-c1fe87dab066 | None        | None      |            | None                                 |</span><br><span class="line">| 47f9cec2-4bc0-4d71-9a02-3a27d46b59f8 | icmp        | None      |            | None                                 |</span><br><span class="line">| 94297175-9439-4df2-8c93-c5576e52e138 | udp         | None      | 546:546    | None                                 |</span><br><span class="line">| 9c6ac9d2-3b9e-4bab-a55a-04a1679b66be | None        | None      |            | c48a1bf5-7b7e-4337-afdf-8057ae8025af |</span><br><span class="line">| b6e95f76-1b64-4135-8b62-b058ec989f7e | None        | None      |            | c48a1bf5-7b7e-4337-afdf-8057ae8025af |</span><br><span class="line">| de5132a5-72e2-4f03-8b6a-dcbc2b7811c3 | tcp         | 0.0.0.0/0 | 3389:3389  | None                                 |</span><br><span class="line">| e72bea9f-84ce-4e3a-8597-c86d40b9b5ef | tcp         | 0.0.0.0/0 | 22:22      | None                                 |</span><br><span class="line">| ecf1415c-c6e9-4cf6-872c-4dac1353c014 | tcp         | 0.0.0.0/0 |            | None                                 |</span><br><span class="line">+--------------------------------------+-------------+-----------+------------+--------------------------------------+</span><br></pre></td></tr></table></figure></p>
<p>底层OpenStack环境(OpenStack Over Openstack)需要做 (见: <a href="https://blog.csdn.net/quqi99/article/details/78437988" target="_blank" rel="external">https://blog.csdn.net/quqi99/article/details/78437988</a> ):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack security group rule create $secgroup --protocol udp --dst-port 546 --ethertype IPv6</span><br></pre></td></tr></table></figure></p>
<p>最容易出现的问题是health-manager-octavia-0-listen-port port为DOWN, 从而o-hm0网络不通而无法从dhcp server处获得IP, 网段不通多半是br-int上的flow rules的问题, 我多次遇到这种情况, 但后来重建环境不知为什么又好了.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">root@juju-50fb86-bionic-rocky-barbican-octavia-11:~# ovs-ofctl dump-flows br-int</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=424018.932s, table=0, n_packets=978, n_bytes=76284, priority=10,icmp6,in_port=&quot;o-hm0&quot;,icmp_type=136 actions=resubmit(,24)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=424018.930s, table=0, n_packets=0, n_bytes=0, priority=10,arp,in_port=&quot;o-hm0&quot; actions=resubmit(,24)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=425788.219s, table=0, n_packets=0, n_bytes=0, priority=2,in_port=&quot;int-br-data&quot; actions=drop</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=424018.943s, table=0, n_packets=10939, n_bytes=2958167, priority=9,in_port=&quot;o-hm0&quot; actions=resubmit(,25)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=425788.898s, table=0, n_packets=10032, n_bytes=1608826, priority=0 actions=resubmit(,60)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=425788.903s, table=23, n_packets=0, n_bytes=0, priority=0 actions=drop</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=424018.940s, table=24, n_packets=675, n_bytes=52650, priority=2,icmp6,in_port=&quot;o-hm0&quot;,icmp_type=136,nd_target=fc00:4a9c:7b9e:80b6:f816:3eff:fe99:8cab actions=resubmit(,60)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=424018.938s, table=24, n_packets=0, n_bytes=0, priority=2,icmp6,in_port=&quot;o-hm0&quot;,icmp_type=136,nd_target=fe80::f816:3eff:fe99:8cab actions=resubmit(,60)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=425788.879s, table=24, n_packets=303, n_bytes=23634, priority=0 actions=drop</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=424018.951s, table=25, n_packets=10939, n_bytes=2958167, priority=2,in_port=&quot;o-hm0&quot;,dl_src=fa:16:3e:99:8c:ab actions=resubmit(,60)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=425788.896s, table=60, n_packets=21647, n_bytes=4620009, priority=3 actions=NORMAL</span><br><span class="line"></span><br><span class="line">root@juju-50fb86-bionic-rocky-barbican-octavia-11:~# ovs-ofctl dump-flows br-data</span><br><span class="line"> cookie=0xb41c0c7781ded568, duration=426779.130s, table=0, n_packets=16816, n_bytes=3580386, priority=2,in_port=&quot;phy-br-data&quot; actions=drop</span><br><span class="line"> cookie=0xb41c0c7781ded568, duration=426779.201s, table=0, n_packets=0, n_bytes=0, priority=0 actions=NORMAL</span><br></pre></td></tr></table></figure></p>
<p>如果o-hm0总是无法获得IP, 我们也可以手工配置一个IPv4管理网段试试.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">neutron router-gateway-clear lb-mgmt</span><br><span class="line">neutron router-interface-delete lb-mgmt lb-mgmt-subnetv6</span><br><span class="line">neutron subnet-delete lb-mgmt-subnetv6</span><br><span class="line">neutron port-list |grep fc00</span><br><span class="line">#neutron port-delete 464e6d47-9830-4966-a2b7-e188c19c407a</span><br><span class="line">openstack subnet create --subnet-range 192.168.0.0/24 --allocation-pool start=192.168.0.2,end=192.168.0.200 --network lb-mgmt-net lb-mgmt-subnet</span><br><span class="line">neutron router-interface-add lb-mgmt lb-mgmt-subnet</span><br><span class="line">#neutron router-gateway-set lb-mgmt ext_net</span><br><span class="line">neutron port-list |grep 192.168.0.1</span><br><span class="line"></span><br><span class="line">#openstack security group create lb-mgmt-sec-grp --project $(openstack security group show lb-mgmt-sec-grp -f value -c project_id)</span><br><span class="line">openstack security group rule create --protocol udp --dst-port 5555 lb-mgmt-sec-grp</span><br><span class="line">openstack security group rule create --protocol tcp --dst-port 22 lb-mgmt-sec-grp</span><br><span class="line">openstack security group rule create --protocol tcp --dst-port 9443 lb-mgmt-sec-grp</span><br><span class="line">openstack security group rule create --protocol icmp lb-mgmt-sec-grp</span><br><span class="line">openstack security group show lb-mgmt-sec-grp</span><br><span class="line">openstack security group rule create --protocol udp --dst-port 5555 lb-health-mgr-sec-grp</span><br><span class="line">openstack security group rule create --protocol icmp lb-health-mgr-sec-grp</span><br><span class="line"></span><br><span class="line"># create a management port o-hm0 on octavia/0 node, first use neutron to allocate a port, then call ovs-vsctl to add-port</span><br><span class="line">LB_HOST=$(juju ssh octavia/0 -- hostname)</span><br><span class="line">juju ssh octavia/0 -- sudo ovs-vsctl del-port br-int o-hm0</span><br><span class="line"># Use LB_HOST to replace juju-70ea4e-bionic-barbican-octavia-11, don&apos;t know why it said &apos;bind failed&apos; when using $LB_HOST directly</span><br><span class="line">neutron port-create --name mgmt-port --security-group $(openstack security group show lb-health-mgr-sec-grp -f value -c id) --device-owner Octavia:health-mgr --binding:host_id=juju-acadb9-bionic-rocky-barbican-octavia-without-vault-9 lb-mgmt-net --tenant-id $(openstack security group show lb-health-mgr-sec-grp -f value -c project_id)</span><br><span class="line"></span><br><span class="line">juju ssh octavia/0 -- sudo ovs-vsctl --may-exist add-port br-int o-hm0 -- set Interface o-hm0 type=internal -- set Interface o-hm0 external-ids:iface-status=active -- set Interface o-hm0 external-ids:attached-mac=$(neutron port-show mgmt-port -f value -c mac_address) -- set Interface o-hm0 external-ids:iface-id=$(neutron port-show mgmt-port -f value -c id)</span><br><span class="line">juju ssh octavia/0 -- sudo ip link set dev o-hm0 address $(neutron port-show mgmt-port -f value -c mac_address)</span><br><span class="line">ping 192.168.0.2</span><br></pre></td></tr></table></figure></p>
<h2 id="测试虚机中安装HTTPS测试服务"><a href="#测试虚机中安装HTTPS测试服务" class="headerlink" title="测试虚机中安装HTTPS测试服务"></a>测试虚机中安装HTTPS测试服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># Prepare CA and ssl pairs for lb server</span><br><span class="line">openssl genrsa -passout pass:password -out ca.key</span><br><span class="line">openssl req -x509 -passin pass:password -new -nodes -key ca.key -days 3650 -out ca.crt -subj &quot;/C=CN/ST=BJ/O=STS&quot;</span><br><span class="line">openssl genrsa -passout pass:password -out lb.key</span><br><span class="line">openssl req -new -key lb.key -out lb.csr -subj &quot;/C=CN/ST=BJ/O=STS/CN=www.quqi.com&quot;</span><br><span class="line">openssl x509 -req -in lb.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out lb.crt -days 3650</span><br><span class="line">cat lb.crt lb.key &gt; lb.pem</span><br><span class="line">#openssl pkcs12 -export -inkey lb.key -in lb.crt -certfile ca.crt -passout pass:password -out lb.p12</span><br><span class="line"></span><br><span class="line"># Create two test servers and run</span><br><span class="line">sudo apt install python-minimal -y</span><br><span class="line">sudo bash -c &apos;cat &gt;simple-https-server.py&apos; &lt;&lt;EOF</span><br><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding=utf-8</span><br><span class="line">import BaseHTTPServer, SimpleHTTPServer</span><br><span class="line">import ssl</span><br><span class="line">httpd = BaseHTTPServer.HTTPServer((&apos;0.0.0.0&apos;, 443), SimpleHTTPServer.SimpleHTTPRequestHandler)</span><br><span class="line">httpd.socket = ssl.wrap_socket (httpd.socket, certfile=&apos;./lb.pem&apos;, server_side=True)</span><br><span class="line">httpd.serve_forever()</span><br><span class="line">EOF</span><br><span class="line">sudo bash -c &apos;cat &gt;index.html&apos; &lt;&lt;EOF</span><br><span class="line">test1</span><br><span class="line">EOF</span><br><span class="line">nohup sudo python simple-https-server.py &amp;</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ curl -k https://10.5.150.4</span><br><span class="line">test1</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ curl -k https://10.5.150.5</span><br><span class="line">test2</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ curl --cacert ~/ca/ca.crt https://10.5.150.4</span><br><span class="line">curl: (51) SSL: certificate subject name (www.quqi.com) does not match target host name &apos;10.5.150.4&apos;</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ curl --resolve www.quqi.com:443:10.5.150.4 --cacert ~/ca/ca.crt https://www.quqi.com</span><br><span class="line">test1</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ curl --resolve www.quqi.com:443:10.5.150.4 -k https://www.quqi.com</span><br><span class="line">test1</span><br></pre></td></tr></table></figure>
<p>20191109更新, 上面的方法使用–cacert时并不work, 改成下列方法works<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openssl req -new -x509 -keyout lb.pem -out lb.pem -days 365 -nodes -subj &quot;/C=CN/ST=BJ/O=STS/CN=www.quqi.com&quot;</span><br><span class="line">$ curl --resolve www.quqi.com:443:192.168.99.135 --cacert ./lb.pem https://www.quqi.com</span><br><span class="line">test1</span><br></pre></td></tr></table></figure></p>
<p>经进一步调试, 原来是因为 这一句”curl –resolve www.quqi.com:443:10.5.150.4 –cacert ~/ca/ca.crt <a href="https://www.quqi.com&quot;应该是&quot;curl" target="_blank" rel="external">https://www.quqi.com&quot;应该是&quot;curl</a> –resolve www.quqi.com:443:10.5.150.4 –cacert ~/ca/lb.pem <a href="https://www.quqi.com" target="_blank" rel="external">https://www.quqi.com</a>“</p>
<p>或者使用apache2安装ssl<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/apache2/sites-available/default-ssl.conf</span><br><span class="line">  ServerName server1.com</span><br><span class="line">  ServerAlias www.server1.com</span><br><span class="line">  SSLCertificateFile      /home/ubuntu/www.server1.com.crt</span><br><span class="line">  SSLCertificateKeyFile /home/ubuntu/www.server1.com.key</span><br><span class="line"></span><br><span class="line">vim /etc/apache2/sites-available/000-default.conf</span><br><span class="line">  ServerName server1.com</span><br><span class="line">  ServerAlias www.server1.com</span><br><span class="line"></span><br><span class="line">sudo apachectl configtest</span><br><span class="line">sudo a2enmod ssl</span><br><span class="line">sudo a2ensite default-ssl</span><br><span class="line">sudo systemctl restart apache2.service</span><br></pre></td></tr></table></figure></p>
<h2 id="测试虚机中安装HTTP测试服务"><a href="#测试虚机中安装HTTP测试服务" class="headerlink" title="测试虚机中安装HTTP测试服务"></a>测试虚机中安装HTTP测试服务</h2><p>下面的这种HTTP测试服务实际上有问题, 会导致haproxy对backend作check时报下列错误.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ae847e94-5aeb-4da6-9b66-07e1a385465b is UP, reason: Layer7 check passed, code: 200, info: &quot;HTTP status check returned code &lt;3C&gt;200&lt;3E&gt;&quot;, check duration: 7ms. 1 active and 0 backup servers online. 0 sessions requeued, 0 total in queue.</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1, deploy http server in backend</span><br><span class="line">MYIP=$(ifconfig ens2|grep &apos;inet addr&apos;|awk -F: &apos;&#123;print $2&#125;&apos;| awk &apos;&#123;print $1&#125;&apos;)</span><br><span class="line">while true; do echo -e &quot;HTTP/1.0 200 OK\r\n\r\nWelcome to $MYIP&quot; | sudo nc -l -p 80 ; done</span><br><span class="line">2, test it</span><br><span class="line">sudo ip netns exec qrouter-2ea1fc45-69e5-4c77-b6c9-d4fabc57145b curl http://192.168.21.7:80</span><br><span class="line">3, add it into haproxy</span><br><span class="line">openstack loadbalancer member create --subnet-id private_subnet --address 192.168.21.7 --protocol-port 80 pool1</span><br></pre></td></tr></table></figure>
<p>并且还会导致在作haproxy vip作curl测试时返回Bad Gateway的错误.<br>所以最后在backend上运行”sudo python -m SimpleHTTPServer 80”之后解决.</p>
<h2 id="How-to-ssh-into-amphora-service-vm"><a href="#How-to-ssh-into-amphora-service-vm" class="headerlink" title="How to ssh into amphora service vm"></a>How to ssh into amphora service vm</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># NOTE： (base64 -w 0 $HOME/.ssh/id_amphora.pub)</span><br><span class="line">sudo mkdir -p /etc/octavia/.ssh &amp;&amp; sudo chown -R $(id -u):$(id -g) /etc/octavia/.ssh</span><br><span class="line">ssh-keygen -b 2048 -t rsa -N &quot;&quot; -f /etc/octavia/.ssh/octavia_ssh_key</span><br><span class="line">openstack user list --domain service_domain</span><br><span class="line"># NOTE: we must add &apos;--user&apos; option to avoid the error &apos;Invalid key_name provided&apos;</span><br><span class="line">nova keypair-add --pub-key=/etc/octavia/.ssh/octavia_ssh_key.pub octavia_ssh_key --user $(openstack user show octavia --domain service_domain -f value -c id)</span><br><span class="line"># openstack cli doesn&apos;t support to list user scope keypairs</span><br><span class="line">nova keypair-list --user $(openstack user show octavia --domain service_domain -f value -c id)</span><br><span class="line"></span><br><span class="line">vim /etc/octavia/octavia.conf</span><br><span class="line">vim /var/lib/juju/agents/unit-octavia-0/charm/templates/rocky/octavia.conf</span><br><span class="line">vim /usr/lib/python3/dist-packages/octavia/compute/drivers/nova_driver.py</span><br><span class="line">vim /usr/lib/python3/dist-packages/octavia/controller/worker/tasks/compute_tasks.py  #import pdb;pdb.set_trace()</span><br><span class="line">[controller_worker]</span><br><span class="line">amp_ssh_key_name = octavia_ssh_key   #for sts, name is called amphora-backdoor</span><br><span class="line">sudo ip netns exec qrouter-0a839377-6b19-419b-9868-616def4d749f ssh -6 -i</span><br><span class="line">~/octavia_ssh_key ubuntu@fc00:4a9c:7b9e:80b6:f816:3eff:fe5f:cdae</span><br><span class="line"></span><br><span class="line">NOTE:</span><br><span class="line">we can&apos;t ssh by: ssh -i /etc/octavia/octavia_ssh_key ubuntu@10.5.150.15</span><br><span class="line">but we can ssh by:</span><br><span class="line">sudo ip netns exec qrouter-0a839377-6b19-419b-9868-616def4d749f ssh -i ~/octavia_ssh_key ubuntu@192.168.0.12 -v</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ nova list --all</span><br><span class="line">+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+-------------------------------------------------------------+</span><br><span class="line">| ID                                   | Name                                         | Tenant ID                        | Status | Task State | Power State | Networks                                                    |</span><br><span class="line">+--------------------------------------+----------------------------------------------+----------------------------------+--------+------------+-------------+-------------------------------------------------------------+</span><br><span class="line">| 1f50fa16-bbbe-47a7-b66b-86de416d0c5e | amphora-2fae038f-08b5-4bce-a2b7-f7bd543c0314 | 5165bc7f79304f67a135fcde3cd78ae1 | ACTIVE | -          | Running     | lb-mgmt-net=192.168.0.12; private=192.168.21.6, 10.5.150.15 |</span><br><span class="line"></span><br><span class="line">openstack security group rule create lb-300e5ee7-2793-4df3-b901-17ce76da0b09 --protocol icmp --remote-ip 0.0.0.0/0</span><br><span class="line">openstack security group rule create lb-300e5ee7-2793-4df3-b901-17ce76da0b09 --protocol tcp --dst-port 22</span><br></pre></td></tr></table></figure>
<h2 id="Deploy-a-non-terminated-HTTPS-load-balancer"><a href="#Deploy-a-non-terminated-HTTPS-load-balancer" class="headerlink" title="Deploy a non-terminated HTTPS load balancer"></a>Deploy a non-terminated HTTPS load balancer</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install python-octaviaclient python3-octaviaclient</span><br><span class="line">openstack complete |sudo tee /etc/bash_completion.d/openstack</span><br><span class="line">source &lt;(openstack complete)</span><br><span class="line">#No module named &apos;oslo_log&apos;</span><br><span class="line">openstack loadbalancer create --name lb1 --vip-subnet-id private_subnet</span><br><span class="line">#lb_vip_port_id=$(openstack loadbalancer create -f value -c vip_port_id --name lb1 --vip-subnet-id private_subnet)</span><br><span class="line"># Re-run the following until lb1 shows ACTIVE and ONLINE statuses:</span><br><span class="line">openstack loadbalancer show lb1</span><br><span class="line">nova list --all</span><br><span class="line">openstack loadbalancer listener create --name listener1 --protocol HTTPS --protocol-port 443 lb1</span><br><span class="line">openstack loadbalancer pool create --name pool1 --lb-algorithm ROUND_ROBIN --listener listener1 --protocol HTTPS</span><br><span class="line">#openstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type HTTPS --url-path / pool1</span><br><span class="line">openstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type TLS-HELLO pool1</span><br><span class="line">openstack loadbalancer member create --subnet-id private_subnet --address 192.168.21.10 --protocol-port 443 pool1</span><br><span class="line">openstack loadbalancer member create --subnet-id private_subnet --address 192.168.21.12 --protocol-port 443 pool1</span><br><span class="line">openstack loadbalancer member list pool1</span><br><span class="line"></span><br><span class="line">root@amphora-a9cf0b97-7f30-4b9c-b16a-7bc54526e0d0:~# ps -ef |grep haproxy</span><br><span class="line">root      1459     1  0 04:34 ?        00:00:00 /usr/sbin/haproxy-systemd-wrapper -f /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/haproxy.cfg -f /var/lib/octavia/haproxy-default-user-group.conf -p /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0.pid -L UlKGE8M_cxJTcktjV8M-eKJkh-g</span><br><span class="line">nobody    1677  1459  0 04:35 ?        00:00:00 /usr/sbin/haproxy -f /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/haproxy.cfg -f /var/lib/octavia/haproxy-default-user-group.conf -p /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0.pid -L UlKGE8M_cxJTcktjV8M-eKJkh-g -Ds -sf 1625</span><br><span class="line">nobody    1679  1677  0 04:35 ?        00:00:00 /usr/sbin/haproxy -f /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/haproxy.cfg -f /var/lib/octavia/haproxy-default-user-group.conf -p /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0.pid -L UlKGE8M_cxJTcktjV8M-eKJkh-g -Ds -sf 1625</span><br><span class="line">root      1701  1685  0 04:36 pts/0    00:00:00 grep --color=auto haproxy</span><br><span class="line">root@amphora-a9cf0b97-7f30-4b9c-b16a-7bc54526e0d0:~#</span><br><span class="line">root@amphora-a9cf0b97-7f30-4b9c-b16a-7bc54526e0d0:~# cat /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/haproxy.cfg</span><br><span class="line"># Configuration for loadbalancer eda3efa5-dd91-437c-81d9-b73d28b5312f</span><br><span class="line">global</span><br><span class="line">    daemon</span><br><span class="line">    user nobody</span><br><span class="line">    log /dev/log local0</span><br><span class="line">    log /dev/log local1 notice</span><br><span class="line">    stats socket /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0.sock mode 0666 level user</span><br><span class="line">    maxconn 1000000</span><br><span class="line">defaults</span><br><span class="line">    log global</span><br><span class="line">    retries 3</span><br><span class="line">    option redispatch</span><br><span class="line">frontend b9d5a192-1a6a-4df7-83d4-fe96ac9574c0</span><br><span class="line">    option tcplog</span><br><span class="line">    maxconn 1000000</span><br><span class="line">    bind 192.168.21.16:443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend 502b6689-40ad-4201-b704-f221e0fddd58</span><br><span class="line">    timeout client 50000</span><br><span class="line">backend 502b6689-40ad-4201-b704-f221e0fddd58</span><br><span class="line">    mode tcp</span><br><span class="line">    balance roundrobin</span><br><span class="line">    timeout check 10s</span><br><span class="line">    option ssl-hello-chk</span><br><span class="line">    fullconn 1000000</span><br><span class="line">    option allbackups</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout server 50000</span><br><span class="line">    server 49f16402-69f4-49bb-8dc0-5ec13a0f1791 192.168.21.10:443 weight 1 check inter 5s fall 3 rise 4</span><br><span class="line">    server 1ab624e1-9cd8-49f3-9297-4fa031a3ca58 192.168.21.12:443 weight 1 check inter 5s fall 3 rise 4</span><br></pre></td></tr></table></figure>
<p>有时候service vm已经创建好, 但octavia-worker因为下列原因退出导致”openstack loadbalancer create –name lb1 –vip-subnet-id private_subnet”这步执行后状态总不对.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2019-01-04 06:30:45.574 8173 INFO cotyledon._service [-] Caught SIGTERM signal, graceful exiting of service ConsumerService(0) [8173]</span><br><span class="line">2019-01-04 06:30:45.573 7983 INFO cotyledon._service_manager [-] Caught SIGTERM signal, graceful exiting of master process</span><br><span class="line">2019-01-04 06:30:45.581 8173 INFO octavia.controller.queue.consumer [-] Stopping consumer...</span><br><span class="line">2019-01-04 06:30:45.593 8173 INFO cotyledon._service [-] Caught SIGTERM signal, graceful exiting of service ConsumerService(0) [8173]</span><br></pre></td></tr></table></figure>
<h2 id="Deploy-a-TLS-terminated-HTTPS-load-balancer"><a href="#Deploy-a-TLS-terminated-HTTPS-load-balancer" class="headerlink" title="Deploy a TLS-terminated HTTPS load balancer"></a>Deploy a TLS-terminated HTTPS load balancer</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 确实不能加密码：　https://opendev.org/openstack/octavia/commit/a501714a76e04b33dfb24c4ead9956ed4696d1df</span><br><span class="line">openssl genrsa -passout pass:password -out ca.key</span><br><span class="line">openssl req -x509 -passin pass:password -new -nodes -key ca.key -days 3650 -out ca.crt -subj &quot;/C=CN/ST=BJ/O=STS&quot;</span><br><span class="line">openssl genrsa -passout pass:password -out lb.key</span><br><span class="line">openssl req -new -key lb.key -out lb.csr -subj &quot;/C=CN/ST=BJ/O=STS/CN=www.quqi.com&quot;</span><br><span class="line">openssl x509 -req -in lb.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out lb.crt -days 3650</span><br><span class="line">cat lb.crt lb.key &gt; lb.pem</span><br><span class="line">openssl pkcs12 -export -inkey lb.key -in lb.crt -certfile ca.crt -passout pass:password -out lb.p12</span><br><span class="line"></span><br><span class="line">sudo apt install python-barbicanclient</span><br><span class="line">#openstack secret delete $(openstack secret list | awk &apos;/ tls_lb_secret / &#123;print $2&#125;&apos;)</span><br><span class="line">openstack secret store --name=&apos;tls_lb_secret&apos; -t &apos;application/octet-stream&apos; -e &apos;base64&apos; --payload=&quot;$(base64 &lt; lb.p12)&quot;</span><br><span class="line">openstack acl user add -u $(openstack user show octavia --domain service_domain -f value -c id) $(openstack secret list | awk &apos;/ tls_lb_secret / &#123;print $2&#125;&apos;)</span><br><span class="line">openstack secret list</span><br><span class="line">openstack loadbalancer create --name lb1 --vip-subnet-id private_subnet</span><br><span class="line"># Re-run the following until lb1 shows ACTIVE and ONLINE statuses:</span><br><span class="line">openstack loadbalancer show lb1</span><br><span class="line"></span><br><span class="line">openstack loadbalancer member list pool1</span><br><span class="line">openstack loadbalancer member delete pool1 &lt;member&gt;</span><br><span class="line">openstack loadbalancer pool delete pool1</span><br><span class="line">openstack loadbalancer listener delete listener1</span><br><span class="line">openstack loadbalancer listener create --protocol-port 443 --protocol TERMINATED_HTTPS --name listener1 --default-tls-container=$(openstack secret list | awk &apos;/ tls_lb_secret / &#123;print $2&#125;&apos;) lb1</span><br><span class="line">openstack loadbalancer pool create --name pool1 --lb-algorithm ROUND_ROBIN --listener listener1 --protocol HTTP</span><br><span class="line">openstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type HTTP --url-path / pool1</span><br><span class="line">openstack loadbalancer member create --subnet-id private_subnet --address 192.168.21.10 --protocol-port 80 pool1</span><br><span class="line">openstack loadbalancer member create --subnet-id private_subnet --address 192.168.21.12 --protocol-port 80 pool1</span><br></pre></td></tr></table></figure>
<p>但是出错了:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@zhhuabj-bastion:~/ca⟫ openstack loadbalancer listener create --protocol-port 443 --protocol TERMINATED_HTTPS --name listener1 --default-tls-container=$(openstack secret list | awk &apos;/ tls_lb_secret / &#123;print $2&#125;&apos;) lb1</span><br><span class="line">Could not retrieve certificate: [&apos;http://10.5.0.25:9312/v1/secrets/7c706fb2-4319-46fc-b78d-81f34393f581&apos;] (HTTP 400) (Request-ID: req-c0c0e4d5-f395-424c-9aab-5c4c4e72fb3d)</span><br></pre></td></tr></table></figure>
<p>出错的原因找到, 是创建密钥时不能加密码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">openssl genrsa -out ca.key</span><br><span class="line">openssl req -x509 -new -nodes -key ca.key -days 3650 -out ca.crt -subj &quot;/C=CN/ST=BJ/O=STS&quot;</span><br><span class="line">openssl genrsa -out lb.key</span><br><span class="line">openssl req -new -key lb.key -out lb.csr -subj &quot;/C=CN/ST=BJ/O=STS/CN=www.quqi.com&quot;</span><br><span class="line">openssl x509 -req -in lb.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out lb.crt -days 3650</span><br><span class="line">cat lb.crt lb.key &gt; lb.pem</span><br><span class="line">openssl pkcs12 -export -inkey lb.key -in lb.crt -certfile ca.crt -passout pass: -out lb.p12</span><br></pre></td></tr></table></figure></p>
<p>但是仍然不成功, 原因已查明, 与密钥无关, 而是之前没有执行这一句(octavia_user_id=$(openstack user show octavia –domain service_domain -f value -c id); openstack acl user add -u $octavia_user_id $secret_id) 所致, 一个完整的脚本如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">#https://wiki.openstack.org/wiki/Network/LBaaS/docs/how-to-create-tls-loadbalancer#Update_neutron_config</span><br><span class="line">#https://lingxiankong.github.io/2018-04-29-octavia-tls-termination-test.html</span><br><span class="line">DOMAIN1=www.server1.com</span><br><span class="line">DOMAIN2=www.server2.com</span><br><span class="line"></span><br><span class="line">echo &quot;Create CA cert(self-signed) and key...&quot;</span><br><span class="line">CA_SUBJECT=&quot;/C=CN/ST=BJ/L=BJ/O=STS/OU=Joshua/CN=CA&quot;</span><br><span class="line">openssl req -new -x509 -nodes -days 3650 -newkey rsa:2048 -keyout ca.key -out ca.crt -subj $CA_SUBJECT</span><br><span class="line"></span><br><span class="line">openssl genrsa -des3 -out $DOMAIN1_encrypted.key 1024</span><br><span class="line">openssl rsa -in $DOMAIN1_encrypted.key -out $DOMAIN1.key</span><br><span class="line">openssl genrsa -des3 -out $DOMAIN2_encrypted.key 1024</span><br><span class="line">openssl rsa -in $DOMAIN2_encrypted.key -out $DOMAIN2.key</span><br><span class="line"></span><br><span class="line">echo &quot;Create server certificate signing request...&quot;</span><br><span class="line">SUBJECT=&quot;/C=CN/ST=BJ/L=BJ/O=STS/OU=Joshua/CN=$DOMAIN1&quot;</span><br><span class="line">openssl req -new -nodes -subj $SUBJECT -key $DOMAIN1.key -out $DOMAIN1.csr</span><br><span class="line">SUBJECT=&quot;/C=CN/ST=BJ/L=BJ/O=STS/OU=Joshua/CN=$DOMAIN2&quot;</span><br><span class="line">openssl req -new -nodes -subj $SUBJECT -key $DOMAIN2.key -out $DOMAIN2.csr</span><br><span class="line"></span><br><span class="line">echo &quot;Sign SSL certificate...&quot;</span><br><span class="line">openssl x509 -req -days 3650 -in $DOMAIN1.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out $DOMAIN1.crt</span><br><span class="line">openssl x509 -req -days 3650 -in $DOMAIN2.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out $DOMAIN2.crt</span><br><span class="line"></span><br><span class="line"># NOTE: must without password when using barbican to save p12</span><br><span class="line">openssl pkcs12 -export -inkey www.server1.com.key -in www.server1.com.crt -certfile ca.crt -passout pass: -out www.server1.com.p12</span><br><span class="line">openssl pkcs12 -export -inkey www.server2.com.key -in www.server2.com.crt -certfile ca.crt -passout pass: -out www.server2.com.p12</span><br><span class="line"></span><br><span class="line">secret1_id=$(openstack secret store --name=&apos;lb_tls_secret_1&apos; -t &apos;application/octet-stream&apos; -e &apos;base64&apos; --payload=&quot;$(base64 &lt; www.server1.com.p12)&quot; -f value -c &quot;Secret href&quot;)</span><br><span class="line">secret2_id=$(openstack secret store --name=&apos;lb_tls_secret_2&apos; -t &apos;application/octet-stream&apos; -e &apos;base64&apos; --payload=&quot;$(base64 &lt; www.server2.com.p12)&quot; -f value -c &quot;Secret href&quot;)</span><br><span class="line"></span><br><span class="line"># allow octavia service user to visit the cert saved in barbican by the user in the novarc</span><br><span class="line">octavia_user_id=$(openstack user show octavia --domain service_domain -f value -c id); echo $octavia_user_id;</span><br><span class="line">openstack acl user add -u $octavia_user_id $secret1_id</span><br><span class="line">openstack acl user add -u $octavia_user_id $secret2_id</span><br><span class="line"></span><br><span class="line">IP=192.168.21.7</span><br><span class="line">subnetid=$(openstack subnet show private_subnet -f value -c id); echo $subnetid</span><br><span class="line">#lb_id=$(openstack loadbalancer create --name lb3 --vip-subnet-id $subnetid -f value -c id); echo $lb_id</span><br><span class="line">lb_id=22ce64e5-585d-43bd-80eb-5c3ff22abacd</span><br><span class="line">listener_id=$(openstack loadbalancer listener create $lb_id --name https_listener --protocol-port 443 --protocol TERMINATED_HTTPS --default-tls-container=$secret1_id --sni-container-refs $secret1_id $secret2_id -f value -c id); echo $listener_id</span><br><span class="line">pool_id=$(openstack loadbalancer pool create --protocol HTTP --listener $listener_id --lb-algorithm ROUND_ROBIN -f value -c id); echo $pool_id</span><br><span class="line">openstack loadbalancer member create --address $&#123;IP&#125; --subnet-id $subnetid --protocol-port 80 $pool_id</span><br><span class="line"></span><br><span class="line">public_network=$(openstack network show ext_net -f value -c id)</span><br><span class="line">fip=$(openstack floating ip create $public_network -f value -c floating_ip_address)</span><br><span class="line">vip=$(openstack loadbalancer show $lb_id -c vip_address -f value)</span><br><span class="line">vip_port=$(openstack port list --fixed-ip ip-address=$vip -c ID -f value)</span><br><span class="line">#openstack floating ip set $fip --fixed-ip-address $vip --port $vip_port</span><br><span class="line">neutron floatingip-associate $fip $vip_port</span><br><span class="line"></span><br><span class="line">curl -k https://$fip</span><br><span class="line">curl --resolve www.server1.com:443:$fip --cacert ~/ca3/ca.crt https://www.server1.com</span><br><span class="line">curl --resolve www.server2.com:443:$fip --cacert ~/ca3/ca.crt https://www.server2.com</span><br><span class="line"></span><br><span class="line">nobody    2202  2200  0 07:23 ?        00:00:00 /usr/sbin/haproxy -f /var/lib/octavia/dfa44538-2c12-411b-b3b3-c709bc139523/haproxy.cfg -f /var/lib/octavia/haproxy-default-user-group.conf -p /var/lib/octavia/dfa44538-2c12-411b-b3b3-c709bc139523/dfa44538-2c12-411b-b3b3-c709bc139523.pid -L 1_a8OAWpvKuB7hMNzt8UwaJ2M00 -Ds -sf 2148</span><br><span class="line"></span><br><span class="line">root@amphora-2fae038f-08b5-4bce-a2b7-f7bd543c0314:~# cat /var/lib/octavia/dfa44538-2c12-411b-b3b3-c709bc139523/haproxy.cfg</span><br><span class="line"># Configuration for loadbalancer 22ce64e5-585d-43bd-80eb-5c3ff22abacd</span><br><span class="line">global</span><br><span class="line">    daemon</span><br><span class="line">    user nobody</span><br><span class="line">    log /dev/log local0</span><br><span class="line">    log /dev/log local1 notice</span><br><span class="line">    stats socket /var/lib/octavia/dfa44538-2c12-411b-b3b3-c709bc139523.sock mode 0666 level user</span><br><span class="line">    maxconn 1000000</span><br><span class="line">defaults</span><br><span class="line">    log global</span><br><span class="line">    retries 3</span><br><span class="line">    option redispatch</span><br><span class="line">frontend dfa44538-2c12-411b-b3b3-c709bc139523</span><br><span class="line">    option httplog</span><br><span class="line">    maxconn 1000000</span><br><span class="line">    redirect scheme https if !&#123; ssl_fc &#125;</span><br><span class="line">    bind 192.168.21.16:443 ssl crt /var/lib/octavia/certs/dfa44538-2c12-411b-b3b3-c709bc139523/b16771bdb053d138575d60e3035d77fa0598ef5c.pem crt /var/lib/octavia/certs/dfa44538-2c12-411b-b3b3-c709bc139523</span><br><span class="line">    mode http</span><br><span class="line">    default_backend e6c4444a-a06e-4c1c-80d4-389516059d46</span><br><span class="line">    timeout client 50000</span><br><span class="line">backend e6c4444a-a06e-4c1c-80d4-389516059d46</span><br><span class="line">    mode http</span><br><span class="line">    balance roundrobin</span><br><span class="line">    fullconn 1000000</span><br><span class="line">    option allbackups</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout server 50000</span><br><span class="line">    server 234ff0d3-5196-4536-bd49-dfbab94732d4 192.168.21.7:80 weight 1</span><br></pre></td></tr></table></figure></p>
<p>目前剩下的问题是service vm无法访问backend 192.168.21.7, 原理应该是:<br>octavia通过_plug_amphora_vip方法添加一个vip port (octavia-lb-vrrp-7e56de03-298e-43dd-a78f-33aa8d4af735), 它应该往amphora虚机上再添加一个port, 然后为此vip添加allowed_address_pairs. 但是在amphora虚机上我们没有发现这块新添的vip NIC, 重新运行下列’nova interface-attach’也不好使<br>nova list –all<br>nova interface-attach –port-id $(neutron port-show octavia-lb-vrrp-f63f0c5b-a541-442a-929c-b8ed7f7b3604 -f value -c id) 044f42c9-d205-4a11-aa8f-6b9aea896861<br>使用下列方法也不好使:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">lb_id=$(openstack loadbalancer show lb3 -f value -c id)</span><br><span class="line">old_vip=$(openstack loadbalancer show lb3 -f value -c vip_address)</span><br><span class="line">private_subnet_id=$(neutron subnet-show private_subnet -f value -c id)</span><br><span class="line"># delete old vip port (named &apos;octavia-lb-$lb_id&apos;)</span><br><span class="line">neutron port-delete octavia-lb-$lb_id</span><br><span class="line"># create new vip port with the same name and vip and binding:host_id is amphora service vm&apos;s host</span><br><span class="line"># nova show $(nova list --all |grep amphora |awk &apos;&#123;print $2&#125;&apos;) |grep OS-EXT-SRV-ATTR:host</span><br><span class="line">neutron port-create --name octavia-lb-$lb_id --device-owner Octavia --binding:host_id=juju-50fb86-bionic-rocky-barbican-octavia-8 --fixed-ip subnet_id=$&#123;private_subnet_id&#125;,ip_address=$&#123;old_vip&#125; private</span><br><span class="line">mac=$(neutron port-show octavia-lb-$lb_id -f value -c mac_address)</span><br><span class="line">nova interface-attach --port-id $(neutron port-show octavia-lb-$lb_id -f value -c id) $(nova list --all |grep amphora |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line"></span><br><span class="line">接着发现admin-state-up为False, 但enable(neutron port-update --admin-state-up True 6dc5b0bd-d0b7-4b29-9fce-b8f7b23c7914)后status仍然为DOWN. 继续检查设置如下;</span><br><span class="line">root@amphora-3d906381-f49f-4efa-bfa7-b5f84eb3b1af:~# cat /etc/netns/amphora-haproxy/network/interfaces</span><br><span class="line">auto lo</span><br><span class="line">iface lo inet loopback</span><br><span class="line">source /etc/netns/amphora-haproxy/network/interfaces.d/*.cfg</span><br><span class="line"></span><br><span class="line">root@amphora-3d906381-f49f-4efa-bfa7-b5f84eb3b1af:~# cat /etc/netns/amphora-haproxy/network/interfaces.d/eth1.cfg</span><br><span class="line"># Generated by Octavia agent</span><br><span class="line">auto eth1 eth1:0</span><br><span class="line">iface eth1 inet static</span><br><span class="line">address 192.168.21.34</span><br><span class="line">broadcast 192.168.21.255</span><br><span class="line">netmask 255.255.255.0</span><br><span class="line">gateway 192.168.21.1</span><br><span class="line">mtu 1458</span><br><span class="line">iface eth1:0 inet static</span><br><span class="line">address 192.168.21.5</span><br><span class="line">broadcast 192.168.21.255</span><br><span class="line">netmask 255.255.255.0</span><br><span class="line"># Add a source routing table to allow members to access the VIP</span><br><span class="line">post-up /sbin/ip route add default via 192.168.21.1 dev eth1 onlink table 1</span><br><span class="line">post-down /sbin/ip route del default via 192.168.21.1 dev eth1 onlink table 1</span><br><span class="line">post-up /sbin/ip route add 192.168.21.0/24 dev eth1 src 192.168.21.5 scope link table 1</span><br><span class="line">post-down /sbin/ip route del 192.168.21.0/24 dev eth1 src 192.168.21.5 scope link table 1</span><br><span class="line">post-up /sbin/ip rule add from 192.168.21.5/32 table 1 priority 100</span><br><span class="line">post-down /sbin/ip rule del from 192.168.21.5/32 table 1 priority 100</span><br><span class="line">post-up /sbin/iptables -t nat -A POSTROUTING -p udp -o eth1 -j MASQUERADE</span><br><span class="line">post-down /sbin/iptables -t nat -D POSTROUTING -p udp -o eth1 -j MASQUERADE</span><br><span class="line"></span><br><span class="line">root@amphora-3d906381-f49f-4efa-bfa7-b5f84eb3b1af:~# cat /var/lib/octavia/plugged_interfaces</span><br><span class="line">fa:16:3e:e2:3a:7f eth1</span><br><span class="line"></span><br><span class="line">ip netns exec amphora-haproxy ifdown eth1</span><br><span class="line">ip netns exec amphora-haproxy ifup eth1</span><br><span class="line">ip netns exec amphora-haproxy ifup eth1.0</span><br><span class="line">ip netns exec amphora-haproxy ip addr show</span><br><span class="line"></span><br><span class="line">但发现无法ifup eth1.0:</span><br><span class="line">root@amphora-3d906381-f49f-4efa-bfa7-b5f84eb3b1af:~# ip netns exec amphora-haproxy ifup eth1.0</span><br><span class="line">Unknown interface eth1.0</span><br><span class="line"></span><br><span class="line">手工执行它:</span><br><span class="line">ip netns exec amphora-haproxy ifdown eth1</span><br><span class="line">ip netns exec amphora-haproxy ip addr add 192.168.21.34/24 dev eth1</span><br><span class="line">ip netns exec amphora-haproxy ip addr add 192.168.21.5/24 dev eth1</span><br><span class="line">ip netns exec amphora-haproxy ifconfig eth1 up</span><br><span class="line">ip netns exec amphora-haproxy ip route add default via 192.168.21.1 dev eth1 onlink table 1</span><br><span class="line">ip netns exec amphora-haproxy ip route add 192.168.21.0/24 dev eth1 src 192.168.21.5 scope link table 1</span><br><span class="line">ip netns exec amphora-haproxy ip rule add from 192.168.21.5/32 table 1 priority 100</span><br><span class="line">ip netns exec amphora-haproxy iptables -t nat -A POSTROUTING -p udp -o eth1 -j MASQUERADE</span><br><span class="line"></span><br><span class="line">注: 要重做image的话, 可以参考: https://github.com/openstack/octavia/tree/master/diskimage-create</span><br><span class="line"></span><br><span class="line">此时, 可以从amphora-haproxy ping backedn vm 192.168.21.7</span><br><span class="line">root@amphora-3d906381-f49f-4efa-bfa7-b5f84eb3b1af:~# ip netns exec amphora-haproxy ping -c 1 192.168.21.7</span><br><span class="line">PING 192.168.21.7 (192.168.21.7) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.21.7: icmp_seq=1 ttl=64 time=3.83 ms</span><br><span class="line"></span><br><span class="line">但是从neutron-gateway节点仍然无法ping vip 192.168.21.5</span><br><span class="line">root@juju-50fb86-bionic-rocky-barbican-octavia-6:~# ip netns exec qrouter-2ea1fc45-69e5-4c77-b6c9-d4fabc57145b ping 192.168.21.5</span><br><span class="line">PING 192.168.21.5 (192.168.21.5) 56(84) bytes of data.</span><br><span class="line"></span><br><span class="line">继续为这个ha port添加allowed-address-pairs port, 但仍然无果.</span><br><span class="line">neutron port-update 6dc5b0bd-d0b7-4b29-9fce-b8f7b23c7914 --allowed-address-pairs type=dict list=true mac_address=fa:16:3e:e2:3a:7f,ip_address=192.168.21.5</span><br><span class="line">root@juju-50fb86-bionic-rocky-barbican-octavia-9:~# iptables-save |grep &apos;IP/MAC pairs&apos;</span><br><span class="line">-A neutron-openvswi-s650aa1af-5 -s 192.168.21.5/32 -m mac --mac-source FA:16:3E:E2:3A:7F -m comment --comment &quot;Allow traffic from defined IP/MAC pairs.&quot; -j RETURN</span><br><span class="line">-A neutron-openvswi-s650aa1af-5 -s 192.168.21.34/32 -m mac --mac-source FA:16:3E:E2:3A:7F -m comment --comment &quot;Allow traffic from defined IP/MAC pairs.&quot; -j RETURN</span><br><span class="line">-A neutron-openvswi-sfad3d0f9-3 -s 192.168.0.16/32 -m mac --mac-source FA:16:3E:EA:54:4F -m comment --comment &quot;Allow traffic from defined IP/MAC pairs.&quot; -j RETURN</span><br><span class="line"></span><br><span class="line">root@juju-50fb86-bionic-rocky-barbican-octavia-9:~# iptables-save |grep &apos;IP/MAC pairs&apos;</span><br><span class="line">-A neutron-openvswi-s650aa1af-5 -s 192.168.21.5/32 -m mac --mac-source FA:16:3E:E2:3A:7F -m comment --comment &quot;Allow traffic from defined IP/MAC pairs.&quot; -j RETURN</span><br><span class="line">-A neutron-openvswi-s650aa1af-5 -s 192.168.21.34/32 -m mac --mac-source FA:16:3E:E2:3A:7F -m comment --comment &quot;Allow traffic from defined IP/MAC pairs.&quot; -j RETURN</span><br><span class="line">-A neutron-openvswi-sfad3d0f9-3 -s 192.168.0.16/32 -m mac --mac-source FA:16:3E:EA:54:4F -m comment --comment &quot;Allow traffic from defined IP/MAC pairs.&quot; -j RETURN</span><br><span class="line"></span><br><span class="line">root@juju-50fb86-bionic-rocky-barbican-octavia-9:~# ovs-appctl bridge/dump-flows br-int |grep 192.168.21</span><br><span class="line">table_id=24, duration=513s, n_packets=6, n_bytes=252, priority=2,arp,in_port=4,arp_spa=192.168.21.5,actions=goto_table:25</span><br><span class="line">table_id=24, duration=513s, n_packets=1, n_bytes=42, priority=2,arp,in_port=4,arp_spa=192.168.21.34,actions=goto_table:25</span><br><span class="line"></span><br><span class="line">root@juju-50fb86-bionic-rocky-barbican-octavia-9:~# ovs-appctl bridge/dump-flows br-int |grep 25,</span><br><span class="line">table_id=25, duration=48s, n_packets=0, n_bytes=0, priority=2,in_port=4,dl_src=fa:16:3e:e2:3a:7f,actions=goto_table:60</span><br><span class="line">table_id=25, duration=48s, n_packets=6, n_bytes=1396, priority=2,in_port=3,dl_src=fa:16:3e:ea:54:4f,actions=goto_table:60</span><br><span class="line">root@juju-50fb86-bionic-rocky-barbican-octavia-9:~# ovs-appctl bridge/dump-flows br-int |grep 60,</span><br><span class="line">table_id=60, duration=76s, n_packets=20, n_bytes=3880, priority=3,actions=NORMAL</span><br></pre></td></tr></table></figure></p>
<p>继续查找原因, 既然从service vm能ping backend说明网络都没问题, 现在只是无法从gateway ping service vm那说明应该还是防火墙的问题. 采用’neutron port-show <ha-vip-port>‘查看该vip port关联的是一个新security group, 添加之后问题解决:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openstack security group rule create --protocol tcp --dst-port 22 lb-7f2985f0-c0d5-47ab-b805-7f5dafe20d3e</span><br><span class="line">openstack security group rule create --protocol icmp lb-7f2985f0-c0d5-47ab-b805-7f5dafe20d3e</span><br></pre></td></tr></table></figure></ha-vip-port></p>
<p>接着就是报这个错, 后来确认是上面在backend模拟HTTP服务的方法有问题, 后改成”sudo python -m SimpleHTTPServer 80”后问题解决 .<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@juju-50fb86-bionic-rocky-barbican-octavia-6:~# ip netns exec qrouter-2ea1fc45-69e5-4c77-b6c9-d4fabc57145b curl -k https://192.168.21.5</span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;h1&gt;502 Bad Gateway&lt;/h1&gt;</span><br><span class="line">Jan  6 05:14:38 amphora-3d906381-f49f-4efa-bfa7-b5f84eb3b1af haproxy[2688]: 192.168.21.1:41372 [06/Jan/2019:05:14:38.038] a5b442f3-7d40-4849-8b88-7f02697bfd5b~ e25b432a-ea45-4191-9448-c364661326dc/ae847e94-5aeb-4da6-9b66-07e1a385465b 28/0/9/-1/40 502 250 - - PH-- 0/0/0/0/0 0/0 &quot;GET / HTTP/1.1</span><br></pre></td></tr></table></figure></p>
<p>整个实验结果见链接- <a href="https://paste.ubuntu.com/p/PPHv9Zfdf6/" target="_blank" rel="external">https://paste.ubuntu.com/p/PPHv9Zfdf6/</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@zhhuabj-bastion:~⟫ curl -k https://10.5.150.5</span><br><span class="line">Hello World!</span><br><span class="line">ubuntu@zhhuabj-bastion:~⟫ curl --resolve www.quqi.com:443:10.5.150.5 --cacert ~/ca2_without_pass/ca.crt https://www.quqi.com</span><br><span class="line">Hello World!</span><br><span class="line">ubuntu@zhhuabj-bastion:~⟫ curl --resolve www.quqi.com:443:10.5.150.5 -k https://www.quqi.com</span><br><span class="line">Hello World!</span><br><span class="line"></span><br><span class="line">root@juju-50fb86-bionic-rocky-barbican-octavia-6:~# ip netns exec qrouter-2ea1fc45-69e5-4c77-b6c9-d4fabc57145b curl -k https://192.168.21.5</span><br><span class="line">Hello World!</span><br><span class="line"></span><br><span class="line">root@amphora-3d906381-f49f-4efa-bfa7-b5f84eb3b1af:~# ip netns exec amphora-haproxy curl 192.168.21.7</span><br><span class="line">Hello World!</span><br><span class="line"></span><br><span class="line">root@amphora-3d906381-f49f-4efa-bfa7-b5f84eb3b1af:~# cat /var/lib/octavia/a5b442f3-7d40-4849-8b88-7f02697bfd5b/haproxy.cfg</span><br><span class="line"># Configuration for loadbalancer 7f2985f0-c0d5-47ab-b805-7f5dafe20d3e</span><br><span class="line">global</span><br><span class="line">    daemon</span><br><span class="line">    user nobody</span><br><span class="line">    log /dev/log local0</span><br><span class="line">    log /dev/log local1 notice</span><br><span class="line">    stats socket /var/lib/octavia/a5b442f3-7d40-4849-8b88-7f02697bfd5b.sock mode 0666 level user</span><br><span class="line">    maxconn 1000000</span><br><span class="line">defaults</span><br><span class="line">    log global</span><br><span class="line">    retries 3</span><br><span class="line">    option redispatch</span><br><span class="line">frontend a5b442f3-7d40-4849-8b88-7f02697bfd5b</span><br><span class="line">    option httplog</span><br><span class="line">    maxconn 1000000</span><br><span class="line">    redirect scheme https if !&#123; ssl_fc &#125;</span><br><span class="line">    bind 192.168.21.5:443 ssl crt /var/lib/octavia/certs/a5b442f3-7d40-4849-8b88-7f02697bfd5b/4aa85f186d19a766c29109577d88734a8fca6385.pem crt /var/lib/octavia/certs/a5b442f3-7d40-4849-8b88-7f02697bfd5b</span><br><span class="line">    mode http</span><br><span class="line">    default_backend e25b432a-ea45-4191-9448-c364661326dc</span><br><span class="line">    timeout client 50000</span><br><span class="line">backend e25b432a-ea45-4191-9448-c364661326dc</span><br><span class="line">    mode http</span><br><span class="line">    balance roundrobin</span><br><span class="line">    timeout check 10s</span><br><span class="line">    option httpchk GET /</span><br><span class="line">    http-check expect rstatus 200</span><br><span class="line">    fullconn 1000000</span><br><span class="line">    option allbackups</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout server 50000</span><br><span class="line">    server ae847e94-5aeb-4da6-9b66-07e1a385465b 192.168.21.7:80 weight 1 check inter 5s fall 3 rise 4</span><br></pre></td></tr></table></figure>
<h2 id="附件-Neutron-LBaaS-v2"><a href="#附件-Neutron-LBaaS-v2" class="headerlink" title="附件 - Neutron LBaaS v2"></a>附件 - Neutron LBaaS v2</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">https://docs.openstack.org/mitaka/networking-guide/config-lbaas.html</span><br><span class="line">neutron lbaas-loadbalancer-create --name test-lb private_subnet</span><br><span class="line">neutron lbaas-listener-create --name test-lb-https --loadbalancer test-lb --protocol HTTPS --protocol-port 443</span><br><span class="line">neutron lbaas-pool-create --name test-lb-pool-https --lb-algorithm LEAST_CONNECTIONS --listener test-lb-https --protocol HTTPS</span><br><span class="line">neutron lbaas-member-create --subnet private_subnet --address 192.168.21.13 --protocol-port 443 test-lb-pool-https</span><br><span class="line">neutron lbaas-member-create --subnet private_subnet --address 192.168.21.8 --protocol-port 443 test-lb-pool-https</span><br><span class="line">neutron lbaas-healthmonitor-create --delay 5 --max-retries 2 --timeout 10 --type HTTPS --pool test-lb-pool-https --name monitor1</span><br><span class="line">root@juju-c9d701-xenail-queens-184313-vrrp-5:~# ip netns exec qlbaas-84fd9a6c-24a2-4c0f-912b-eedc254ac1f4 curl -k  https://192.168.21.14</span><br><span class="line">test1</span><br><span class="line">root@juju-c9d701-xenail-queens-184313-vrrp-5:~# ip netns exec qlbaas-84fd9a6c-24a2-4c0f-912b-eedc254ac1f4 curl -k  https://192.168.21.14</span><br><span class="line">test2</span><br><span class="line">root@juju-c9d701-xenail-queens-184313-vrrp-5:~# ip netns exec qlbaas-84fd9a6c-24a2-4c0f-912b-eedc254ac1f4 curl --cacert /home/ubuntu/lb.pem  https://192.168.21.14</span><br><span class="line">curl: (51) SSL: certificate subject name (www.quqi.com) does not match target host name &apos;192.168.21.14&apos;</span><br><span class="line">root@juju-c9d701-xenail-queens-184313-vrrp-5:~# ip netns exec qlbaas-84fd9a6c-24a2-4c0f-912b-eedc254ac1f4 curl --cacert /home/ubuntu/lb.pem  --resolve www.quqi.com:443:192.168.21.14 https://www.quqi.com</span><br><span class="line">test1</span><br><span class="line"></span><br><span class="line">root@juju-c9d701-xenail-queens-184313-vrrp-5:~# echo &apos;show stat;show table&apos; | socat stdio /var/lib/neutron/lbaas/v2/84fd9a6c-24a2-4c0f-912b-eedc254ac1f4/haproxy_stats.sock</span><br><span class="line"># pxname,svname,qcur,qmax,scur,smax,slim,stot,bin,bout,dreq,dresp,ereq,econ,eresp,wretr,wredis,status,weight,act,bck,chkfail,chkdown,lastchg,downtime,qlimit,pid,iid,sid,throttle,lbtot,tracked,type,rate,rate_lim,rate_max,check_status,check_code,check_duration,hrsp_1xx,hrsp_2xx,hrsp_3xx,hrsp_4xx,hrsp_5xx,hrsp_other,hanafail,req_rate,req_rate_max,req_tot,cli_abrt,srv_abrt,comp_in,comp_out,comp_byp,comp_rsp,lastsess,last_chk,last_agt,qtime,ctime,rtime,ttime,</span><br><span class="line">c2a42906-e160-44dd-8590-968af2077b4a,FRONTEND,,,0,0,2000,0,0,0,0,0,0,,,,,OPEN,,,,,,,,,1,2,0,,,,0,0,0,0,,,,,,,,,,,0,0,0,,,0,0,0,0,,,,,,,,</span><br><span class="line">52112201-05ce-4f4d-b5a8-9e67de2a895a,37a1f5a8-ec7e-4208-9c96-27d2783a594f,0,0,0,0,,0,0,0,,0,,0,0,0,0,no check,1,1,0,,,,,,1,3,1,,0,,2,0,,0,,,,,,,,,,0,,,,0,0,,,,,-1,,,0,0,0,0,</span><br><span class="line">52112201-05ce-4f4d-b5a8-9e67de2a895a,8e722b4b-08b8-4089-bba5-8fa5dd26a87f,0,0,0,0,,0,0,0,,0,,0,0,0,0,no check,1,1,0,,,,,,1,3,2,,0,,2,0,,0,,,,,,,,,,0,,,,0,0,,,,,-1,,,0,0,0,0,</span><br><span class="line">52112201-05ce-4f4d-b5a8-9e67de2a895a,BACKEND,0,0,0,0,200,0,0,0,0,0,,0,0,0,0,UP,2,2,0,,0,117,0,,1,3,0,,0,,1,0,,0,,,,,,,,,,,,,,0,0,0,0,0,0,-1,,,0,0,0,0,</span><br><span class="line"></span><br><span class="line">root@juju-c9d701-xenail-queens-184313-vrrp-5:~# cat /var/lib/neutron/lbaas/v2/84fd9a6c-24a2-4c0f-912b-eedc254ac1f4/haproxy.conf</span><br><span class="line"># Configuration for test-lb</span><br><span class="line">global</span><br><span class="line">    daemon</span><br><span class="line">    user nobody</span><br><span class="line">    group nogroup</span><br><span class="line">    log /dev/log local0</span><br><span class="line">    log /dev/log local1 notice</span><br><span class="line">    maxconn 2000</span><br><span class="line">    stats socket /var/lib/neutron/lbaas/v2/84fd9a6c-24a2-4c0f-912b-eedc254ac1f4/haproxy_stats.sock mode 0666 level user</span><br><span class="line">defaults</span><br><span class="line">    log global</span><br><span class="line">    retries 3</span><br><span class="line">    option redispatch</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout client 50000</span><br><span class="line">    timeout server 50000</span><br><span class="line">frontend c2a42906-e160-44dd-8590-968af2077b4a</span><br><span class="line">    option tcplog</span><br><span class="line">    bind 192.168.21.14:443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend 52112201-05ce-4f4d-b5a8-9e67de2a895a</span><br><span class="line">backend 52112201-05ce-4f4d-b5a8-9e67de2a895a</span><br><span class="line">    mode tcp</span><br><span class="line">    balance leastconn</span><br><span class="line">    timeout check 10s</span><br><span class="line">    option httpchk GET /</span><br><span class="line">    http-check expect rstatus 200</span><br><span class="line">    option ssl-hello-chk</span><br><span class="line">    server 37a1f5a8-ec7e-4208-9c96-27d2783a594f 192.168.21.13:443 weight 1 check inter 5s fall 2</span><br><span class="line">    server 8e722b4b-08b8-4089-bba5-8fa5dd26a87f 192.168.21.8:443 weight 1 check inter 5s fall 2</span><br><span class="line"></span><br><span class="line"># TCP monitor</span><br><span class="line">neutron lbaas-healthmonitor-delete monitor1</span><br><span class="line">neutron lbaas-healthmonitor-create --delay 5 --max-retries 2 --timeout 10 --type TCP --pool test-lb-pool-https --name monitor1 --url-path /</span><br><span class="line">backend 52112201-05ce-4f4d-b5a8-9e67de2a895a</span><br><span class="line">    mode tcp</span><br><span class="line">    balance leastconn</span><br><span class="line">    timeout check 10s</span><br><span class="line">    server 37a1f5a8-ec7e-4208-9c96-27d2783a594f 192.168.21.13:443 weight 1 check inter 5s fall 2</span><br><span class="line">    server 8e722b4b-08b8-4089-bba5-8fa5dd26a87f 192.168.21.8:443 weight 1 check inter 5s fall 2</span><br></pre></td></tr></table></figure>
<h2 id="附录-上层的k8s如何使用下层的openstack中的LBaaS资源"><a href="#附录-上层的k8s如何使用下层的openstack中的LBaaS资源" class="headerlink" title="附录 - 上层的k8s如何使用下层的openstack中的LBaaS资源"></a>附录 - 上层的k8s如何使用下层的openstack中的LBaaS资源</h2><p>如果在一个OpenStack云上面再创建K8S的话, 在k8s中使用下列命令创建LoadBalancer服务会永远Pending状态.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl run test-nginx --image=nginx --replicas=2 --port=80 --expose --service-overrides=&apos;&#123; &quot;spec&quot;: &#123; &quot;type&quot;: &quot;LoadBalancer&quot; &#125; &#125;&apos;</span><br><span class="line">kubectl get svc test-nginx</span><br></pre></td></tr></table></figure></p>
<p>那是因为k8s需要调用底层OpenStack  LBaaS服务创建VIP资源, 然后将所有后端服务的<nodeip:nodeport>作为backend. 那么该如何让k8s具有访问openstack lbaas资源的能力呢? 如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># https://blog.ubuntu.com/2019/01/28/taking-octavia-for-a-ride-with-kubernetes-on-openstack</span><br><span class="line"># openstack上部署k8s, &apos;juju trust openstack-integrator&apos;将让openstack-integrator具有访问bootstrap时所用的openstack credential的权限,</span><br><span class="line"># 之后, 因为cdk实现了interface-openstack-integration接口, 所以cdk k8s可以使用这些openstack credential来直接使用openstack里的LBaaS等资源</span><br><span class="line">juju deploy cs:~containers/openstack-integrator</span><br><span class="line">juju add-relation openstack-integrator kubernetes-master</span><br><span class="line">juju add-relation openstack-integrator kubernetes-worker</span><br><span class="line">juju config openstack-integrator subnet-id=&lt;UUID of subnet&gt;</span><br><span class="line">juju config openstack-integrator floating-network-id=&lt;UUID of ext_net&gt;</span><br><span class="line"></span><br><span class="line"># &apos;juju trust&apos; grants openstack-integrator access to the credential used in the bootstrap command, this charm acts as a proxy for the</span><br><span class="line">juju trust openstack-integrator</span><br><span class="line"></span><br><span class="line">测试yaml, 有时要提供loadbalancer.openstack.org/floating-network-id, 见:  https://github.com/kubernetes/cloud-provider-openstack/tree/master/examples/loadbalancers</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: external-http-nginx-service</span><br><span class="line">  annotations:</span><br><span class="line">    service.beta.kubernetes.io/openstack-internal-load-balancer: &quot;false&quot;</span><br><span class="line">    loadbalancer.openstack.org/floating-network-id: &quot;6f05a9de-4fc9-41f5-9c51-d5f43cd244b9&quot;</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  type: LoadBalancer</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br></pre></td></tr></table></figure></nodeip:nodeport></p>
<p>同时也应该确保service tenant下的security group的quata别超了.quata超了的现象是例如对于FIP, 有时可以有时不可以.</p>
<p>后面继续搭建k8s可参见 - <a href="https://ubuntu.com/blog/taking-octavia-for-a-ride-with-kubernetes-on-openstack" target="_blank" rel="external">https://ubuntu.com/blog/taking-octavia-for-a-ride-with-kubernetes-on-openstack</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br></pre></td><td class="code"><pre><span class="line"># deploy underlying openstack model</span><br><span class="line">./generate-bundle.sh --name o7k:stsstack --create-model --octavia -r stein --dvr-snat --num-compute 8</span><br><span class="line">sed -i &quot;s/mem=8G/mem=8G cores=4/g&quot; ./b/o7k/openstack.yaml</span><br><span class="line">./generate-bundle.sh --name o7k:stsstack --replay --run</span><br><span class="line">./bin/add-data-ports.sh</span><br><span class="line">juju config neutron-openvswitch data-port=&quot;br-data:ens7&quot;</span><br><span class="line"></span><br><span class="line">#refere this page to generate certs - https://docs.openstack.org/project-deploy-guide/charm-deployment-guide/latest/app-octavia.html</span><br><span class="line">juju config octavia \</span><br><span class="line">    lb-mgmt-issuing-cacert=&quot;$(base64 controller_ca.pem)&quot; \</span><br><span class="line">    lb-mgmt-issuing-ca-private-key=&quot;$(base64 controller_ca_key.pem)&quot; \</span><br><span class="line">    lb-mgmt-issuing-ca-key-passphrase=foobar \</span><br><span class="line">    lb-mgmt-controller-cacert=&quot;$(base64 controller_ca.pem)&quot; \</span><br><span class="line">    lb-mgmt-controller-cert=&quot;$(base64 controller_cert_bundle.pem)&quot;</span><br><span class="line"></span><br><span class="line">juju run-action --wait octavia/0 configure-resources</span><br><span class="line">source ~/stsstack-bundles/openstack/novarc</span><br><span class="line">./configure</span><br><span class="line">#juju config octavia loadbalancer-topology=ACTIVE_STANDBY</span><br><span class="line">juju run-action octavia-diskimage-retrofit/0 --wait retrofit-image source-image=$(openstack image list |grep bionic |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">openstack image list</span><br><span class="line"></span><br><span class="line"># enable all ingress traffic</span><br><span class="line">PROJECT_ID=$(openstack project show --domain admin_domain admin -f value -c id)</span><br><span class="line">SECGRP_ID=$(openstack security group list --project $&#123;PROJECT_ID&#125; | awk &apos;/default/ &#123;print $2&#125;&apos;)</span><br><span class="line">openstack security group rule create $&#123;SECGRP_ID&#125; --protocol any --ethertype IPv6 --ingress</span><br><span class="line">openstack security group rule create $&#123;SECGRP_ID&#125; --protocol any --ethertype IPv4 --ingress</span><br><span class="line"></span><br><span class="line"># disable quotas for project, neutron and nova</span><br><span class="line">openstack quota set --instances -1 $&#123;PROJECT_ID&#125;</span><br><span class="line">openstack quota set --floating-ips -1 $&#123;PROJECT_ID&#125;</span><br><span class="line">openstack quota set --cores -1 $&#123;PROJECT_ID&#125;</span><br><span class="line">openstack quota set --ram -1 $&#123;PROJECT_ID&#125;</span><br><span class="line">openstack quota set --gigabytes -1 $&#123;PROJECT_ID&#125;</span><br><span class="line">openstack quota set --volumes -1 $&#123;PROJECT_ID&#125;</span><br><span class="line">openstack quota set --secgroups -1 $&#123;PROJECT_ID&#125;</span><br><span class="line">openstack quota set --secgroup-rules -1 $&#123;PROJECT_ID&#125;</span><br><span class="line">neutron quota-update --network -1</span><br><span class="line">neutron quota-update --floatingip -1</span><br><span class="line">neutron quota-update --port -1</span><br><span class="line">neutron quota-update --router -1</span><br><span class="line">neutron quota-update --security-group -1</span><br><span class="line">neutron quota-update --security-group-rule -1</span><br><span class="line">neutron quota-update --subnet -1</span><br><span class="line">neutron quota-show</span><br><span class="line"></span><br><span class="line"># set up router to allow bastion to access juju controller</span><br><span class="line">GATEWAY_IP=$(openstack router show provider-router -f value -c external_gateway_info \</span><br><span class="line">    |awk &apos;/ip_address/ &#123; for (i=1;i&lt;NF;i++) if ($i~&quot;ip_address&quot;) print $(i+1)&#125;&apos; |cut -f2 -d\&apos;)</span><br><span class="line">CIDR=$(openstack subnet show private_subnet -f value -c cidr)</span><br><span class="line">sudo ip route add $&#123;CIDR&#125; via $&#123;GATEWAY_IP&#125;</span><br><span class="line"></span><br><span class="line"># define juju cloud</span><br><span class="line">sudo bash -c &apos;cat &gt; mystack.yaml&apos; &lt;&lt; EOF</span><br><span class="line">clouds:</span><br><span class="line">  mystack:</span><br><span class="line">    type: openstack</span><br><span class="line">    auth-types: [ userpass ]</span><br><span class="line">    regions:</span><br><span class="line">      RegionOne:</span><br><span class="line">        endpoint: $OS_AUTH_URL</span><br><span class="line">EOF</span><br><span class="line">juju remove-cloud --local mystack</span><br><span class="line">juju add-cloud --local mystack mystack.yaml</span><br><span class="line">juju show-cloud mystack --local</span><br><span class="line">sudo bash -c &apos;cat &gt; mystack_credentials.txt&apos; &lt;&lt; EOF</span><br><span class="line">credentials:</span><br><span class="line">  mystack:</span><br><span class="line">    admin:</span><br><span class="line">      auth-type: userpass</span><br><span class="line">      password: openstack</span><br><span class="line">      tenant-name: admin</span><br><span class="line">      domain-name: &quot;&quot; # ensure we don&apos;t get a domain-scoped token</span><br><span class="line">      project-domain-name: admin_domain</span><br><span class="line">      user-domain-name: admin_domain</span><br><span class="line">      username: admin</span><br><span class="line">      version: &quot;3&quot;</span><br><span class="line">EOF</span><br><span class="line">juju remove-credential --local mystack admin</span><br><span class="line">juju add-credential --local mystack -f ./mystack_credentials.txt</span><br><span class="line">juju show-credential --local mystack admin</span><br><span class="line"></span><br><span class="line"># deploy juju controller</span><br><span class="line">mkdir -p ~/simplestreams/images &amp;&amp; rm -rf ~/simplestreams/images/*</span><br><span class="line">source ~/stsstack-bundles/openstack/novarc</span><br><span class="line">IMAGE_ID=$(openstack image list -f value |grep &apos;bionic active&apos; |awk &apos;&#123;print $1&#125;&apos;)</span><br><span class="line">OS_SERIES=$(openstack image list -f value |grep &apos;bionic active&apos; |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">juju metadata generate-image -d ~/simplestreams -i $IMAGE_ID -s $OS_SERIES -r $OS_REGION_NAME -u $OS_AUTH_URL</span><br><span class="line">ls ~/simplestreams/*/streams/*</span><br><span class="line">NETWORK_ID=$(openstack network show private -f value -c id)</span><br><span class="line"># can remove juju controller &apos;mystack-regionone&apos; from ~/.local/share/juju/controllers.yaml if it exists</span><br><span class="line">juju bootstrap mystack --config network=$&#123;NETWORK_ID&#125; --model-default network=$&#123;NETWORK_ID&#125; --model-default use-default-secgroup=true --metadata-source ~/simplestreams</span><br><span class="line"></span><br><span class="line"># deploy upper k8s model</span><br><span class="line">juju switch mystack-regionone</span><br><span class="line">juju destroy-model --destroy-storage --force upperk8s -y &amp;&amp; juju add-model upperk8s</span><br><span class="line">wget https://api.jujucharms.com/charmstore/v5/bundle/canonical-kubernetes-933/archive/bundle.yaml</span><br><span class="line">sed -i &quot;s/num_units: 2/num_units: 1/g&quot; ./bundle.yaml</span><br><span class="line">sed -i &quot;s/num_units: 3/num_units: 2/g&quot; ./bundle.yaml</span><br><span class="line">sed -i &quot;s/cores=4/cores=2/g&quot; ./bundle.yaml</span><br><span class="line">juju deploy ./bundle.yaml</span><br><span class="line">juju deploy cs:~containers/openstack-integrator</span><br><span class="line">juju add-relation openstack-integrator:clients kubernetes-master:openstack</span><br><span class="line">juju add-relation openstack-integrator:clients kubernetes-worker:openstack</span><br><span class="line"></span><br><span class="line">juju config openstack-integrator subnet-id=$(openstack subnet show private_subnet -c id -f value)</span><br><span class="line">juju config openstack-integrator floating-network-id=$(openstack network show ext_net -c id -f value)</span><br><span class="line">juju trust openstack-integrator</span><br><span class="line">watch -c juju status --color</span><br><span class="line"></span><br><span class="line"># we don&apos;t use the following way to deploy k8s because it can&apos;t be customized to use bionic</span><br><span class="line">juju deploy charmed-kubernetes</span><br><span class="line"># we don&apos;t use the following way as well because it says no networks exist with label &quot;zhhuabj_port_sec_enabled&quot;</span><br><span class="line">~/stsstack-bundles/kubernetes/generate-bundle.sh --name k8s:mystack --create-model -s bionic --run</span><br><span class="line"></span><br><span class="line"># deploy test pod</span><br><span class="line">mkdir -p ~/.kube</span><br><span class="line">juju scp kubernetes-master/0:config ~/.kube/config</span><br><span class="line">sudo snap install kubectl --classic</span><br><span class="line">#Flag --replicas has been deprecated</span><br><span class="line">#kubectl run hello-world --replicas=2 --labels=&quot;run=lb-test&quot; --image=gcr.io/google-samples/node-hello:1.0 --port=8080</span><br><span class="line">kubectl create deployment hello-world --image=gcr.io/google-samples/node-hello:1.0 -o yaml --dry-run=client &gt; helloworld.yaml</span><br><span class="line">sudo bash -c &apos;cat &gt; helloworld.yaml&apos; &lt;&lt; EOF</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: hello-world</span><br><span class="line">  name: hello-world</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: hello-world</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: hello-world</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: gcr.io/google-samples/node-hello:1.0</span><br><span class="line">        name: node-hello</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">EOF</span><br><span class="line">kubectl create -f ./helloworld.yaml</span><br><span class="line">kubectl get deployments hello-world -o wide</span><br><span class="line"></span><br><span class="line"># deploy LoadBalancer service</span><br><span class="line"># # remember to relace the following &lt;ext_net_id&gt; to avoid &apos;pending&apos; status for FIP</span><br><span class="line">sudo bash -c &apos;cat &gt; helloservice.yaml&apos; &lt;&lt; EOF</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: hello</span><br><span class="line">  annotations:</span><br><span class="line">    service.beta.kubernetes.io/openstack-internal-load-balancer: &quot;false&quot;</span><br><span class="line">    loadbalancer.openstack.org/floating-network-id: &quot;&lt;ext_net_id&gt;&quot;</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: hello-world</span><br><span class="line">  type: LoadBalancer</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 8080</span><br><span class="line">EOF</span><br><span class="line">kubectl create -f ./helloservice.yaml</span><br><span class="line">watch kubectl get svc -o wide hello</span><br><span class="line">NAME    TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE     SELECTOR</span><br><span class="line">hello   LoadBalancer   10.152.183.88   10.5.150.220   80:32315/TCP   2m47s   app=hello-world</span><br><span class="line">$ curl http://10.5.150.220:80</span><br><span class="line">Hello Kubernetes!</span><br><span class="line"></span><br><span class="line">openstack loadbalancer list            #use &apos;kubectl  delete svc hello&apos; to delete lb</span><br><span class="line">openstack loadbalancer amphora list</span><br><span class="line"></span><br><span class="line">#vip=$(openstack loadbalancer list |grep ACTIVE |awk &apos;&#123;print $8&#125;&apos;)</span><br><span class="line">#public_network=$(openstack network show ext_net -f value -c id)</span><br><span class="line">#fip=$(openstack floating ip create $public_network -f value -c floating_ip_address)</span><br><span class="line">#openstack floating ip set $fip --fixed-ip-address $vip --port $(openstack port list --fixed-ip ip-address=$vip -c id -f value)</span><br><span class="line"></span><br><span class="line">$ openstack floating ip list</span><br><span class="line">+--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+</span><br><span class="line">| ID                                   | Floating IP Address | Fixed IP Address | Port                                 | Floating Network                     | Project                          |</span><br><span class="line">+--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+</span><br><span class="line">| c284dfc3-d294-48ae-8ccf-20a7e47fe039 | 10.5.150.220        | 192.168.21.26    | ded911a8-f213-4884-a387-7efcf14c8a89 | 1a83b2d3-1c1b-4bc9-b882-f132b9ff9d87 | 0d1886170941437fa46fb34508e67d24 |</span><br><span class="line">+--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+</span><br><span class="line"></span><br><span class="line">$ openstack loadbalancer list</span><br><span class="line">+--------------------------------------+------------------------------------------------------------------------+----------------------------------+---------------+---------------------+----------+</span><br><span class="line">| id                                   | name                                                                   | project_id                       | vip_address   | provisioning_status | provider |</span><br><span class="line">+--------------------------------------+------------------------------------------------------------------------+----------------------------------+---------------+---------------------+----------+</span><br><span class="line">| 72d96673-8723-4dde-9035-66c3bd095632 | kube_service_kubernetes-n7cgun28wpbsfgiiza5qralulupdtspv_default_hello | 0d1886170941437fa46fb34508e67d24 | 192.168.21.26 | ACTIVE              | amphora  |</span><br><span class="line">+--------------------------------------+------------------------------------------------------------------------+----------------------------------+---------------+---------------------+----------+</span><br><span class="line"></span><br><span class="line">$ openstack loadbalancer amphora list</span><br><span class="line">+--------------------------------------+--------------------------------------+-----------+------------+-----------------------------------------+---------------+</span><br><span class="line">| id                                   | loadbalancer_id                      | status    | role       | lb_network_ip                           | ha_ip         |</span><br><span class="line">+--------------------------------------+--------------------------------------+-----------+------------+-----------------------------------------+---------------+</span><br><span class="line">| 068104f1-ffee-4b4a-88ab-2e46cee1cbbc | 72d96673-8723-4dde-9035-66c3bd095632 | ALLOCATED | STANDALONE | fc00:961f:bb53:993b:f816:3eff:fe0d:6433 | 192.168.21.26 |</span><br><span class="line">+--------------------------------------+--------------------------------------+-----------+------------+-----------------------------------------+---------------+</span><br><span class="line"></span><br><span class="line">$ nova list --all |grep amphora</span><br><span class="line">| 966f0ec0-b48a-48b9-8078-d7406ee08311 | amphora-068104f1-ffee-4b4a-88ab-2e46cee1cbbc | 144901cff394489d9095b1361caa6872 | ACTIVE | -          | Running     | lb-mgmt-net=fc00:961f:bb53:993b:f816:3eff:fe0d:6433; private=192.168.21.174 |</span><br><span class="line"></span><br><span class="line">#nova keypair-add --pub-key=~/.ssh/id_amphora.pub amphora-backdoor --user $(openstack user show octavia --domain service_domain -f value -c id)</span><br><span class="line">$ nova keypair-list --user $(openstack user show octavia --domain service_domain -f value -c id)</span><br><span class="line">+------------------+------+-------------------------------------------------+</span><br><span class="line">| Name             | Type | Fingerprint                                     |</span><br><span class="line">+------------------+------+-------------------------------------------------+</span><br><span class="line">| amphora-backdoor | ssh  | d9:53:1e:eb:70:42:24:f3:01:e2:4c:9d:c9:97:bd:11 |</span><br><span class="line">+------------------+------+-------------------------------------------------+</span><br><span class="line"></span><br><span class="line">$ openstack router list</span><br><span class="line">+--------------------------------------+-----------------+--------+-------+----------------------------------+-------------+-------+</span><br><span class="line">| ID                                   | Name            | Status | State | Project                          | Distributed | HA    |</span><br><span class="line">+--------------------------------------+-----------------+--------+-------+----------------------------------+-------------+-------+</span><br><span class="line">| 05d8e256-ba53-4b32-b40d-17472ac09040 | provider-router | ACTIVE | UP    | 0d1886170941437fa46fb34508e67d24 | True        | False |</span><br><span class="line">| 6876c9c3-d8c5-4b31-876b-fe830d4b5f0b | lb-mgmt         | ACTIVE | UP    | 144901cff394489d9095b1361caa6872 | False       | False |</span><br><span class="line">+--------------------------------------+-----------------+--------+-------+----------------------------------+-------------+-------+</span><br><span class="line">$ openstack subnet list</span><br><span class="line">+--------------------------------------+------------------+--------------------------------------+--------------------------+</span><br><span class="line">| ID                                   | Name             | Network                              | Subnet                   |</span><br><span class="line">+--------------------------------------+------------------+--------------------------------------+--------------------------+</span><br><span class="line">| 401b11c1-b209-4545-81f2-81dc93674616 | private_subnet   | 2c927db4-ee05-4d4a-b35b-f106cbd785c4 | 192.168.21.0/24          |</span><br><span class="line">| 54365c1c-1987-4607-8e98-4341cff4795f | ext_net_subnet   | 1a83b2d3-1c1b-4bc9-b882-f132b9ff9d87 | 10.5.0.0/16              |</span><br><span class="line">| 56847939-4776-4b39-bdac-e04a4a9b6555 | lb-mgmt-subnetv6 | 983933d9-3078-47ce-b581-961fbb53993b | fc00:961f:bb53:993b::/64 |</span><br><span class="line">+--------------------------------------+------------------+--------------------------------------+--------------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#nova list --all |grep amphora</span><br><span class="line">#juju scp ~/.ssh/id_amphora* nova-compute/5:/home/ubuntu/</span><br><span class="line">#amphora instance is on nova-compute/3 (juju-7917e4-octavia-9), but gateway for lb-mgmt-subnetv6 is on nova-compute/5 (neutron l3-agent-list-hosting-router lb-mgmt)</span><br><span class="line">juju ssh nova-compute/5 -- sudo ip netns exec qrouter-6876c9c3-d8c5-4b31-876b-fe830d4b5f0b ping6 fc00:961f:bb53:993b:f816:3eff:fe0d:6433</span><br><span class="line">juju ssh nova-compute/5 -- sudo ip netns exec qrouter-6876c9c3-d8c5-4b31-876b-fe830d4b5f0b ssh -6 -i ~/id_amphora ubuntu@fc00:961f:bb53:993b:f816:3eff:fe0d:6433 -v</span><br><span class="line"></span><br><span class="line">ubuntu@amphora-068104f1-ffee-4b4a-88ab-2e46cee1cbbc:~$ sudo ip netns ls</span><br><span class="line">amphora-haproxy (id: 0)</span><br><span class="line">ubuntu@amphora-068104f1-ffee-4b4a-88ab-2e46cee1cbbc:~$ sudo ip netns exec amphora-haproxy ip addr show eth1</span><br><span class="line">3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1508 qdisc fq state UP group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:df:d2:3b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.21.174/24 brd 192.168.21.255 scope global eth1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.21.26/24 brd 192.168.21.255 scope global secondary eth1:0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">ubuntu@amphora-068104f1-ffee-4b4a-88ab-2e46cee1cbbc:~$ ps -ef |grep haproxy</span><br><span class="line">root      1546     1  0 01:18 ?        00:00:01 /usr/sbin/haproxy -Ws -f /var/lib/octavia/72d96673-8723-4dde-9035-66c3bd095632/haproxy.cfg -f /var/lib/octavia/haproxy-default-user-group.conf -p /var/lib/octavia/72d96673-8723-4dde-9035-66c3bd095632/72d96673-8723-4dde-9035-66c3bd095632.pid -L 7eAyDHfLMNtMDh0lrSe_vQODo0g -sf 1800</span><br><span class="line">ubuntu@amphora-068104f1-ffee-4b4a-88ab-2e46cee1cbbc:~$ sudo cat /var/lib/octavia/72d96673-8723-4dde-9035-66c3bd095632/haproxy.cfg</span><br><span class="line">global</span><br><span class="line">    daemon</span><br><span class="line">    user nobody</span><br><span class="line">    log /dev/log local0</span><br><span class="line">    log /dev/log local1 notice</span><br><span class="line">    stats socket /var/lib/octavia/72d96673-8723-4dde-9035-66c3bd095632.sock mode 0666 level user</span><br><span class="line">    maxconn 1000000</span><br><span class="line">defaults</span><br><span class="line">    log global</span><br><span class="line">    retries 3</span><br><span class="line">    option redispatch</span><br><span class="line">    option splice-request</span><br><span class="line">    option splice-response</span><br><span class="line">    option http-keep-alive</span><br><span class="line">frontend 7f26223c-63f8-490a-8aa3-0141b112bacb</span><br><span class="line">    option tcplog</span><br><span class="line">    maxconn 1000000</span><br><span class="line">    bind 192.168.21.26:80</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend 632d6cf2-020f-4c63-9ca7-4bc952f8f324:7f26223c-63f8-490a-8aa3-0141b112bacb</span><br><span class="line">    timeout client 50000</span><br><span class="line">backend 632d6cf2-020f-4c63-9ca7-4bc952f8f324:7f26223c-63f8-490a-8aa3-0141b112bacb</span><br><span class="line">    mode tcp</span><br><span class="line">    balance roundrobin</span><br><span class="line">    fullconn 1000000</span><br><span class="line">    option allbackups</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout server 50000</span><br><span class="line">    server 150e2027-ec51-469d-b5f8-675575c76c79 192.168.21.114:32315 weight 1</span><br><span class="line">    server 9b42d8f0-5719-48fe-b17c-2e7cd2b29b10 192.168.21.232:32315 weight 1</span><br><span class="line">    server d9f1f6ba-895b-43c6-ad09-362f4993aa73 192.168.21.251:32315 weight 1</span><br><span class="line">    server e7485d2c-1de5-4438-8b3b-0f74ba870895 192.168.21.87:32315 weight 1</span><br><span class="line"></span><br><span class="line">$ kubectl get svc</span><br><span class="line">NAME         TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE</span><br><span class="line">hello        LoadBalancer   10.152.183.88   10.5.150.220   80:32315/TCP   51m</span><br><span class="line">kubernetes   ClusterIP      10.152.183.1    &lt;none&gt;         443/TCP        10h</span><br><span class="line">$ juju status |grep worker</span><br><span class="line">kubernetes-worker      1.18.4   waiting    4/5  kubernetes-worker      jujucharms  682  ubuntu  exposed</span><br><span class="line">kubernetes-worker/0       waiting   allocating  6        192.168.21.128                  waiting for machine</span><br><span class="line">kubernetes-worker/1*      active    idle        7        192.168.21.114  80/tcp,443/tcp  Kubernetes worker running.</span><br><span class="line">kubernetes-worker/2       active    idle        8        192.168.21.232  80/tcp,443/tcp  Kubernetes worker running.</span><br><span class="line">kubernetes-worker/3       active    idle        10       192.168.21.251  80/tcp,443/tcp  Kubernetes worker running.</span><br><span class="line">kubernetes-worker/4       active    idle        11       192.168.21.87   80/tcp,443/tcp  Kubernetes worker running.</span><br><span class="line"></span><br><span class="line">$ juju switch zhhuabj</span><br><span class="line">mystack-regionone:admin/k8s -&gt; zhhuabj:admin/octavia</span><br><span class="line">$ nova list</span><br><span class="line">+--------------------------------------+--------------------------+--------+------------+-------------+------------------------+</span><br><span class="line">| ID                                   | Name                     | Status | Task State | Power State | Networks               |</span><br><span class="line">+--------------------------------------+--------------------------+--------+------------+-------------+------------------------+</span><br><span class="line">| 5cbd03dc-9e6b-42ea-a8a2-9ca725144df0 | juju-68d107-k8s-0        | ACTIVE | -          | Running     | private=192.168.21.219 |</span><br><span class="line">| 5cc74c32-1547-44b3-8c2d-68fcf33e5080 | juju-68d107-k8s-1        | ACTIVE | -          | Running     | private=192.168.21.127 |</span><br><span class="line">| 46dc9a58-1a51-4b45-912c-8c768a4d5f28 | juju-68d107-k8s-10       | ACTIVE | -          | Running     | private=192.168.21.251 |</span><br><span class="line">| 773a845c-f87d-4352-9c74-b3f3d354590f | juju-68d107-k8s-11       | ACTIVE | -          | Running     | private=192.168.21.87  |</span><br><span class="line">| 211a6abd-bc97-4faf-984f-987fa90f21f3 | juju-68d107-k8s-2        | ACTIVE | -          | Running     | private=192.168.21.78  |</span><br><span class="line">| df6b642a-9af8-4468-b1bf-691401c4835f | juju-68d107-k8s-3        | ACTIVE | -          | Running     | private=192.168.21.158 |</span><br><span class="line">| c6f82159-98c9-4884-a7b1-eb9e950618bb | juju-68d107-k8s-4        | ACTIVE | -          | Running     | private=192.168.21.242 |</span><br><span class="line">| e81b9566-3627-4a05-9460-415e76db9483 | juju-68d107-k8s-5        | ACTIVE | -          | Running     | private=192.168.21.217 |</span><br><span class="line">| 569087bd-9ef8-4e6f-a898-72b69daef6ea | juju-68d107-k8s-6        | ACTIVE | -          | Running     | private=192.168.21.128 |</span><br><span class="line">| 38d8887c-54ae-4b2c-b0c4-7f64542af8f1 | juju-68d107-k8s-7        | ACTIVE | -          | Running     | private=192.168.21.114 |</span><br><span class="line">| 178c5eee-945a-4295-8c94-5ad5d821f251 | juju-68d107-k8s-8        | ACTIVE | -          | Running     | private=192.168.21.232 |</span><br><span class="line">| 09a9c0c1-c795-494b-ac21-49fa3a2ef070 | juju-68d107-k8s-9        | ACTIVE | -          | Running     | private=192.168.21.147 |</span><br><span class="line">| 2d5bff68-21f1-46e0-b6b8-c83d97c8cc27 | juju-bb6752-controller-0 | ACTIVE | -          | Running     | private=192.168.21.21  |</span><br><span class="line">+--------------------------------------+--------------------------+--------+------------+-------------+------------------------+</span><br><span class="line"></span><br><span class="line">$ nova show juju-68d107-k8s-7 |grep security</span><br><span class="line">| security_groups                      | default, juju-f49ad379-07f8-4564-847b-f7999f9df56c-39ead59a-0d9c-4664-8a4d-40a74168d107, juju-f49ad379-07f8-4564-847b-f7999f9df56c-39ead59a-0d9c-4664-8a4d-40a74168d107-7                                     |</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ openstack security group list --project $(openstack project list --domain admin_domain |grep admin |awk &apos;&#123;print $2&#125;&apos;) |grep default</span><br><span class="line">| bfc84b34-6f23-4561-87ff-58077f667bea | default                                                                           | Default security group | 0d1886170941437fa46fb34508e67d24 | []   |</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ubuntu@amphora-068104f1-ffee-4b4a-88ab-2e46cee1cbbc:~$ sudo ip netns exec amphora-haproxy ping 192.168.21.114 -c 1</span><br><span class="line">sudo: unable to resolve host amphora-068104f1-ffee-4b4a-88ab-2e46cee1cbbc</span><br><span class="line">PING 192.168.21.114 (192.168.21.114) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.21.114: icmp_seq=1 ttl=64 time=1.14 ms</span><br><span class="line"></span><br><span class="line">ubuntu@amphora-068104f1-ffee-4b4a-88ab-2e46cee1cbbc:~$ sudo ip netns exec amphora-haproxy nc -w4 -vz 192.168.21.114 32315</span><br><span class="line">Connection to 192.168.21.114 32315 port [tcp/*] succeeded!</span><br><span class="line"></span><br><span class="line">#NOTE</span><br><span class="line">os security group rule create --ingress --protocol tcp --remote-group b1eff65c-b6c1-4f09-8ed6-1b163e447318 --dst-port 32315:32315 --ethertype ipv4 3f95d2fa-5ba5-4719-96ee-a0e6211c7c46&quot;</span><br><span class="line">Where remote-group is the sec group associated with the port on 192.168.100.xxx of the amphora VM</span><br><span class="line">And 3f95d2fa-5ba5-4719-96ee-a0e6211c7c46 is the default SG of the juju VMs in the k8s model</span><br><span class="line"></span><br><span class="line">$ nova list --all |grep amp</span><br><span class="line">| 966f0ec0-b48a-48b9-8078-d7406ee08311 | amphora-068104f1-ffee-4b4a-88ab-2e46cee1cbbc | 144901cff394489d9095b1361caa6872 | ACTIVE | -          | Running     | lb-mgmt-net=fc00:961f:bb53:993b:f816:3eff:fe0d:6433; private=192.168.21.174 |</span><br><span class="line">$ nova show 966f0ec0-b48a-48b9-8078-d7406ee08311 |grep sec</span><br><span class="line">| security_groups                      | lb-72d96673-8723-4dde-9035-66c3bd095632, lb-mgmt-sec-grp          |</span><br><span class="line">$ openstack security group rule list lb-72d96673-8723-4dde-9035-66c3bd095632</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+------------+-----------------------+</span><br><span class="line">| ID                                   | IP Protocol | Ethertype | IP Range  | Port Range | Remote Security Group |</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+------------+-----------------------+</span><br><span class="line">| 1197c266-015b-4967-8680-35f76328fa85 | tcp         | IPv4      | 0.0.0.0/0 | 1025:1025  | None                  |</span><br><span class="line">| 2f715ac6-f7fb-4823-871c-617559cf8d0d | tcp         | IPv4      | 0.0.0.0/0 | 80:80      | None                  |</span><br><span class="line">| dc0d368e-74f6-4cf0-a863-0e56e071ad46 | None        | IPv4      | 0.0.0.0/0 |            | None                  |</span><br><span class="line">| f38b32b6-3472-43bb-89dc-3f0ef221e95b | None        | IPv6      | ::/0      |            | None                  |</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+------------+-----------------------+</span><br><span class="line">$ openstack security group rule list lb-mgmt-sec-grp</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+------------+-----------------------+</span><br><span class="line">| ID                                   | IP Protocol | Ethertype | IP Range  | Port Range | Remote Security Group |</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+------------+-----------------------+</span><br><span class="line">| 4e913725-bc6b-441f-be3e-8ea668d8d2bd | None        | IPv6      | ::/0      |            | None                  |</span><br><span class="line">| 698d9ee1-86d2-407f-8ec8-2f1453ad2427 | tcp         | IPv6      | ::/0      | 22:22      | None                  |</span><br><span class="line">| b7605acb-feb7-4a47-ba48-c6a395fca934 | tcp         | IPv6      | ::/0      | 9443:9443  | None                  |</span><br><span class="line">| c1b5fcc9-16ee-4f7a-9488-c2096b05941e | None        | IPv4      | 0.0.0.0/0 |            | None                  |</span><br><span class="line">| dcbdaaf2-140c-41e6-8f4e-f36624628047 | icmpv6      | IPv6      | ::/0      |            | None                  |</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+------------+-----------------------+</span><br><span class="line"></span><br><span class="line">$ openstack server list |grep 114</span><br><span class="line">| 38d8887c-54ae-4b2c-b0c4-7f64542af8f1 | juju-68d107-k8s-7        | ACTIVE | private=192.168.21.114 | bionic | m1.medium |</span><br><span class="line">$ nova show 38d8887c-54ae-4b2c-b0c4-7f64542af8f1 |grep sec</span><br><span class="line">| security_groups                      | default, juju-f49ad379-07f8-4564-847b-f7999f9df56c-39ead59a-0d9c-4664-8a4d-40a74168d107, juju-f49ad379-07f8-4564-847b-f7999f9df56c-39ead59a-0d9c-4664-8a4d-40a74168d107-7                                     |</span><br><span class="line">$ openstack security group rule list juju-f49ad379-07f8-4564-847b-f7999f9df56c-39ead59a-0d9c-4664-8a4d-40a74168d107</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+-------------+--------------------------------------+</span><br><span class="line">| ID                                   | IP Protocol | Ethertype | IP Range  | Port Range  | Remote Security Group                |</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+-------------+--------------------------------------+</span><br><span class="line">| 00469e1d-109c-47ea-878a-403cc47f94f2 | tcp         | IPv6      | ::/0      | 1:65535     | eee9c463-5689-460e-9ca5-22f880e1a761 |</span><br><span class="line">| 2134cf1f-a82d-43e4-a73f-8b1c06c2e400 | icmp        | IPv4      | 0.0.0.0/0 |             | eee9c463-5689-460e-9ca5-22f880e1a761 |</span><br><span class="line">| 2a5e6886-4657-4227-b9e6-215a121c70c5 | tcp         | IPv6      | ::/0      | 22:22       | None                                 |</span><br><span class="line">| 430c12ff-0a15-4878-90b0-67ff18ef5cb7 | None        | IPv4      | 0.0.0.0/0 |             | None                                 |</span><br><span class="line">| 4a4644ef-6563-492f-8732-749a5b02eb78 | udp         | IPv4      | 0.0.0.0/0 | 1:65535     | eee9c463-5689-460e-9ca5-22f880e1a761 |</span><br><span class="line">| 8435b75a-f40b-4adb-9eda-1bbef5aecca6 | icmp        | IPv6      | ::/0      |             | eee9c463-5689-460e-9ca5-22f880e1a761 |</span><br><span class="line">| 9adf865c-04ed-40db-a506-6a9687efb1b0 | None        | IPv6      | ::/0      |             | None                                 |</span><br><span class="line">| a68fac61-ed0c-4027-96ff-e98d182a287d | tcp         | IPv6      | ::/0      | 17070:17070 | None                                 |</span><br><span class="line">| b475b293-b071-4ae9-a3ab-b8a29cbcce33 | tcp         | IPv4      | 0.0.0.0/0 | 17070:17070 | None                                 |</span><br><span class="line">| c5450f4c-7c82-45a2-a676-4fe3ee413ff5 | tcp         | IPv4      | 0.0.0.0/0 | 1:65535     | eee9c463-5689-460e-9ca5-22f880e1a761 |</span><br><span class="line">| cf8dd9b6-f59f-4d90-9bbf-76eeb1754680 | udp         | IPv6      | ::/0      | 1:65535     | eee9c463-5689-460e-9ca5-22f880e1a761 |</span><br><span class="line">| f8246bec-4d07-4764-8f37-dd2c204e80e4 | tcp         | IPv4      | 0.0.0.0/0 | 22:22       | None                                 |</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+-------------+--------------------------------------+</span><br><span class="line">$ openstack security group rule list juju-f49ad379-07f8-4564-847b-f7999f9df56c-39ead59a-0d9c-4664-8a4d-40a74168d107-7</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+------------+-----------------------+</span><br><span class="line">| ID                                   | IP Protocol | Ethertype | IP Range  | Port Range | Remote Security Group |</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+------------+-----------------------+</span><br><span class="line">| 115688b5-ab4d-4614-afc1-db1458d67ea5 | tcp         | IPv4      | 0.0.0.0/0 | 443:443    | None                  |</span><br><span class="line">| 77bb8ed0-2e7c-4e2f-8320-9acf59b50301 | tcp         | IPv4      | 0.0.0.0/0 | 80:80      | None                  |</span><br><span class="line">| 8c412473-74f0-410f-bd16-7851404743bc | None        | IPv4      | 0.0.0.0/0 |            | None                  |</span><br><span class="line">| ba9a07f9-07d6-4222-ae3c-57255d4a8f83 | None        | IPv6      | ::/0      |            | None                  |</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+------------+-----------------------+</span><br><span class="line">$ openstack security group list --project $(openstack project list --domain admin_domain |grep admin |awk &apos;&#123;print $2&#125;&apos;) |grep default</span><br><span class="line">| bfc84b34-6f23-4561-87ff-58077f667bea | default                                                                           | Default security group | 0d1886170941437fa46fb34508e67d24 | []   |</span><br><span class="line"> openstack security group rule list bfc84b34-6f23-4561-87ff-58077f667bea</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+------------+--------------------------------------+</span><br><span class="line">| ID                                   | IP Protocol | Ethertype | IP Range  | Port Range | Remote Security Group                |</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+------------+--------------------------------------+</span><br><span class="line">| 12a3c6b3-a4c4-470d-a4d2-dcf6cde5b740 | tcp         | IPv4      | 0.0.0.0/0 | 443:443    | None                                 |</span><br><span class="line">| 13068f00-11d4-4cec-b145-a71b68ffc583 | icmp        | IPv4      | 0.0.0.0/0 |            | None                                 |</span><br><span class="line">| 39ab4cb5-4bee-4272-9811-4e405ae568aa | None        | IPv6      | ::/0      |            | None                                 |</span><br><span class="line">| 3d005483-bd84-467b-8cc6-427771e68645 | tcp         | IPv4      | 0.0.0.0/0 | 80:80      | None                                 |</span><br><span class="line">| 436b644b-cf5f-4dfc-a5b0-f86159f8a3fb | None        | IPv4      | 0.0.0.0/0 |            | None                                 |</span><br><span class="line">| 985dc09f-213c-426e-aedf-35b673fd5830 | tcp         | IPv4      | 0.0.0.0/0 | 53:53      | None                                 |</span><br><span class="line">| 9d936527-04c0-490f-9249-27e2f156d452 | None        | IPv6      | ::/0      |            | bfc84b34-6f23-4561-87ff-58077f667bea |</span><br><span class="line">| e7179f46-a322-43fd-aeef-fce38685e749 | tcp         | IPv4      | 0.0.0.0/0 | 22:22      | None                                 |</span><br><span class="line">| f5b12670-7c76-42e7-9cd8-e0fd1ba5eaef | None        | IPv4      | 0.0.0.0/0 |            | bfc84b34-6f23-4561-87ff-58077f667bea |</span><br><span class="line">| f9fdfe84-b239-4adf-96c5-94402debbf1c | None        | IPv6      | ::/0      |            | None                                 |</span><br><span class="line">| fdb8a39b-747b-4b52-96a3-e8cc0110d6b7 | None        | IPv4      | 0.0.0.0/0 |            | None                                 |</span><br><span class="line">+--------------------------------------+-------------+-----------+-----------+------------+--------------------------------------+</span><br></pre></td></tr></table></figure></p>
<h2 id="20190904更新-octavia测试环境搭建全过程"><a href="#20190904更新-octavia测试环境搭建全过程" class="headerlink" title="20190904更新 - octavia测试环境搭建全过程"></a>20190904更新 - octavia测试环境搭建全过程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br></pre></td><td class="code"><pre><span class="line">juju kill-controller zhhuabj -y -t 1s  #or delete vm directly</span><br><span class="line">rm .local/share/juju/controllers.yaml</span><br><span class="line">#modify your ~/juju_config/2.x/bootstrap.sh to add &apos;container-networking-method=&quot;provider&quot;&apos; in model_defaults variable and comment proxy parts</span><br><span class="line">~/juju_config/2.x/gencloud.sh</span><br><span class="line"></span><br><span class="line">./generate-bundle.sh --name octavia --create-model --run --octavia -r stein --dvr-snat --num-compute 2</span><br><span class="line">./bin/add-data-ports.sh</span><br><span class="line">juju config neutron-openvswitch data-port=&quot;br-data:ens7&quot;</span><br><span class="line">#refere here to create certs - https://docs.openstack.org/project-deploy-guide/charm-deployment-guide/latest/app-octavia.html</span><br><span class="line">juju config octavia \</span><br><span class="line">    lb-mgmt-issuing-cacert=&quot;$(base64 controller_ca.pem)&quot; \</span><br><span class="line">    lb-mgmt-issuing-ca-private-key=&quot;$(base64 controller_ca_key.pem)&quot; \</span><br><span class="line">    lb-mgmt-issuing-ca-key-passphrase=foobar \</span><br><span class="line">    lb-mgmt-controller-cacert=&quot;$(base64 controller_ca.pem)&quot; \</span><br><span class="line">    lb-mgmt-controller-cert=&quot;$(base64 controller_cert_bundle.pem)&quot;</span><br><span class="line">juju run-action --wait octavia/0 configure-resources</span><br><span class="line">BARE_METAL=TRUE ./configure</span><br><span class="line">juju run-action octavia-diskimage-retrofit/0 --wait retrofit-image image-id=$(openstack image list |grep bionic |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line"></span><br><span class="line">wget https://people.canonical.com/~zhhuabj/amphora-x64-haproxy.qcow2  #ssh root@&lt;amphora-ip, password:123qwe</span><br><span class="line">#scp -i ~/.ssh/phykey amphora-x64-haproxy.qcow2 zhhuabj@people.canonical.com:/home/zhhuabj/public_html/</span><br><span class="line">source ~/stsstack-bundles/openstack/novarc</span><br><span class="line">glance image-create --tag octavia-amphora --disk-format qcow2 --name amphora-haproxy-xenial --file ./amphora-x64-haproxy.qcow2 --visibility public --container-format bare --progress</span><br><span class="line">#注意, 20191206更新</span><br><span class="line"># 镜像应该由octavia-diskimage-retrofit来创建来操作apmphora client与api版本一致, 否则会造成service vm无法work</span><br><span class="line">#https://github.com/openstack-charmers/octavia-diskimage-retrofit</span><br><span class="line">#amphora vms have to be created using the uca that corresponds to the release of openstack deployed</span><br><span class="line">#so that amphora client and api versions match, you can build one with the octavia-diskimage-retrofit charm</span><br><span class="line">sudo snap install --edge --devmode octavia-diskimage-retrofit</span><br><span class="line">sudo -s</span><br><span class="line">wget https://cloud-images.ubuntu.com/minimal/releases/bionic/release/ubuntu-18.04-minimal-cloudimg-amd64.img</span><br><span class="line">- or -</span><br><span class="line">wget https://cloud-images.ubuntu.com/minimal/daily/bionic/current/bionic-minimal-cloudimg-amd64.img</span><br><span class="line">sudo mv ubuntu-18.04-minimal-cloudimg-amd64.img /var/snap/octavia-diskimage-retrofit/common</span><br><span class="line">sudo octavia-diskimage-retrofit /var/snap/octavia-diskimage-retrofit/common/ubuntu-18.04-minimal-cloudimg-amd64.img /var/snap/octavia-diskimage-retrofit/common/ubuntu-amphora-haproxy-amd64.qcow2 -d u stein</span><br><span class="line">openstack image create --disk-format qcow2 --container-format bare --public --tag octavia-amphora --file /var/snap/octavia-diskimage-retrofit/common/ubuntu-amphora-haproxy-amd64.qcow2 amphora-bionic-x64-haproxy</span><br><span class="line"></span><br><span class="line">#create a test backend</span><br><span class="line">./tools/instance_launch.sh 1 xenial</span><br><span class="line">./tools/sec_groups.sh</span><br><span class="line">fix_ip=$(openstack server list |grep &apos;private=&apos; |awk -F &apos;=&apos; &apos;&#123;print $2&#125;&apos; |awk &apos;&#123;print $1&#125;&apos;)</span><br><span class="line">ext_net=$(openstack network show ext_net -f value -c id)</span><br><span class="line">fip=$(openstack floating ip create $ext_net -f value -c floating_ip_address)</span><br><span class="line">openstack floating ip set $fip --fixed-ip-address $fix_ip --port $(openstack port list --fixed-ip ip-address=$fix_ip -c id -f value)</span><br><span class="line">ssh -i ~/testkey.priv ubuntu@$fip -- sudo apt install python-minimal -y</span><br><span class="line">ssh -i ~/testkey.priv ubuntu@$fip -- sudo python -m SimpleHTTPServer 80 &amp;</span><br><span class="line">curl $fip</span><br><span class="line"></span><br><span class="line">#backend ip 192.168.21.252 (fip: 10.5.151.97)</span><br><span class="line">#service vm fc00:a895:61e6:b86f:f816:3eff:fef7:26f5/192.168.21.54  (vip: 192.168.21.117, fip: 10.5.151.155)</span><br><span class="line">sudo apt install python-octaviaclient python3-octaviaclient</span><br><span class="line">openstack complete |sudo tee /etc/bash_completion.d/openstack</span><br><span class="line">source &lt;(openstack complete)</span><br><span class="line">#No module named &apos;oslo_log&apos;</span><br><span class="line">openstack loadbalancer create --name lb1 --vip-subnet-id private_subnet</span><br><span class="line">#lb_vip_port_id=$(openstack loadbalancer create -f value -c vip_port_id --name lb1 --vip-subnet-id private_subnet)</span><br><span class="line"># Re-run the following until lb1 shows ACTIVE and ONLINE statuses:</span><br><span class="line">openstack loadbalancer show lb1</span><br><span class="line">nova list --all</span><br><span class="line">openstack loadbalancer listener create --name listener1 --protocol HTTP --protocol-port 80 lb1</span><br><span class="line">openstack loadbalancer pool create --name pool1 --lb-algorithm ROUND_ROBIN --listener listener1 --protocol HTTP</span><br><span class="line">openstack loadbalancer member create --subnet-id private_subnet --address $fix_ip --protocol-port 80 pool1</span><br><span class="line">openstack loadbalancer member list pool1</span><br><span class="line">vip=$(openstack loadbalancer show lb1 -f value -c vip_address)</span><br><span class="line">vip_fip=$(openstack floating ip create $ext_net -f value -c floating_ip_address)</span><br><span class="line">openstack floating ip set $vip_fip --fixed-ip-address $vip --port $(openstack port list --fixed-ip ip-address=$vip -c id -f value)</span><br><span class="line">curl $vip_fip</span><br><span class="line"></span><br><span class="line">#ssh into service vm</span><br><span class="line">nova list --all</span><br><span class="line">juju ssh nova-compute/1 -- sudo ip netns exec qrouter-73d87977-2eaf-40ba-818d-6a17aecd1d16 ping6 fc00:a895:61e6:b86f:f816:3eff:fef7:26f5</span><br><span class="line">#password: 123qwe</span><br><span class="line">juju ssh nova-compute/1 -- sudo ip netns exec qrouter-73d87977-2eaf-40ba-818d-6a17aecd1d16 ssh -6 root@fc00:a895:61e6:b86f:f816:3eff:fef7:26f5</span><br><span class="line">#need to add icmp firewall rule by hand when using ipv4 address</span><br><span class="line">#juju ssh nova-compute/1 -- sudo ip netns exec qrouter-891c4da6-c03b-4a56-a901-a5efb1dbcd15 ssh root@192.168.21.54</span><br><span class="line">#openstack security group rule create lb-4fca1a12-fd1d-4da3-bf7b-e48386c5c1a2 --protocol icmp --remote-ip 0.0.0.0/0</span><br><span class="line">#openstack security group rule create lb-4fca1a12-fd1d-4da3-bf7b-e48386c5c1a2 --protocol tcp --dst-port 22</span><br><span class="line"></span><br><span class="line"># cat /var/lib/octavia/27057bca-c504-4ca2-9bff-89b342767afd/haproxy.cfg</span><br><span class="line"># Configuration for loadbalancer 4fca1a12-fd1d-4da3-bf7b-e48386c5c1a2</span><br><span class="line">global</span><br><span class="line">    daemon</span><br><span class="line">    user nobody</span><br><span class="line">    log /dev/log local0</span><br><span class="line">    log /dev/log local1 notice</span><br><span class="line">    stats socket /var/lib/octavia/27057bca-c504-4ca2-9bff-89b342767afd.sock mode 0666 level user</span><br><span class="line">    maxconn 1000000</span><br><span class="line">defaults</span><br><span class="line">    log global</span><br><span class="line">    retries 3</span><br><span class="line">    option redispatch</span><br><span class="line">    option splice-request</span><br><span class="line">    option splice-response</span><br><span class="line">    option http-keep-alive</span><br><span class="line">frontend 27057bca-c504-4ca2-9bff-89b342767afd</span><br><span class="line">    option httplog</span><br><span class="line">    maxconn 1000000</span><br><span class="line">    bind 192.168.21.117:80</span><br><span class="line">    mode http</span><br><span class="line">    default_backend 87d56822-1f5c-4a47-88d6-ddd5d038523d</span><br><span class="line">    timeout client 50000</span><br><span class="line">backend 87d56822-1f5c-4a47-88d6-ddd5d038523d</span><br><span class="line">    mode http</span><br><span class="line">    http-reuse safe</span><br><span class="line">    balance roundrobin</span><br><span class="line">    fullconn 1000000</span><br><span class="line">    option allbackups</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout server 50000</span><br><span class="line">    server 4dcd830b-33b2-49e7-b50b-e91b2ce65afb 192.168.21.252:80 weight 1</span><br><span class="line"></span><br><span class="line">root@amphora-91c5098c-7578-4de7-b38e-d3712711bb15:~# cat /etc/netns/amphora-haproxy/network/interfaces.d/eth1.cfg</span><br><span class="line"># Generated by Octavia agent</span><br><span class="line">auto eth1 eth1:0</span><br><span class="line">iface eth1 inet static</span><br><span class="line">address 192.168.21.54</span><br><span class="line">broadcast 192.168.21.255</span><br><span class="line">netmask 255.255.255.0</span><br><span class="line">gateway 192.168.21.1</span><br><span class="line">mtu 1458</span><br><span class="line">iface eth1:0 inet static</span><br><span class="line">address 192.168.21.117</span><br><span class="line">broadcast 192.168.21.255</span><br><span class="line">netmask 255.255.255.0</span><br><span class="line"># Add a source routing table to allow members to access the VIP</span><br><span class="line">post-up /sbin/ip route add default via 192.168.21.1 dev eth1 onlink table 1</span><br><span class="line">post-down /sbin/ip route del default via 192.168.21.1 dev eth1 onlink table 1</span><br><span class="line">post-up /sbin/ip route add 192.168.21.0/24 dev eth1 src 192.168.21.117 scope link table 1</span><br><span class="line">post-down /sbin/ip route del 192.168.21.0/24 dev eth1 src 192.168.21.117 scope link table 1</span><br><span class="line">post-up /sbin/ip rule add from 192.168.21.117/32 table 1 priority 100</span><br><span class="line">post-down /sbin/ip rule del from 192.168.21.117/32 table 1 priority 100</span><br><span class="line">post-up /sbin/iptables -t nat -A POSTROUTING -p udp -o eth1 -j MASQUERADE</span><br><span class="line">post-down /sbin/iptables -t nat -D POSTROUTING -p udp -o eth1 -j MASQUERADEroot@amphora-91c5098c-7578-4de7-b38e-d3712711bb15</span><br><span class="line"></span><br><span class="line">root@amphora-91c5098c-7578-4de7-b38e-d3712711bb15:~# ip netns exec amphora-haproxy ip -4 addr show eth1</span><br><span class="line">3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1458 qdisc fq_codel state UP group default qlen 1000</span><br><span class="line">    inet 192.168.21.54/24 brd 192.168.21.255 scope global eth1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.21.117/24 brd 192.168.21.255 scope global secondary eth1:0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line">root@amphora-91c5098c-7578-4de7-b38e-d3712711bb15:~# ip netns exec amphora-haproxy ip route list</span><br><span class="line">default via 192.168.21.1 dev eth1 onlink</span><br><span class="line">192.168.21.0/24 dev eth1 proto kernel scope link src 192.168.21.54</span><br><span class="line">root@amphora-91c5098c-7578-4de7-b38e-d3712711bb15:~# ip netns exec amphora-haproxy ip route list table 1</span><br><span class="line">default via 192.168.21.1 dev eth1 onlink</span><br><span class="line">192.168.21.0/24 dev eth1 scope link src 192.168.21.117</span><br><span class="line"></span><br><span class="line">root@amphora-91c5098c-7578-4de7-b38e-d3712711bb15:~# ip netns exec amphora-haproxy ip route show table all</span><br><span class="line">default via 192.168.21.1 dev eth1 table 1 onlink</span><br><span class="line">192.168.21.0/24 dev eth1 table 1 scope link src 192.168.21.117</span><br><span class="line">default via 192.168.21.1 dev eth1 onlink</span><br><span class="line">192.168.21.0/24 dev eth1 proto kernel scope link src 192.168.21.54</span><br><span class="line">broadcast 192.168.21.0 dev eth1 table local proto kernel scope link src 192.168.21.54</span><br><span class="line">local 192.168.21.54 dev eth1 table local proto kernel scope host src 192.168.21.54</span><br><span class="line">local 192.168.21.117 dev eth1 table local proto kernel scope host src 192.168.21.54</span><br><span class="line">broadcast 192.168.21.255 dev eth1 table local proto kernel scope link src 192.168.21.54</span><br><span class="line">ff00::/8 dev eth1 table local metric 256 pref medium</span><br><span class="line"></span><br><span class="line">root@amphora-91c5098c-7578-4de7-b38e-d3712711bb15:~# ip netns exec amphora-haproxy iptables-save</span><br><span class="line"># Generated by iptables-save v1.6.1 on Wed Sep  4 04:00:59 2019</span><br><span class="line">*nat</span><br><span class="line">:PREROUTING ACCEPT [3:448]</span><br><span class="line">:INPUT ACCEPT [3:448]</span><br><span class="line">:OUTPUT ACCEPT [1:60]</span><br><span class="line">:POSTROUTING ACCEPT [1:60]</span><br><span class="line">-A POSTROUTING -o eth1 -p udp -j MASQUERADE</span><br><span class="line">COMMIT</span><br><span class="line"># Completed on Wed Sep  4 04:00:59 2019</span><br><span class="line"></span><br><span class="line">#backend ip 192.168.21.252 (fip: 10.5.151.97)</span><br><span class="line">#service vm fc00:a895:61e6:b86f:f816:3eff:fef7:26f5/192.168.21.54  (vip: 192.168.21.117, fip: 10.5.151.155)</span><br><span class="line"></span><br><span class="line">#ping backend from service vm</span><br><span class="line">root@amphora-91c5098c-7578-4de7-b38e-d3712711bb15:~# ip netns exec amphora-haproxy ping 192.168.21.252</span><br><span class="line">PING 192.168.21.252 (192.168.21.252) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.21.252: icmp_seq=1 ttl=64 time=3.45 ms</span><br><span class="line"></span><br><span class="line">#ping service vm vip from backend</span><br><span class="line">ubuntu@xenial-030345:~$ ping -c 1 192.168.21.54</span><br><span class="line">PING 192.168.21.54 (192.168.21.54) 56(84) bytes of data.</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ nova show aa09d2d0-a8fb-4a2f-bc8f-f39c6dff6713 |grep security_groups</span><br><span class="line">| security_groups                      | lb-4fca1a12-fd1d-4da3-bf7b-e48386c5c1a2, lb-mgmt-sec-grp      |</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ openstack security group rule list lb-mgmt-sec-grp</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">| ID                                   | IP Protocol | IP Range | Port Range | Remote Security Group |</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">| 1bb77bae-6e4c-4982-8c00-8ebfafd896c7 | icmpv6      | None     |            | None                  |</span><br><span class="line">| 407d8dd6-a0c3-406f-b16d-2453ae4ad015 | tcp         | None     | 9443:9443  | None                  |</span><br><span class="line">| 4d927c00-b4aa-4033-ab66-b96fdb2d9722 | None        | None     |            | None                  |</span><br><span class="line">| 771122a1-ad5e-4842-8ba3-dcd0b950d47a | None        | None     |            | None                  |</span><br><span class="line">| 8b91fd37-dddf-4200-8835-261c203696d0 | tcp         | None     | 22:22      | None                  |</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ openstack security group rule list lb-4fca1a12-fd1d-4da3-bf7b-e48386c5c1a2</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">| ID                                   | IP Protocol | IP Range | Port Range | Remote Security Group |</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">| 4eb66163-44dc-44d2-b969-a890d35986a6 | tcp         | None     | 80:80      | None                  |</span><br><span class="line">| 80e0d109-9f97-4559-bc20-de89c001725e | tcp         | None     | 1025:1025  | None                  |</span><br><span class="line">| 92c09499-c9ad-4682-8697-9c17cc33e785 | None        | None     |            | None                  |</span><br><span class="line">| feeb42a2-b49b-4f89-8b3e-5a30cee1e08f | None        | None     |            | None                  |</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line"></span><br><span class="line">openstack security group rule create lb-4fca1a12-fd1d-4da3-bf7b-e48386c5c1a2 --protocol icmp --remote-ip 0.0.0.0/0</span><br><span class="line">openstack security group rule create lb-4fca1a12-fd1d-4da3-bf7b-e48386c5c1a2 --protocol tcp --dst-port 22</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ openstack security group rule list lb-4fca1a12-fd1d-4da3-bf7b-e48386c5c1a2</span><br><span class="line">+--------------------------------------+-------------+-----------+------------+-----------------------+</span><br><span class="line">| ID                                   | IP Protocol | IP Range  | Port Range | Remote Security Group |</span><br><span class="line">+--------------------------------------+-------------+-----------+------------+-----------------------+</span><br><span class="line">| 4eb66163-44dc-44d2-b969-a890d35986a6 | tcp         | None      | 80:80      | None                  |</span><br><span class="line">| 80e0d109-9f97-4559-bc20-de89c001725e | tcp         | None      | 1025:1025  | None                  |</span><br><span class="line">| 92c09499-c9ad-4682-8697-9c17cc33e785 | None        | None      |            | None                  |</span><br><span class="line">| 936b47f0-3897-4abe-89fe-37d7c11862ea | icmp        | 0.0.0.0/0 |            | None                  |</span><br><span class="line">| bd832940-7e80-414d-b43b-51042084b934 | tcp         | 0.0.0.0/0 | 22:22      | None                  |</span><br><span class="line">| feeb42a2-b49b-4f89-8b3e-5a30cee1e08f | None        | None      |            | None                  |</span><br><span class="line">+--------------------------------------+-------------+-----------+------------+-----------------------+</span><br><span class="line"></span><br><span class="line">ubuntu@xenial-030345:~$ ping -c 1 10.5.151.155</span><br><span class="line">PING 10.5.151.155 (10.5.151.155) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.5.151.155: icmp_seq=1 ttl=60 time=37.2 ms</span><br><span class="line"></span><br><span class="line">客户遇到的问题:</span><br><span class="line">1, arp issue - https://bugs.launchpad.net/neutron/+bug/1794991/comments/59</span><br><span class="line">2, fip issue - 对于octavia的计算节点使用了没有data-port的neutron-openvswitch charm, dvr-snat会在一堆计算节点中找一个来安装snat-xxx, 这样当恰好找了一个没有data-port的ocatavia计算节点就出问题了 (如使用了https://bugs.launchpad.net/charm-neutron-openvswitch/+bug/1822558中提到的neutron-openvswitch-octavia)</span><br><span class="line">3, https://bugs.launchpad.net/charm-neutron-openvswitch/+bug/1843557</span><br><span class="line">4, 最后建议使用一个existing provider network解决 （一个tenant看不见它的话需要使用rbac设置共享)</span><br></pre></td></tr></table></figure>
<h2 id="20191207更新"><a href="#20191207更新" class="headerlink" title="20191207更新"></a>20191207更新</h2><p>遇到又一例, service vm不work, 创建lb后, service vm是ACTIVE状态其管理网段IP也work, 但LB的状态是PENDING_STATE, 但LB有VIP也可以ssh, 但过一会之后, service vm变成ERROR状态且被删除. 查出来的原因是:<br>During verification of the agent update, It was also found that firewall changes introduced caused DNS to become unreachable for the octavia load balancer instances. These rules have been updated to allow access, which allowed DNS to work inside the amphora VMs, which in turn, in combination with disabling a host, and the update of the agent,returned the load balancers to a working ‘state.”</p>
<h2 id="20200220更新"><a href="#20200220更新" class="headerlink" title="20200220更新"></a>20200220更新</h2><p>用下列方法更新添加nbthread元素之后，LB创建成功后一会儿死掉，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">juju config octavia haproxy-template | base64 -d &gt; /tmp/haproxy-custom.j2</span><br><span class="line">vim /tmp/haproxy-custom.j2 # edit the nbthread option</span><br><span class="line">nbproc 1</span><br><span class="line">nbthread 2</span><br><span class="line">cpu-map auto:1/1-2 0-1</span><br><span class="line">maxconns=64000</span><br><span class="line">juju config octavia haproxy-template=&quot;$(base64 /tmp/haproxy-custom.j2)&quot;</span><br></pre></td></tr></table></figure></p>
<p>这种问题一般能从octavia-worker.log中找到答案：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2020-02-19 04:53:11.465 14180 ERROR octavia.controller.worker.controller_worker jinja2.exceptions.UndefinedError: &apos;dict object&apos; has no attribute &apos;listener&apos;</span><br></pre></td></tr></table></figure></p>
<p>原因是这个patch(<a href="https://review.opendev.org/#/c/673518/)引入了combined_listeners" target="_blank" rel="external">https://review.opendev.org/#/c/673518/)引入了combined_listeners</a>, combined_listeners是为了解决下列问题：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Since 1.8.x version, haproxy consumes at least 160MB at init time using the default configuration provided by octavia. When a LB is updated, haproxy configuration is updated and a signal is sent to haproxy to reload the configuration.</span><br><span class="line">When haproxy reloads its configuration, it creates a new worker, does some allocations (up to 160MB), then destroys the previous worker. So during a short time, memory consumption is increased, and if 2 processes reload a the same time, it may fail with a &quot;Cannot fork&quot; error.</span><br></pre></td></tr></table></figure></p>
<p>所以custom template (<a href="https://paste.ubuntu.com/p/PGvD7fzjd2/" target="_blank" rel="external">https://paste.ubuntu.com/p/PGvD7fzjd2/</a> )中的之前的split_listeners的中loadbalancer.listener.pools应该改成现在combined_listeners中的loadbalancer.listeners.pools</p>
<h2 id="20200427更新"><a href="#20200427更新" class="headerlink" title="20200427更新 -"></a>20200427更新 -</h2><p>另一个问题, 如下, socket.getfqdn在处理/etc/hosts里的fqdn(见: <a href="https://bugs.python.org/issue5004" target="_blank" rel="external">https://bugs.python.org/issue5004</a>), 所以还是要使用socket.getaddrinfo代替socket.getfqdn来处理fqdn (neutron agent现在改成使用fqdn注册), 但观察到socket.getaddrinfo时时而能获取到fqdn时而不能获取, 从而导致octavia里的出现binding error从而导致o-hm0无法获取IP.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@juju-5b1810-octavia-6:~$ python3 -c &apos;import socket; print(socket.getaddrinfo(&quot;juju-5b1810-octavia-6&quot;, None, 0, socket.SOCK_DGRAM, 0,</span><br><span class="line">socket.AI_CANONNAME)[0][3])&apos;</span><br><span class="line">juju-5b1810-octavia-6</span><br><span class="line"></span><br><span class="line">ubuntu@juju-5b1810-octavia-6:~$ python3 -c &apos;import socket; print(socket.getfqdn(&quot;juju-5b1810-octavia-6&quot;))&apos;</span><br><span class="line">juju-5b1810-octavia-6.cloud.sts</span><br><span class="line"></span><br><span class="line">ubuntu@juju-5b1810-octavia-6:~$ python3 -c &apos;import socket; print(socket.getaddrinfo(&quot;juju-5b1810-octavia-6&quot;, None, 0, socket.SOCK_DGRAM, 0,</span><br><span class="line">&gt; socket.AI_CANONNAME))&apos;</span><br><span class="line">[(&lt;AddressFamily.AF_INET: 2&gt;, &lt;SocketKind.SOCK_DGRAM: 2&gt;, 17, &apos;juju-5b1810-octavia-6&apos;, (&apos;10.5.0.14&apos;, 0)), (&lt;AddressFamily.AF_INET: 2&gt;, &lt;SocketKind.SOCK_DGRAM: 2&gt;, 17, &apos;&apos;, (&apos;252.0.14.1&apos;, 0)), (&lt;AddressFamily.AF_INET6: 10&gt;, &lt;SocketKind.SOCK_DGRAM: 2&gt;, 17, &apos;&apos;, (&apos;fe80::f816:3eff:fe8a:6e3&apos;, 0, 0, 0)), (&lt;AddressFamily.AF_INET6: 10&gt;, &lt;SocketKind.SOCK_DGRAM: 2&gt;, 17, &apos;&apos;, (&apos;fe80::34b8:fff:feea:4717&apos;, 0, 0, 0)), (&lt;AddressFamily.AF_INET6: 10&gt;, &lt;SocketKind.SOCK_DGRAM: 2&gt;, 17, &apos;&apos;, (&apos;fe80::34b8:fff:feea:4717&apos;, 0, 0, 0))]</span><br><span class="line"></span><br><span class="line">ubuntu@juju-5b1810-octavia-6:~$ hostname --fqdn</span><br><span class="line">juju-5b1810-octavia-6</span><br><span class="line">ubuntu@juju-5b1810-octavia-6:~$ hostname --all-fqdns</span><br><span class="line">juju-5b1810-octavia-6.cloud.sts juju-5b1810-octavia-6</span><br><span class="line">ubuntu@juju-5b1810-octavia-6:~$ hostname -f</span><br><span class="line">juju-5b1810-octavia-6</span><br></pre></td></tr></table></figure></p>
<p>尤其是vm里的/etc/resovle.conf里用到了如maas dns与neutron ml2-dns等多个search项时, ml2-dns的search项排到第一位时会导致octavia上的l2-agent无法正常处理fqdn从而导致 o-hm0无IP, 见: <a href="https://bugs.launchpad.net/charm-octavia/+bug/1845303/comments/15" target="_blank" rel="external">https://bugs.launchpad.net/charm-octavia/+bug/1845303/comments/15</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">while true; do for X in &#123;0..2&#125;; do juju ssh octavia/$X &quot;cat /etc/resolv.conf | grep -v &quot;^#&quot;; sudo python -c &apos;import socket; name=socket.gethostname(); addrs = socket.getaddrinfo(name, None, 0, socket.SOCK_DGRAM, 0, socket.AI_CANONNAME); print(addrs)&apos;&quot; 2&gt;/dev/null; done; done</span><br></pre></td></tr></table></figure></p>
<p>解决方法是在systemd-network中使用UseDomains=route<br>UseDomains接受布尔参数或特殊值“ route”。设置为true时，将从DHCP服务器(neutron dns)收到的域名用作DNS, 通过该链接搜索域。如果设置为“ route”，则从DHCP接收的域名 , 服务器将仅用于路由DNS查询.</p>
<h2 id="产生密钥"><a href="#产生密钥" class="headerlink" title="产生密钥"></a>产生密钥</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">#https://www.dazhuanlan.com/2019/12/15/5df63aed10999</span><br><span class="line">#generate ca key pairs</span><br><span class="line">mkdir -p ca/&#123;private,certs,newcerts&#125; &amp;&amp; cd ca</span><br><span class="line">openssl genrsa -aes256 -passout pass:password -out private/ca.key.pem 4096</span><br><span class="line">chmod 400 private/ca.key.pem</span><br><span class="line">wget https://jamielinux.com/docs/openssl-certificate-authority/_downloads/root-config.txt -O openssl.cnf</span><br><span class="line">sed -i &quot;s,/root/ca,.,g&quot; openssl.cnf</span><br><span class="line">openssl req -config ./openssl.cnf -key private/ca.key.pem -new -x509 -days 7300 -sha256 -extensions v3_ca -passin pass:password \</span><br><span class="line">     -subj &quot;/C=CN/ST=BJ/O=STS/OU=quqi.com/CN=quqi.com.rootca/emailAddress=quqi@mail.com&quot; -out certs/ca.cert.pem</span><br><span class="line">chmod 444 certs/ca.cert.pem</span><br><span class="line">openssl x509 -noout -text -in certs/ca.cert.pem #verify</span><br><span class="line"></span><br><span class="line">#generate intermediate key pairs</span><br><span class="line">mkdir -p intermediate/&#123;certs,crl,csr,newcerts,private&#125;</span><br><span class="line">chmod 744 intermediate/private</span><br><span class="line">touch index.txt &amp;&amp; echo 1000 &gt; serial &amp;&amp; echo 1000 &gt; crlnumber</span><br><span class="line">openssl genrsa -aes256 -passout pass:password -out intermediate/private/intermediate.key.pem 4096</span><br><span class="line">chmod 400 intermediate/private/intermediate.key.pem</span><br><span class="line">cp ./openssl.cnf ./openssl-im.cnf</span><br><span class="line">#modify the following section of openssl-im.cnf file</span><br><span class="line">[ CA_default ]</span><br><span class="line">dir             = .</span><br><span class="line">private_key     = $dir/private/intermediate.key.pem</span><br><span class="line">certificate     = $dir/certs/intermediate.cert.pem</span><br><span class="line">crl             = $dir/crl/intermediate.crl.pem</span><br><span class="line">policy          = policy_loose</span><br><span class="line">openssl req -config ./openssl-im.cnf -new -sha256 -passin pass:password \</span><br><span class="line">     -subj &quot;/C=CN/ST=BJ/O=STS/OU=quqi.com/CN=quqi.com.imca/emailAddress=quqi@mail.com&quot; \</span><br><span class="line">     -key intermediate/private/intermediate.key.pem -out intermediate/csr/intermediate.csr.pem</span><br><span class="line">openssl ca -config ./openssl.cnf -extensions v3_intermediate_ca -days 3650 -notext -md sha256 -passin pass:password \</span><br><span class="line">     -in intermediate/csr/intermediate.csr.pem -out intermediate/certs/intermediate.cert.pem</span><br><span class="line">chmod 444 intermediate/certs/intermediate.cert.pem</span><br><span class="line">openssl x509 -noout -text -in intermediate/certs/intermediate.cert.pem</span><br><span class="line">openssl verify -CAfile certs/ca.cert.pem intermediate/certs/intermediate.cert.pem</span><br><span class="line"></span><br><span class="line">#generate certificate chain</span><br><span class="line">cat intermediate/certs/intermediate.cert.pem certs/ca.cert.pem &gt; intermediate/certs/ca-chain.cert.pem</span><br><span class="line">chmod 444 intermediate/certs/ca-chain.cert.pem</span><br><span class="line"></span><br><span class="line">#generate clinet.quqi.com key pairs</span><br><span class="line">openssl genrsa -out intermediate/private/client.quqi.com.key.pem 2048</span><br><span class="line">chmod 444 intermediate/private/client.quqi.com.key.pem</span><br><span class="line">openssl req -config ./openssl-im.cnf -key intermediate/private/client.quqi.com.key.pem \</span><br><span class="line">     -subj &quot;/C=CN/ST=BJ/O=STS/OU=quqi.com/CN=client.quqi.com/emailAddress=quqi@mail.com&quot; \</span><br><span class="line">     -new -sha256 -out intermediate/csr/client.quqi.com.csr.pem</span><br><span class="line">openssl ca -config ./openssl.cnf -extensions server_cert -days 3650 -notext -md sha256 -passin pass:password\</span><br><span class="line">     -in intermediate/csr/client.quqi.com.csr.pem -out intermediate/certs/client.quqi.com.cert.pem</span><br><span class="line">chmod 444 intermediate/certs/client.quqi.com.cert.pem</span><br><span class="line">openssl x509 -noout -text -in intermediate/certs/client.quqi.com.cert.pem</span><br><span class="line">openssl verify -CAfile intermediate/certs/ca-chain.cert.pem intermediate/certs/client.quqi.com.cert.pem</span><br></pre></td></tr></table></figure>
<h2 id="证书"><a href="#证书" class="headerlink" title="证书"></a>证书</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">openssl req -newkey rsa:4096 -x509 -sha256 -days 3650 -nodes -out ca.crt -keyout ca.key -subj &quot;/C=US/ST=UK/L=London/O=Ubuntu/OU=IT/CN=CA&quot;</span><br><span class="line">openssl genrsa -out server.key</span><br><span class="line">openssl req -new -key server.key -out server.csr -subj &quot;/C=GB/ST=UK/L=London/O=Ubuntu/OU=Cloud/CN=server&quot;</span><br><span class="line">openssl x509 -req -in server.csr -out server.crt -sha256 -CA ca.crt -CAkey ca.key -CAcreateserial -days 3650</span><br><span class="line">cat server.crt server.key &gt; server.pem</span><br><span class="line">openssl x509 -noout -text -in server.crt |grep CN</span><br><span class="line">#openssl pkcs12 -export -inkey server.key -in server.crt -certfile ca.crt -passout pass: -out server.p12</span><br><span class="line">#kubectl create secret generic keystone-auth-certs --from-file=cert-file=server.crt --from-file=key-file=server.key -n kube-system</span><br><span class="line"></span><br><span class="line">sudo apt install python3-minimal -y</span><br><span class="line">sudo bash -c &apos;cat &gt;simple-https-server.py&apos; &lt;&lt;EOF</span><br><span class="line">#!/usr/bin/env python3</span><br><span class="line"># coding=utf-8</span><br><span class="line">import http.server, ssl</span><br><span class="line">server_address = (&apos;0.0.0.0&apos;, 443)</span><br><span class="line">httpd = http.server.HTTPServer(server_address, http.server.SimpleHTTPRequestHandler)</span><br><span class="line">httpd.socket = ssl.wrap_socket(httpd.socket,</span><br><span class="line">                               server_side=True,</span><br><span class="line">                               keyfile=&apos;server.key&apos;,</span><br><span class="line">                               certfile=&apos;server.crt&apos;,</span><br><span class="line">                               ssl_version=ssl.PROTOCOL_TLS)</span><br><span class="line">httpd.serve_forever()</span><br><span class="line">EOF</span><br><span class="line">sudo bash -c &apos;cat &gt;index.html&apos; &lt;&lt;EOF</span><br><span class="line">test1</span><br><span class="line">EOF</span><br><span class="line">sudo python3 simple-https-server.py</span><br><span class="line">#nohup sudo python3 simple-https-server.py &amp;</span><br><span class="line"></span><br><span class="line">$ curl -k https://192.168.99.136</span><br><span class="line">test1</span><br><span class="line">$ curl --cacert ./ca.crt https://192.168.99.136</span><br><span class="line">curl: (60) SSL: certificate subject name &apos;server&apos; does not match target host name &apos;192.168.99.136&apos;</span><br><span class="line">$ curl --resolve server:443:192.168.99.136 --cacert ./ca.crt https://server</span><br><span class="line">test1</span><br><span class="line"></span><br><span class="line">注意：当为keystone创建key时，应该使用domain而不是ip避免SNI问题，同时务必记得为keystone配置hostname让openstackclient也能拿到带domain的url</span><br></pre></td></tr></table></figure>
<h2 id="20200515更新-loadbalancer-topology-ACTIVE-STANDBY’"><a href="#20200515更新-loadbalancer-topology-ACTIVE-STANDBY’" class="headerlink" title="20200515更新 - loadbalancer-topology=ACTIVE_STANDBY’"></a>20200515更新 - loadbalancer-topology=ACTIVE_STANDBY’</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br></pre></td><td class="code"><pre><span class="line">After running &apos;juju config octavia loadbalancer-topology=ACTIVE_STANDBY&apos;</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ openstack loadbalancer list</span><br><span class="line">+--------------------------------------+------------------------------------------------------------------------+----------------------------------+---------------+---------------------+----------+</span><br><span class="line">| id                                   | name                                                                   | project_id                       | vip_address   | provisioning_status | provider |</span><br><span class="line">+--------------------------------------+------------------------------------------------------------------------+----------------------------------+---------------+---------------------+----------+</span><br><span class="line">| dc343e21-0dca-4be3-8815-c5645e07c28d | kube_service_kubernetes-tkwt84oxsw1bvxt0xorlgp3ibcrwgjo9_default_hello | 40cd6bca224f46c9b34c0f6813c1f2d0 | 192.168.21.63 | ACTIVE              | amphora  |</span><br><span class="line">+--------------------------------------+------------------------------------------------------------------------+----------------------------------+---------------+---------------------+----------+</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ openstack loadbalancer amphora list</span><br><span class="line">+--------------------------------------+--------------------------------------+-----------+--------+-----------------------------------------+---------------+</span><br><span class="line">| id                                   | loadbalancer_id                      | status    | role   | lb_network_ip                           | ha_ip         |</span><br><span class="line">+--------------------------------------+--------------------------------------+-----------+--------+-----------------------------------------+---------------+</span><br><span class="line">| a82b27d0-68cb-49a7-9387-4df3dbe617ca | dc343e21-0dca-4be3-8815-c5645e07c28d | ALLOCATED | BACKUP | fc00:9084:1613:154e:f816:3eff:fee3:7bf8 | 192.168.21.63 |</span><br><span class="line">| d03e0ab4-a23c-4a4c-939c-3bcaf5356e30 | dc343e21-0dca-4be3-8815-c5645e07c28d | ALLOCATED | MASTER | fc00:9084:1613:154e:f816:3eff:fe70:dc9c | 192.168.21.63 |</span><br><span class="line">+--------------------------------------+--------------------------------------+-----------+--------+-----------------------------------------+---------------+</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ kubectl get svc</span><br><span class="line">NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">hello        LoadBalancer   10.152.183.26   10.5.150.54   80:32371/TCP   9m17s</span><br><span class="line">kubernetes   ClusterIP      10.152.183.1    &lt;none&gt;        443/TCP        16h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">juju scp ~/.ssh/id_amphora* nova-compute/8:/home/ubuntu/</span><br><span class="line">juju ssh nova-compute/8 -- sudo ip netns exec qrouter-b03302b5-fc48-4ef7-8ded-ba17ac20c6da ssh -6 -i ~/id_amphora ubuntu@fc00:9084:1613:154e:f816:3eff:fe70:dc9c</span><br><span class="line"></span><br><span class="line">ubuntu@amphora-d03e0ab4-a23c-4a4c-939c-3bcaf5356e30:~$ sudo cat /var/lib/octavia/dc343e21-0dca-4be3-8815-c5645e07c28d/haproxy.cfg</span><br><span class="line">sudo: unable to resolve host amphora-d03e0ab4-a23c-4a4c-939c-3bcaf5356e30</span><br><span class="line"># Configuration for loadbalancer dc343e21-0dca-4be3-8815-c5645e07c28d</span><br><span class="line">global</span><br><span class="line">    daemon</span><br><span class="line">    user nobody</span><br><span class="line">    log /dev/log local0</span><br><span class="line">    log /dev/log local1 notice</span><br><span class="line">    stats socket /var/lib/octavia/dc343e21-0dca-4be3-8815-c5645e07c28d.sock mode 0666 level user</span><br><span class="line">    maxconn 1000000</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    log global</span><br><span class="line">    retries 3</span><br><span class="line">    option redispatch</span><br><span class="line">    option splice-request</span><br><span class="line">    option splice-response</span><br><span class="line">    option http-keep-alive</span><br><span class="line"></span><br><span class="line">peers dc343e210dca4be38815c5645e07c28d_peers</span><br><span class="line">    peer dVi2pNT1gedR2yc35WJnwbb5fQI 192.168.21.60:1025</span><br><span class="line">    peer ejcAi8u6hFfukxfJNty7MxS8unY 192.168.21.241:1025</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">frontend 3d6a3f5a-c35d-4718-809d-e1cce82b059b</span><br><span class="line">    option tcplog</span><br><span class="line">    maxconn 1000000</span><br><span class="line">    bind 192.168.21.63:80</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend c28d0103-75cd-42f2-ba7b-ee7c3946eb81:3d6a3f5a-c35d-4718-809d-e1cce82b059b</span><br><span class="line">    timeout client 50000</span><br><span class="line"></span><br><span class="line">backend c28d0103-75cd-42f2-ba7b-ee7c3946eb81:3d6a3f5a-c35d-4718-809d-e1cce82b059b</span><br><span class="line">    mode tcp</span><br><span class="line">    balance roundrobin</span><br><span class="line">    fullconn 1000000</span><br><span class="line">    option allbackups</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout server 50000</span><br><span class="line">    server 193a2e05-12a5-4aa3-baf2-1bcc0847324c 192.168.21.160:32371 weight 1</span><br><span class="line">    server 79ea1870-45e9-4934-9a11-5826271dec96 192.168.21.252:32371 weight 1</span><br><span class="line"></span><br><span class="line">ubuntu@amphora-d03e0ab4-a23c-4a4c-939c-3bcaf5356e30:~$ sudo cat /var/lib/octavia/vrrp/octavia-keepalived.conf</span><br><span class="line">sudo: unable to resolve host amphora-d03e0ab4-a23c-4a4c-939c-3bcaf5356e30</span><br><span class="line">vrrp_script check_script &#123;</span><br><span class="line">  script /var/lib/octavia/vrrp/check_script.sh</span><br><span class="line">  interval 5</span><br><span class="line">  fall 2</span><br><span class="line">  rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance dc343e210dca4be38815c5645e07c28d &#123;</span><br><span class="line">  state MASTER</span><br><span class="line">  interface eth1</span><br><span class="line">  virtual_router_id 1</span><br><span class="line">  priority 100</span><br><span class="line">  nopreempt</span><br><span class="line">  accept</span><br><span class="line">  garp_master_refresh 5</span><br><span class="line">  garp_master_refresh_repeat 2</span><br><span class="line">  advert_int 1</span><br><span class="line">  authentication &#123;</span><br><span class="line">    auth_type PASS</span><br><span class="line">    auth_pass 36750c5</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  unicast_src_ip 192.168.21.241</span><br><span class="line">  unicast_peer &#123;</span><br><span class="line">    192.168.21.60</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  virtual_ipaddress &#123;</span><br><span class="line">    192.168.21.63</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  virtual_routes &#123;</span><br><span class="line">    192.168.21.0/24 dev eth1 src 192.168.21.63 scope link table 1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  virtual_rules &#123;</span><br><span class="line">    from 192.168.21.63/32 table 1 priority 100</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  track_script &#123;</span><br><span class="line">    check_script</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ubuntu@amphora-d03e0ab4-a23c-4a4c-939c-3bcaf5356e30:~$ sudo ip netns exec amphora-haproxy ip addr show</span><br><span class="line">sudo: unable to resolve host amphora-d03e0ab4-a23c-4a4c-939c-3bcaf5356e30</span><br><span class="line">1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1508 qdisc fq state UP group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:67:47:a7 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.21.241/24 brd 192.168.21.255 scope global eth1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.21.63/32 scope global eth1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line">juju ssh nova-compute/8 -- sudo ip netns exec qrouter-b03302b5-fc48-4ef7-8ded-ba17ac20c6da ssh -6 -i ~/id_amphora ubuntu@fc00:9084:1613:154e:f816:3eff:fee3:7bf8</span><br><span class="line"></span><br><span class="line">ubuntu@amphora-a82b27d0-68cb-49a7-9387-4df3dbe617ca:~$ sudo cat /var/lib/octavia/dc343e21-0dca-4be3-8815-c5645e07c28d/haproxy.cfg</span><br><span class="line">sudo: unable to resolve host amphora-a82b27d0-68cb-49a7-9387-4df3dbe617ca</span><br><span class="line"># Configuration for loadbalancer dc343e21-0dca-4be3-8815-c5645e07c28d</span><br><span class="line">global</span><br><span class="line">    daemon</span><br><span class="line">    user nobody</span><br><span class="line">    log /dev/log local0</span><br><span class="line">    log /dev/log local1 notice</span><br><span class="line">    stats socket /var/lib/octavia/dc343e21-0dca-4be3-8815-c5645e07c28d.sock mode 0666 level user</span><br><span class="line">    maxconn 1000000</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">    log global</span><br><span class="line">    retries 3</span><br><span class="line">    option redispatch</span><br><span class="line">    option splice-request</span><br><span class="line">    option splice-response</span><br><span class="line">    option http-keep-alive</span><br><span class="line"></span><br><span class="line">peers dc343e210dca4be38815c5645e07c28d_peers</span><br><span class="line">    peer dVi2pNT1gedR2yc35WJnwbb5fQI 192.168.21.60:1025</span><br><span class="line">    peer ejcAi8u6hFfukxfJNty7MxS8unY 192.168.21.241:1025</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">frontend 3d6a3f5a-c35d-4718-809d-e1cce82b059b</span><br><span class="line">    option tcplog</span><br><span class="line">    maxconn 1000000</span><br><span class="line">    bind 192.168.21.63:80</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend c28d0103-75cd-42f2-ba7b-ee7c3946eb81:3d6a3f5a-c35d-4718-809d-e1cce82b059b</span><br><span class="line">    timeout client 50000</span><br><span class="line"></span><br><span class="line">backend c28d0103-75cd-42f2-ba7b-ee7c3946eb81:3d6a3f5a-c35d-4718-809d-e1cce82b059b</span><br><span class="line">    mode tcp</span><br><span class="line">    balance roundrobin</span><br><span class="line">    fullconn 1000000</span><br><span class="line">    option allbackups</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout server 50000</span><br><span class="line">    server 193a2e05-12a5-4aa3-baf2-1bcc0847324c 192.168.21.160:32371 weight 1</span><br><span class="line">    server 79ea1870-45e9-4934-9a11-5826271dec96 192.168.21.252:32371 weight 1</span><br><span class="line"></span><br><span class="line">ubuntu@amphora-a82b27d0-68cb-49a7-9387-4df3dbe617ca:~$ sudo cat /var/lib/octavia/vrrp/octavia-keepalived.conf</span><br><span class="line">sudo: unable to resolve host amphora-a82b27d0-68cb-49a7-9387-4df3dbe617ca</span><br><span class="line">vrrp_script check_script &#123;</span><br><span class="line">  script /var/lib/octavia/vrrp/check_script.sh</span><br><span class="line">  interval 5</span><br><span class="line">  fall 2</span><br><span class="line">  rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance dc343e210dca4be38815c5645e07c28d &#123;</span><br><span class="line">  state BACKUP</span><br><span class="line">  interface eth1</span><br><span class="line">  virtual_router_id 1</span><br><span class="line">  priority 90</span><br><span class="line">  nopreempt</span><br><span class="line">  accept</span><br><span class="line">  garp_master_refresh 5</span><br><span class="line">  garp_master_refresh_repeat 2</span><br><span class="line">  advert_int 1</span><br><span class="line">  authentication &#123;</span><br><span class="line">    auth_type PASS</span><br><span class="line">    auth_pass 36750c5</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  unicast_src_ip 192.168.21.60</span><br><span class="line">  unicast_peer &#123;</span><br><span class="line">    192.168.21.241</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  virtual_ipaddress &#123;</span><br><span class="line">    192.168.21.63</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  virtual_routes &#123;</span><br><span class="line">    192.168.21.0/24 dev eth1 src 192.168.21.63 scope link table 1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  virtual_rules &#123;</span><br><span class="line">    from 192.168.21.63/32 table 1 priority 100</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  track_script &#123;</span><br><span class="line">    check_script</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ubuntu@amphora-a82b27d0-68cb-49a7-9387-4df3dbe617ca:~$ sudo ip netns exec amphora-haproxy ip addr show</span><br><span class="line">sudo: unable to resolve host amphora-a82b27d0-68cb-49a7-9387-4df3dbe617ca</span><br><span class="line">1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1508 qdisc fq state UP group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:b7:fb:25 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.21.60/24 brd 192.168.21.255 scope global eth1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<h2 id="20201030更新"><a href="#20201030更新" class="headerlink" title="20201030更新"></a>20201030更新</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line">#./generate-bundle.sh --name ovn-octavia --series bionic --release ussuri --octavia --ovn --create-model --run</span><br><span class="line">#juju config neutron-api default-tenant-network-type=gre</span><br><span class="line">#openstack --insecure endpoint list</span><br><span class="line">#DOMAIN=$(juju ssh keystone/0 -- hostname -f |sed s/[[:space:]]//g)</span><br><span class="line">#juju config keystone os-admin-hostname=$DOMAIN os-public-hostname=$DOMAIN os-internal-hostname=$DOMAIN</span><br><span class="line">#source novarc</span><br><span class="line">#export OS_AUTH_URL=https://$&#123;DOMAIN&#125;:5000/v3</span><br><span class="line">#export OS_CACERT=/etc/ssl/certs/    #need install cacert.pem into system level as well</span><br><span class="line">#juju ssh keystone/0 -- openssl x509 -noout -text -in /etc/apache2/ssl/keystone/cert_10.5.1.45 |grep CN</span><br><span class="line">#curl --resolve $&#123;DOMAIN&#125;:5000:10.5.1.45 --cacert ~/stsstack-bundles/openstack/ssl/openstack-ssl/results/cacert.pem https://$&#123;DOMAIN&#125;:5000</span><br><span class="line">#./tools/vault-unseal-and-authorise.sh</span><br><span class="line">#./tools/configure_octavia.sh &amp;&amp; openstack port show octavia-health-manager-octavia-0-listen-port</span><br><span class="line"></span><br><span class="line">./generate-bundle.sh --name octavia:stsstack --create-model --octavia -s focal --num-compute 1</span><br><span class="line">./generate-bundle.sh --name octavia:stsstack --replay --run</span><br><span class="line">./configure</span><br><span class="line">source novarc</span><br><span class="line">./tools/instance_launch.sh 1 cirros2</span><br><span class="line">./tools/float_all.sh</span><br><span class="line">./tools/sec_groups.sh</span><br><span class="line"></span><br><span class="line">./tools/configure_octavia.sh</span><br><span class="line">openstack port show octavia-health-manager-octavia-0-listen-port</span><br><span class="line"></span><br><span class="line"># 第一个容易出现的问题o-hm0 出现binding failed错误多半是因为host没有使用fqdn( ( https://bugs.launchpad.net/charm-octavia/+bug/1902765, workaroud: openstack port set --host &lt;octavia-unit-fqdn&gt; &lt;port-uuid&gt; ))，即与neutron-ovs-agent绑定的host不一致所致</span><br><span class="line">#fix binding failed error &apos;Cannot get tag for port o-hm0 from its other_config: &#123;&#125;&apos;</span><br><span class="line">neutron router-gateway-clear lb-mgmt</span><br><span class="line">neutron router-interface-delete lb-mgmt lb-mgmt-subnetv6</span><br><span class="line">neutron subnet-delete lb-mgmt-subnetv6</span><br><span class="line">neutron port-delete octavia-health-manager-octavia-0-listen-port</span><br><span class="line">neutron net-delete lb-mgmt-net</span><br><span class="line">neutron router-delete lb-mgmt</span><br><span class="line">#./tools/create_ipv4_octavia.sh</span><br><span class="line">openstack network create lb-mgmt-net --tag charm-octavia</span><br><span class="line">openstack subnet create --tag charm-octavia --subnet-range 21.0.0.0/29 --dhcp  --ip-version 4 --network lb-mgmt-net lb-mgmt-subnet</span><br><span class="line">openstack router create lb-mgmt --tag charm-octavia</span><br><span class="line">openstack router add subnet lb-mgmt lb-mgmt-subnet   #neutron router-interface-add lb-mgmt lb-mgmt-subnet</span><br><span class="line">#openstack security group create lb-mgmt-sec-grp --project $(openstack security group show lb-mgmt-sec-grp -f value -c project_id)</span><br><span class="line">openstack security group rule create --protocol udp --dst-port 5555 lb-mgmt-sec-grp</span><br><span class="line">openstack security group rule create --protocol tcp --dst-port 22 lb-mgmt-sec-grp</span><br><span class="line">openstack security group rule create --protocol tcp --dst-port 9443 lb-mgmt-sec-grp</span><br><span class="line">openstack security group rule create --protocol icmp lb-mgmt-sec-grp</span><br><span class="line">openstack security group show lb-mgmt-sec-grp</span><br><span class="line">#openstack security group create lb-health-mgr-sec-grp --project $(openstack security group show lb-mgmt-sec-grp -f value -c project_id)</span><br><span class="line">openstack security group rule create --protocol udp --dst-port 5555 lb-health-mgr-sec-grp</span><br><span class="line">openstack security group rule create --protocol icmp lb-health-mgr-sec-grp</span><br><span class="line">LB_HOST=$(juju ssh octavia/0 -- hostname -f)  #should be fqdn name juju-305883-octavia-10.cloud.sts</span><br><span class="line">juju ssh octavia/0 -- sudo ovs-vsctl del-port br-int o-hm0</span><br><span class="line">neutron port-delete mgmt-port</span><br><span class="line">neutron port-create --name mgmt-port --security-group $(openstack security group show lb-health-mgr-sec-grp -f value -c id) --device-owner Octavia:health-mgr --binding:host_id=&quot;juju-8284e4-ovn-octavia-7.cloud.sts&quot; lb-mgmt-net --tenant-id $(openstack security group show lb-health-mgr-sec-grp -f value -c project_id)</span><br><span class="line">neutron port-show mgmt-port |grep binding</span><br><span class="line">juju ssh octavia/0 -- sudo ovs-vsctl del-port br-int o-hm0</span><br><span class="line">juju ssh octavia/0 -- sudo ovs-vsctl --may-exist add-port br-int o-hm0 -- set Interface o-hm0 type=internal -- set Interface o-hm0 external-ids:iface-status=active -- set Interface o-hm0 external-ids:attached-mac=$(neutron port-show mgmt-port -f value -c mac_address) -- set Interface o-hm0 external-ids:iface-id=$(neutron port-show mgmt-port -f value -c id)</span><br><span class="line">juju ssh octavia/0 -- sudo ip link set dev o-hm0 address $(neutron port-show mgmt-port -f value -c mac_address)</span><br><span class="line"></span><br><span class="line">#finally need to change amp_boot_network_list in octavia.conf, then run &apos;systemctl restart octavia*&apos;</span><br><span class="line"></span><br><span class="line">#./tools/upload_octavia_amphora_image.sh --release focal</span><br><span class="line">juju run-action octavia-diskimage-retrofit/0 --wait retrofit-image source-image=$(openstack image list |grep focal |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">#openstack loadbalancer create --name lb2 --vip-network-id lb-mgmt</span><br><span class="line">sudo snap install --edge --devmode octavia-diskimage-retrofit</span><br><span class="line">sudo -s</span><br><span class="line">cd /var/snap/octavia-diskimage-retrofit/common/tmp</span><br><span class="line">wget https://cloud-images.ubuntu.com/minimal/releases/focal/release/ubuntu-20.04-minimal-cloudimg-amd64.img</span><br><span class="line">/snap/bin/octavia-diskimage-retrofit ubuntu-20.04-minimal-cloudimg-amd64.img ubuntu-amphora-haproxy-amd64.qcow2 -d u ussuri</span><br><span class="line">exit</span><br><span class="line"># this image is just for ussuri</span><br><span class="line">openstack image create --disk-format qcow2 --container-format bare --public --tag octavia-amphora --file /var/snap/octavia-diskimage-retrofit/common/tmp/ubuntu-amphora-haproxy-amd64.qcow2 amphora-bionic-x64-haproxy</span><br><span class="line"></span><br><span class="line">#./tools/create_octavia_lb.sh --name lb1 --member-vm bionic-081730 --protocol HTTP --protocol-port 80</span><br><span class="line">sudo apt install python3-octaviaclient -y</span><br><span class="line">openstack loadbalancer create --name lb1 --vip-subnet-id private_subnet</span><br><span class="line">while true; do</span><br><span class="line">    [[ `openstack loadbalancer show lb1 --column provisioning_status --format value` = ACTIVE ]] \</span><br><span class="line">        &amp;&amp; break</span><br><span class="line">    echo &quot;waiting for lb1&quot;</span><br><span class="line">done</span><br><span class="line">openstack loadbalancer listener create --name lb1-listener --protocol HTTP --protocol-port 80 lb1</span><br><span class="line">while true; do</span><br><span class="line">    [[ `openstack loadbalancer listener show lb1-listener --column provisioning_status --format value` = ACTIVE ]] \</span><br><span class="line">        &amp;&amp; break</span><br><span class="line">    echo &quot;waiting for lb1-listener&quot;</span><br><span class="line">done</span><br><span class="line">openstack loadbalancer pool create --name lb1-pool --lb-algorithm LEAST_CONNECTIONS \</span><br><span class="line">--session-persistence type=SOURCE_IP --listener lb1-listener --protocol HTTP</span><br><span class="line">while true; do</span><br><span class="line">    [[ `openstack loadbalancer pool show lb1-pool --column provisioning_status --format value` = ACTIVE ]] \</span><br><span class="line">        &amp;&amp; break</span><br><span class="line">    echo &quot;waiting for lb1-pool&quot;</span><br><span class="line">done</span><br><span class="line">member1_IP=&apos;192.168.21.48&apos;</span><br><span class="line">member_id=$(openstack loadbalancer member create --subnet-id private_subnet --address $member1_IP --protocol-port 80 --format value --column id lb1-pool)</span><br><span class="line">while true; do</span><br><span class="line">[[ $(openstack loadbalancer member show --format value \</span><br><span class="line">    --column provisioning_status lb1-pool $&#123;member_id&#125;) = ACTIVE ]] \</span><br><span class="line">    &amp;&amp; break</span><br><span class="line">echo &quot;waiting for member ($&#123;member_id&#125;)&quot;</span><br><span class="line">done</span><br><span class="line">openstack loadbalancer healthmonitor create --name lb1-monitor --timeout 120 --max-retries 3 --delay 5 --type PING lb1-pool</span><br><span class="line">openstack loadbalancer healthmonitor list</span><br><span class="line"></span><br><span class="line">#第二个容易出现的问题,　如果lb1没有管理IP的话，会造成o-hm0无法从lb1 mgmt:5555处获得udp心跳，你将反复看到下列日志，但amphora_health表为空．</span><br><span class="line">2020-10-29 10:13:25.201 31215 DEBUG futurist.periodics [-] Submitting periodic callback &apos;octavia.cmd.health_manager.hm_health_check.&lt;locals&gt;.periodic_health_check&apos; _process_scheduled /usr/lib/python3/dist-packages/futurist/periodics.py:642</span><br><span class="line"></span><br><span class="line"># debug sql</span><br><span class="line">select * from health_monitor;</span><br><span class="line">select * from load_balancer;</span><br><span class="line">select * from listener;</span><br><span class="line">select * from pool;</span><br><span class="line">select * from member;</span><br><span class="line">SELECT load_balancer.id, load_balancer.enabled, \</span><br><span class="line">load_balancer.provisioning_status AS lb_prov_status, \</span><br><span class="line">load_balancer.operating_status AS lb_op_status, \</span><br><span class="line">listener.id AS list_id, \</span><br><span class="line">listener.operating_status AS list_op_status, \</span><br><span class="line">listener.enabled AS list_enabled, \</span><br><span class="line">listener.protocol AS list_protocol, \</span><br><span class="line">pool.id AS pool_id, \</span><br><span class="line">pool.operating_status AS pool_op_status, \</span><br><span class="line">member.id AS member_id, \</span><br><span class="line">member.operating_status AS mem_op_status from \</span><br><span class="line">amphora JOIN load_balancer ON \</span><br><span class="line">amphora.load_balancer_id = load_balancer.id LEFT JOIN \</span><br><span class="line">listener ON load_balancer.id = listener.load_balancer_id \</span><br><span class="line">LEFT JOIN pool ON load_balancer.id = pool.load_balancer_id \</span><br><span class="line">LEFT JOIN member ON pool.id = member.pool_id WHERE \</span><br><span class="line">amphora.id = &apos;44c230a150db410082ef209e2b6392fb&apos;;</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="http://www.iceyao.com.cn/2017/11/19/Neutron-lbaas%E4%BB%A3%E7%90%86https%E5%AE%9E%E8%B7%B5/" target="_blank" rel="external">http://www.iceyao.com.cn/2017/11/19/Neutron-lbaas%E4%BB%A3%E7%90%86https%E5%AE%9E%E8%B7%B5/</a><br>[2] <a href="https://wiki.openstack.org/wiki/Network/LBaaS/docs/how-to-create-tls-loadbalancer#Update_neutron_config" target="_blank" rel="external">https://wiki.openstack.org/wiki/Network/LBaaS/docs/how-to-create-tls-loadbalancer#Update_neutron_config</a><br>[3] <a href="https://serversforhackers.com/c/using-ssl-certificates-with-haproxy" target="_blank" rel="external">https://serversforhackers.com/c/using-ssl-certificates-with-haproxy</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/29/为租户下的虚机提供IPv6-DNS服务/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/29/为租户下的虚机提供IPv6-DNS服务/" itemprop="url">为租户下的虚机提供IPv6 DNS服务</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-29T17:37:02+08:00">
                2018-10-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (<a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a>)</strong></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>当虚机运行下列代码时，我们需要考虑为tenant下的VM提供DNS服务:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import dns</span><br><span class="line">import dns.resolver</span><br><span class="line">answers = dns.resolver.query(&apos;node1&apos;, &apos;AAAA&apos;)</span><br><span class="line">print answers[0].address</span><br></pre></td></tr></table></figure>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>需要截获neutron port event把IP/MAC拿到写到DNS的record中去。neutron port表的fixed_ips字段（在neutron ipallocations表里）同时记录了VM的IPv4与IPv6(netaddr.IPNetwork(fixed_ip[‘ip_address’]).version == 6)地址 [1]，neutron dns_integration特性可以从这里面将IPv4与IPv6地址都取出来记录到neutron-dhcp-agent下的dnsmasq中从而实现ml2-dns内置DNS服务。</p>
<h2 id="IPv6"><a href="#IPv6" class="headerlink" title="IPv6"></a>IPv6</h2><p>如果只是让OpenStack tenant network支持IPv6的话，很简单，直接用下列命令（下列命令有两个重要属性：<strong>ipv6_address_mode 与 ipv6_ra_mode</strong>）。当然，如果是OpenStack Over Openstack环境的话，可以让底层provider network也支持IPv6。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">neutron subnet-create --ip-version=6 --name=zhhuabj_admin_subnet_v6 --ipv6-address-mode=slaac --ipv6-ra-mode=slaac zhhuabj_admin_net 2001:db8:0:1::/64</span><br><span class="line">neutron router-interface-add zhhuabj_router zhhuabj_admin_subnet_v6</span><br></pre></td></tr></table></figure>
<p>上面命令相当于：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ cat /var/lib/neutron/ra/5c33033b-a4e1-494d-ab20-e0498b423b6c.radvd.conf</span><br><span class="line">interface qr-10bb0b85-53</span><br><span class="line">&#123;</span><br><span class="line">   AdvSendAdvert on;</span><br><span class="line">   MinRtrAdvInterval 30;</span><br><span class="line">   MaxRtrAdvInterval 100;</span><br><span class="line">   AdvLinkMTU 1458;</span><br><span class="line">   prefix 2001:db8:0:1::/64</span><br><span class="line">   &#123;</span><br><span class="line">        AdvOnLink on;</span><br><span class="line">        AdvAutonomous on;</span><br><span class="line">   &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">$ sudo ip netns exec qdhcp-be442335-ec55-4df8-b68d-dd03fa6edf00 ps -ef|grep radvd</span><br><span class="line">root     16114     1  0 Nov01 ?        00:00:00 radvd -C /var/lib/neutron/ra/5c33033b-a4e1-494d-ab20-e0498b423b6c.radvd.conf -p /var/lib/neutron/external/pids/5c33033b-a4e1-494d-ab20-e0498b423b6c.pid.radvd -m syslog</span><br><span class="line">root     16115 16114  0 Nov01 ?        00:00:00 radvd -C /var/lib/neutron/ra/5c33033b-a4e1-494d-ab20-e0498b423b6c.radvd.conf -p /var/lib/neutron/external/pids/5c33033b-a4e1-494d-ab20-e0498b423b6c.pid.radvd -m syslog</span><br><span class="line">$ sudo ip netns exec qrouter-5c33033b-a4e1-494d-ab20-e0498b423b6c ip addr show qr-10bb0b85-53 |grep inet6 |grep global</span><br><span class="line">    inet6 2001:db8:0:2::1/64 scope global</span><br><span class="line">$ sudo ip netns exec qdhcp-be442335-ec55-4df8-b68d-dd03fa6edf00 ip addr show ns-af35afad-b2 |grep inet6 |grep global</span><br><span class="line">    inet6 2001:db8:0:2:f816:3eff:feef:5190/64 scope global</span><br></pre></td></tr></table></figure>
<p>如果它不work的话，多半两个原因：<br>1, 底层OpenStack环境 (openstack over openstack)计算节点上的security group应该disable掉, 因为它有类似这种固定的anti-dhcp-spoof for ipv6规则.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-A neutron-openvswi-od7c63bee-9 -p udp -m udp --sport 547 --dport 546 -j DROP                        #Anti-dhcp-spoof for IPv6</span><br></pre></td></tr></table></figure></p>
<p>2, radvd server端, 至少得有下列防火墙规则:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo ip6tables -A INPUT -p icmpv6 -j ACCEPT</span><br><span class="line">sudo ip6tables -A OUTPUT -p icmpv6 -j ACCEPT</span><br><span class="line">sudo ip6tables -A FORWARD -p icmpv6 -j ACCEPT</span><br><span class="line">sudo ip6tables -A INPUT -p udp --dport 546:547 -j ACCEPT</span><br><span class="line">sudo ufw allow proto udp from fe80::/64 to any port 547</span><br></pre></td></tr></table></figure></p>
<p>最好测试时先临时采用:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo ip6tables -F</span><br><span class="line">sudo ufw disable</span><br><span class="line">sudo sysctl -w net.ipv6.conf.all.forwarding=1</span><br></pre></td></tr></table></figure></p>
<p>3, radvd进程是否正常启动</p>
<p>附1： 若是juju搭建的OpenStack环境要enable IPv6支持的话直接在yaml里添加下列内容即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">overrides:</span><br><span class="line">  prefer-ipv6: true</span><br></pre></td></tr></table></figure>
<p>附2： 使用OpenStack IPv6环境时，直接设置OS_AUTH_URL环境变量指向keystone的IPv6地址即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export OS_AUTH_URL=$&#123;OS_AUTH_PROTOCOL:-http&#125;://[2001:db8:0:1:f816:3eff:fe3e:5e47]:5000/v2.0</span><br></pre></td></tr></table></figure>
<h2 id="Enable-ML2-DNS"><a href="#Enable-ML2-DNS" class="headerlink" title="Enable ML2-DNS"></a>Enable ML2-DNS</h2><p>该特性有dns_name与dns_domain两个重要的属性，dns_domain可用在network与floatingip中，dns_name可用在port和floatingip中。如创建network时指定dns_name (neutron port-create my-net –dns_name my-port), 这样该dns_name和IP会作为dns record。</p>
<p>检查OpenStack是否支持dns extention API。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">neutron ext-list |grep dns</span><br><span class="line">| dns-integration           | DNS Integration</span><br></pre></td></tr></table></figure>
<p>如果不支持，可以修改下列两个文件去支持，dns_domain相当于dnsmasq给不同组织的IP提供DNS服务时的一个区别标志。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/neutron/neutron.conf</span><br><span class="line">dns_domain = example.org.</span><br><span class="line">vi /etc/neutron/plugins/ml2/ml2_conf.ini</span><br><span class="line">[ml2]</span><br><span class="line">extension_drivers = port_security,dns</span><br></pre></td></tr></table></figure>
<p>如果OpenStack是由juju创建，直接使用下列命令即可enable上述两个配置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">juju config neutron-api enable-ml2-dns</span><br><span class="line">juju config neutron-api enable-ml2-dns=True</span><br></pre></td></tr></table></figure>
<h2 id="Test-ML2-DNS"><a href="#Test-ML2-DNS" class="headerlink" title="Test ML2-DNS"></a>Test ML2-DNS</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#sudo ip addr del 2001:db8:0:122::1/64 dev ens3</span><br><span class="line">dig google.com @&lt;DNS-SERVER&gt; -p 53 AAAA</span><br><span class="line">sudo tcpdump -ni ens3 ip6 host fe80::f816:3eff:feb4:8d1f</span><br><span class="line">#  tcpdump -n -i ens3 icmp6 and ip6[40] == 134</span><br><span class="line">sudo dhclient -6 -d ens3</span><br><span class="line"></span><br><span class="line">ubuntu@bionic:~$ sudo tcpdump -ni ens3 ip6 host fe80::f816:3eff:feb4:8d1f</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on ens3, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">13:35:28.263222 IP6 fe80::f816:3eff:feb4:8d1f.546 &gt; ff02::1:2.547: dhcp6 solicit</span><br></pre></td></tr></table></figure>
<h2 id="designate架构"><a href="#designate架构" class="headerlink" title="designate架构"></a>designate架构</h2><p><img src="https://img-blog.csdnimg.cn/20181029172830827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F1cWk5OQ==,size_27,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>designate的架构如上图:</p>
<ul>
<li>designate-api, 接收来自远端用户的HTTP/HTTPS请求，通过Keystone验证远端用户的合法性，将HTTP/HTTPS请求传递给Central模块。</li>
<li>designate-sink, 监听来自Nova和Neutron的某些事件，用于自动生成域名资源记录，比如当监听到Nova的compute.instance.create.end事件通知后，自动创建一条对应于刚创建的实例的A记录；当监听到Nuetron的floatingip.update.end事件通知后，自动更新一条相应的A记录。</li>
<li>designate-central, 业务逻辑处理核心。响应API请求以及处理Sink所监听到的来自Nova和Neutron的特定通知事件。同时会存取数据库，对业务逻辑处理所产生的数据进行持久化存储。</li>
<li>designate-mdns, 实现了标准的DNS Notify和Zone Transfer的处理. designate-mdns is the service that sends DNS NOTIFY and answers zone transfer (AXFR) requests. This allows Designate to integrate with any DNS server that supports these very standard methods of communicating. designate-mdns also encapsulates all other forms of DNS protocol that Designate performs. For example, sending SOA queries to check that a change is live.</li>
<li>designate-pool-manager, 连接后端驱动，管理DNS服务器池，与MiniDNS(即designate-mdns)配合同步DNS服务器的域名以及资源记录等数据。</li>
</ul>
<p>MiniDNS(designate-mdns)：Hidden Master设计<br><a href="https://blog.csdn.net/andyron/article/details/46053241" target="_blank" rel="external">https://blog.csdn.net/andyron/article/details/46053241</a><br>Hidden Master是DNS网络安全管理系统设计中所推荐的一种最佳实践。主DNS服务器“隐藏”在内网防火墙背后，负责DNS域名资源的管理并同步变更到从DNS服务器；从DNS服务器部署在DMZ区域，对外提供DNS查询服务。由于主DNS服务器不接受DNS查询，增强了安全性。Designate MiniDNS功能模块就采用了Hidden Master的设计思想。所有托管到Designate中的DNS域都将MiniDNS视为主DNS服务器，而其被委托的DNS服务器都作为从DNS服务器。MiniDNS实现了标准的DNS Notify和Zone Transfer协议，负责同步DNS域名资源记录到从DNS服务器上。<br>其工作流程如下图:<br><img src="https://img-blog.csdnimg.cn/20181029173149275.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F1cWk5OQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li>首先，用户通过Desingate API创建一个example.com的DNS域；</li>
<li>Designate API将请求传递给Central，Central先将example.com域保存到数据库，接着发送RPC请求给Pool Manager；</li>
<li>Pool Manager收到来自Central的创建域名的请求之后，调用DNS后端驱动，在该域名被委托的服务器池中的所有服务器中创建example.com域。同时在这些服务器中，指定example.com的master服务器是MiniDNS；</li>
<li>Pool Manager完成所有从服务器上example.com域的创建之后，发送RPC请求给MiniDNS。</li>
<li>MiniDNS收到Pool Manager的RPC请求之后，向从服务器发送DNS Notify消息，告诉从服务器example.com有资源更新。</li>
<li>从服务器收到DNS Notify消息后，要求主从数据库启动Zone Transfer，域迁移的方式可以是AXFR，也可以是IXFR。</li>
<li>主服务器从数据库中读取为example.com域自动创建的SOA和NS记录，并将SOA和NS记录传送到从服务器。<br>后续任何对example.com域的变更操作都会遵循上述过程，由MiniDNS将变更同步到Designate所委派管理example.com域的DNS服务器上。</li>
</ul>
<p>看一下代码结构, designate支持很多backend(eg: bind), 安装bind服务的机器上可使用rndc命令行工具create/delete zone remotely. The traffic between rndc and bind/named(953/tcp) is authenticated with a key. designate将为每个pool生成下列配置, 这样就可以远程运行rndc命令了(rndc -s 10.5.0.29 -p 953 -k /etc/designate/rndc.key status), 其中5353是designate-mdns监听的端口:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/designate/pools.yaml</span><br><span class="line">- id: 794ccc2c-d751-44fe-b57f-8894c9f5c842</span><br><span class="line">  name: default</span><br><span class="line">  description: Pool genergated by Juju</span><br><span class="line">  ns_records:</span><br><span class="line">    - hostname: openstack-au-east-2.oc.xxx.com.</span><br><span class="line">      priority: 10</span><br><span class="line">  nameservers:</span><br><span class="line">    - host: 10.5.0.29</span><br><span class="line">      port: 53</span><br><span class="line">  targets:</span><br><span class="line">    - type: bind9</span><br><span class="line">      masters:</span><br><span class="line">        - host: 10.5.0.23</span><br><span class="line">          port: 5354</span><br><span class="line">      options:</span><br><span class="line">        host: 10.5.0.29</span><br><span class="line">        rndc_host: 10.5.0.29</span><br><span class="line">        rndc_key_file: /etc/designate/rndc.key</span><br><span class="line">  also_notifies: []</span><br></pre></td></tr></table></figure></p>
<p>在designate-bind节点上装有bind服务(运行在953端口, /usr/sbin/named -f -u bind), 需要确保bind能够访问/etc/bind/named.conf和/etc/bind/rndc.key, 并且能够接受从Pool Manager过来的rndc流量:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/bind/named.conf</span><br><span class="line">include &quot;/etc/bind/named.conf.options&quot;;</span><br><span class="line">include &quot;/etc/bind/named.conf.local&quot;;</span><br><span class="line">include &quot;/etc/bind/named.conf.default-zones&quot;;</span><br><span class="line">controls &#123;</span><br><span class="line">  inet 127.0.0.1 allow &#123;localhost;&#125;;</span><br><span class="line">  inet 10.5.0.29 allow &#123; 10.5.0.23; &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"># cat /etc/bind/named.conf.options</span><br><span class="line">options &#123;</span><br><span class="line">        directory &quot;/var/cache/bind&quot;;</span><br><span class="line">        dnssec-validation auto;</span><br><span class="line">        auth-nxdomain no;    # conform to RFC1035</span><br><span class="line">        listen-on-v6 &#123; any; &#125;;</span><br><span class="line">        allow-new-zones yes;</span><br><span class="line">        request-ixfr no;</span><br><span class="line">        recursion no;</span><br><span class="line">        statistics-file &quot;/var/cache/bind/named.stats&quot;;</span><br><span class="line">        zone-statistics yes;</span><br><span class="line">        allow-notify &#123; 10.5.0.23; &#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>rndc命令:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rndc querylog</span><br><span class="line">rndc status</span><br><span class="line">dig -t A openstack-au-east-2.oc.xxx.com@10.5.0.23</span><br><span class="line">dig -t MX openstack-au-east-2.oc.xxx.com@10.5.0.23</span><br></pre></td></tr></table></figure></p>
<p>bind DNS服务器可作为缓存服务器, 主DNS服务器和辅助DNS服务器, 配置分配如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 缓存服务器, 不负责解析，仅为加速，不需要注册</span><br><span class="line">options &#123;</span><br><span class="line">       forward only;</span><br><span class="line">       forwarders &#123;</span><br><span class="line">               168.95.1.1;</span><br><span class="line">               139.175.10.20;</span><br><span class="line">       &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"># 主DNS服务器, 负责解析本地客户端请求</span><br><span class="line">zone &quot;test.com&quot; IN &#123;</span><br><span class="line">       type master;</span><br><span class="line">       file &quot;test.com.zone&quot;;</span><br><span class="line">&#125;;</span><br><span class="line"># 辅助DNS服务器, 辅助服务器的区域数据都是从主服务器复制而来，其数据都是只读的. 根据序列号大小决定是否复制</span><br><span class="line">zone &quot;test.com&quot; IN &#123;</span><br><span class="line">       type slave;</span><br><span class="line">       masters &#123;ip;&#125;;</span><br><span class="line">       file &quot;slaves/test.com.zone&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>区域传送, 解析库文件同步的过程，即辅助DNS服务器从主DNS服务器或其他的辅助DNS服务器请求数据传输过程 。</p>
<ul>
<li>完全区域传送：传送区域的所有数据，简称AXFR</li>
<li>增量区域传送：传送区域中改变的数据部分，简称IXFR<br>bind配置中之DNS主从同步，区域安全传送<br><a href="http://www.it165.net/admin/html/201403/2548.html" target="_blank" rel="external">http://www.it165.net/admin/html/201403/2548.html</a><h2 id="附件-一个designate问题"><a href="#附件-一个designate问题" class="headerlink" title="附件 - 一个designate问题"></a>附件 - 一个designate问题</h2>可能因为designate不断地升级, 而且是多网卡, 这样导致每个designate-bind unit所指定的master ip (designate unit)不是同一子网. 这样会导致create zone的时候有时候会发两次rndc addzone命令.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- designate/7 only programs designate-bind/3 [172.18.248.94] (it does not know about designate-bind/2). It also uses the .247 subnet for the Master IPs instead of the correct 248 subnet.</span><br><span class="line">- designate/8 programs both bind9 servers [172.18.248.94, 172.18.248.99] and uses the correct .248 subnet</span><br><span class="line">- designate/9 only programs designate-bind/3 and also uses the wrong master IPs.</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>并且因为之前的DB问题, 导致bind cache里有很多stale zone, 这样当create zone的时候, 会说zone已存在.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">How to remove stale zones from bind9 dynamic zone configuration</span><br><span class="line"></span><br><span class="line">1,check the list of zones in bind9 like so:</span><br><span class="line">cat /var/cache/bind/*.nzf|grep ^zone|awk &apos;&#123;print $2&#125;&apos;|sed &apos;s/&quot;$/.&quot;,/g&apos; #the sed here adds a trailing dot so we can query the database for the same name</span><br><span class="line">2, check if this zone is currently active by querying the database:</span><br><span class="line">mysql -u root -p designate</span><br><span class="line">select * from zones where name=&quot;xxx.openstack-au-east-2.oc.xxx.com.&quot; WHERE deleted_at IS NOT NULL;</span><br><span class="line">3, rndc delzone xxx.openstack-au-east-2.oc.xx.com</span><br></pre></td></tr></table></figure></p>
<p>designate recovery service检测到一个zone长期为pending状态时, 再会继续create zone. 这样在日志中会看到一个zone被反复多次create</p>
<h2 id="附件-Designate环境搭建及测试"><a href="#附件-Designate环境搭建及测试" class="headerlink" title="附件 - Designate环境搭建及测试"></a>附件 - Designate环境搭建及测试</h2><p>ml2dns只能使用neutron.conf配置文件里的域名(如:ml2dns.example.),  network里可以配置外部多个顶级不固定的域名(如:neutron net-update private –dns_domain extdns.example.).<br>ml2dns只为internal fixed ip在dnsmasq里配置域名, designate只为external fip配置域名, dnsmasq里需要配置forwarder到designate-bind中去.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">juju add-model xenial-queens-designate</span><br><span class="line"># https://paste.ubuntu.com/p/y5NQwXB95R/</span><br><span class="line">juju deploy ./b/openstack.yaml --overlay ./b/o/memcached.yaml --overlay ./b/o/designate.yaml</span><br><span class="line">juju config neutron-api dns-domain=ml2dns.example.</span><br><span class="line">juju config neutron-api reverse-dns-lookup=True</span><br><span class="line">juju config neutron-api enable-ml2-dns=True</span><br><span class="line">juju config designate nameservers=ns1.extdns.example.</span><br><span class="line">./configure</span><br><span class="line">source ~/stsstack-bundles/openstack/novarc</span><br><span class="line">./tools/sec_groups.sh</span><br><span class="line"></span><br><span class="line">  applications:</span><br><span class="line">    neutron-api:</span><br><span class="line">      charm: cs:~openstack-charmers-next/neutron-api</span><br><span class="line">      options:</span><br><span class="line">        enable-ml2-dns: True</span><br><span class="line">        dns-domain: ml2dns.example.</span><br><span class="line">        reverse-dns-lookup: True</span><br><span class="line">        ipv4-ptr-zone-prefix-size: 24</span><br><span class="line">    designate:</span><br><span class="line">      charm: cs:~openstack-charmers-next/designate</span><br><span class="line">      options:</span><br><span class="line">        nameservers: ns1.extdns.example.</span><br><span class="line">    designate-bind:</span><br><span class="line">      charm: cs:~openstack-charmers-next/designate-bind</span><br><span class="line">  relations:</span><br><span class="line">    - [ designate, designate-bind ]</span><br><span class="line">    - [ designate, neutron-api ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># https://docs.openstack.org/python-designateclient/latest/user/shell-v2.html</span><br><span class="line">sudo apt install -y python-openstackclient python-designateclient</span><br><span class="line"></span><br><span class="line"># create the zone for your external dns, the zone name must end in a &apos;.&apos;</span><br><span class="line">DOMAIN_NAME=extdns.example.</span><br><span class="line">openstack zone create --email extdns@example $DOMAIN_NAME</span><br><span class="line"></span><br><span class="line"># create an &apos;A&apos; record for the automaticall created NS record that points to one or more of the designate-bind units</span><br><span class="line">DESIGNATE_BIND_IP=$(juju status designate-bind --format short | awk &apos;/designate-bind/ &#123;print $3&#125;&apos;)</span><br><span class="line">openstack recordset create --record $DESIGNATE_BIND_IP --type A $DOMAIN_NAME ns1</span><br><span class="line">dig @$DESIGNATE_BIND_IP -type NS</span><br><span class="line"></span><br><span class="line"># tell neutron that for any floating IPs for our tenant network create the the hostname in a given DNS domain</span><br><span class="line">neutron net-update private --dns_domain $DOMAIN_NAME</span><br><span class="line"></span><br><span class="line"># tell neutron to use designate bind as it&apos;s forwarder by dnsmasq&apos;s configuration &apos;--server=10.5.0.16 --domain=ml2dns.example.&apos;</span><br><span class="line"># query path: Project instance -&gt; Neutron dnsmasq -&gt; Designate Bind -&gt; External DNS</span><br><span class="line"># NOTE: must DO NOT SET –dns-nameserver on the tenent subnet</span><br><span class="line">juju config neutron-gateway dns-servers=$DESIGNATE_BIND_IP</span><br><span class="line"></span><br><span class="line"># create a VM for integration test</span><br><span class="line">nova keypair-add --pub-key ~/.ssh/id_rsa.pub mykey</span><br><span class="line">openstack port create --network $(neutron net-show private -f value -c id) --dns-name i1 port_i1</span><br><span class="line">$ openstack port show port_i1 -f value -c dns_assignment</span><br><span class="line">fqdn=&apos;i1.ml2dns.example.&apos;, hostname=&apos;i1&apos;, ip_address=&apos;192.168.21.18&apos;</span><br><span class="line"></span><br><span class="line">openstack server create --wait --image cirros2 --flavor m1.tiny --key-name mykey --port $(openstack port show port_i1 -f value -c id) i1</span><br><span class="line">public_network=$(openstack network show ext_net -f value -c id)</span><br><span class="line">fip=$(openstack floating ip create $public_network -f value -c floating_ip_address)</span><br><span class="line">openstack floating ip set $fip --fixed-ip-address 192.168.21.18 --port $(openstack port list --fixed-ip ip-address=192.168.21.18 -c id -f value)</span><br><span class="line"></span><br><span class="line"># Neutron will create forward and reverse records in dnsmasq for the internal DNS.</span><br><span class="line"># NOTE: must DO NOT SET –dns-nameserver on the tenent subnet, By not setting the dns server</span><br><span class="line">#       the project network will default to using the dnsmasq service associated with the network.</span><br><span class="line">#       This is Neutron handling our project internal DNS records automatically.</span><br><span class="line">openstack subnet unset --dns-nameserver 10.230.64.2 private_subnet</span><br><span class="line">nova boot --hard i1</span><br><span class="line"></span><br><span class="line"># test dns inside VM, 192.168.21.2 is the IP inside qdhcp-xxx namespace</span><br><span class="line">$ ssh cirros@10.5.150.2 -- cat /etc/resolv.conf</span><br><span class="line">search ml2dns.example</span><br><span class="line">nameserver 192.168.21.2</span><br><span class="line"></span><br><span class="line">$ ssh cirros@10.5.150.2 -- nslookup i1.ml2dns.example 192.168.21.2</span><br><span class="line">Server:    192.168.21.2</span><br><span class="line">Address 1: 192.168.21.2 host-192-168-21-2.ml2dns.example</span><br><span class="line">Name:      i1.ml2dns.example</span><br><span class="line">Address 1: 192.168.21.18 i1.ml2dns.example</span><br><span class="line"></span><br><span class="line">$ juju ssh neutron-gateway/0 -- cat /var/lib/neutron/dhcp/bc66c327-265b-4491-95ab-2d2e14d801d2/host |grep i1</span><br><span class="line">fa:16:3e:09:b7:37,i1.ml2dns.example.,192.168.21.18</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Neutron will inform Designate to create the forward and reverse records for the floating IP in the external DNS</span><br><span class="line">dig @$DESIGNATE_BIND_IP i1.extdns.example</span><br><span class="line">dig @$DESIGNATE_BIND_IP -x $fip</span><br><span class="line"></span><br><span class="line">$ dig @$DESIGNATE_BIND_IP i1.extdns.example</span><br><span class="line">...</span><br><span class="line">i1.extdns.example.	3600	IN	A	10.5.150.2</span><br><span class="line">extdns.example.		3600	IN	NS	ns1.extdns.example.</span><br><span class="line">ns1.extdns.example.	3600	IN	A	10.5.0.16</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"># We can also see the external DNS record sets in Designate</span><br><span class="line">openstack recordset list extdns.example.</span><br><span class="line"></span><br><span class="line">$ openstack recordset list extdns.example.</span><br><span class="line">+--------------------------------------+---------------------+------+--------------------------------------------------------------------+--------+--------+</span><br><span class="line">| id                                   | name                | type | records                                                            | status | action |</span><br><span class="line">+--------------------------------------+---------------------+------+--------------------------------------------------------------------+--------+--------+</span><br><span class="line">| 92c8f5d6-eeca-4b0d-968b-deed4697b787 | extdns.example.     | NS   | ns1.extdns.example.                                                | ACTIVE | NONE   |</span><br><span class="line">| a0167123-714d-4c34-8542-ad8b5019c318 | extdns.example.     | SOA  | ns1.extdns.example. extdns.example. 1548816241 3505 600 86400 3600 | ACTIVE | NONE   |</span><br><span class="line">| 99f97832-edea-4727-be68-73536d9145d3 | ns1.extdns.example. | A    | 10.5.0.16                                                          | ACTIVE | NONE   |</span><br><span class="line">| 728808e5-e6dd-4b9d-b7fa-ad260a855ac4 | i1.extdns.example.  | A    | 10.5.150.2                                                         | ACTIVE | NONE   |</span><br><span class="line">+--------------------------------------+---------------------+------+--------------------------------------------------------------------+--------+--------+</span><br></pre></td></tr></table></figure></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://github.com/openstack/neutron/blob/stable/pike/neutron/plugins/ml2/extensions/dns_integration.py#L285" target="_blank" rel="external">https://github.com/openstack/neutron/blob/stable/pike/neutron/plugins/ml2/extensions/dns_integration.py#L285</a><br>[2] <a href="https://docs.openstack.org/mitaka/networking-guide/config-dns-int.html" target="_blank" rel="external">https://docs.openstack.org/mitaka/networking-guide/config-dns-int.html</a><br>[3] <a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/8/html-single/dns-as-a-service_guide/index" target="_blank" rel="external">https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/8/html-single/dns-as-a-service_guide/index</a><br>[4] <a href="https://openstackdevops.wordpress.com/2018/01/27/designate-and-neutron-dns-integration/" target="_blank" rel="external">https://openstackdevops.wordpress.com/2018/01/27/designate-and-neutron-dns-integration/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/09/Win-10-UEFI-Ubuntu-18-04-UEFI-双系统/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/09/Win-10-UEFI-Ubuntu-18-04-UEFI-双系统/" itemprop="url">Win 10 UEFI + Ubuntu 18.04 UEFI 双系统</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-09T01:28:15+08:00">
                2018-09-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>本人昨天买了一块SSD, 结果后来发现原来这块SSD存在硬件质量问题, 造成了软件上的种种诡异问题, 如U盘时而识别时而不识别, 如触摸屏左键时而抽风, 如ghost安装win10时几乎到100%的进度时忽然来一个无响应, 重启系统后出现了”To interrupt normal start up, press the blue ThinkVantage button.”, 此时键盘无反应, 既进不了系统, 也进不了BIOS. 拨CMOS电源也无效. 最后发现是这块SSD有质量问题. 估计是SSD有控制器主要是软件吧, 控制器软件有bug导致运行ghost这种软件时也能导致硬件挂住.<br>也正是因为这个问题吧, 七搞八搞, 一不小心在重试的过程中将之前的一块linux分区误删了, 于是之前打算的迁移双系统的想法泡汤(当然, 那些通过分区助手或者ghost来迁移分区的网上文章照着做没一个是成功的).<br>这样, 有机会事隔多年再一次重装双系统的机会, 但是发现世道变了, 之前百试不爽的方法现在行不通了. 后经查证, 主要原因是ubuntu 18.04开始默认采用UEFI, 而win10默认仍然是MBR. 这样会导致一系列的问题, 如报错: grub-efi-amd64-signed failed to install 18.04, 统一采用UEFI安装.</p>
<h2 id="BIOS设置"><a href="#BIOS设置" class="headerlink" title="BIOS设置"></a>BIOS设置</h2><p>在BIOS中将Boot Mode设置为UEFI Only, 如果有Secure Boot选项还要disable它(不做这一步可能会造成按F12键之后无法找到U盘)<br>注: 改成UEFI only之后, 运行双系统, 四系统都没问题, 但后来进不了U盘的livecd, 报: couldn’t get UEFI db list, 所以只得改回Both, 但UEFI优先.</p>
<h2 id="安装win10"><a href="#安装win10" class="headerlink" title="安装win10"></a>安装win10</h2><ul>
<li>下载大白菜UEFI专版 - <a href="http://www.bigbaicai.com/download.html?down2" target="_blank" rel="external">http://www.bigbaicai.com/download.html?down2</a></li>
<li>下载win10 ghost - axel -n 10 <a href="http://xz.win10cjb.com/18.5/win10_64/DEEP_Win10x64_201805.rar" target="_blank" rel="external">http://xz.win10cjb.com/18.5/win10_64/DEEP_Win10x64_201805.rar</a></li>
<li>制作大白菜启动U盘, 如果界面上有UEFI字眼就点上(不记得了, 有就点上), 还要注意一点, 记得点里面的格式转换, 将FAT32格式(HDD-FAT32)转换成NTFS(HDD-NTFS)转换, 否则HDD-FAT32格式不能拷贝大于4G的ghost文件哦,</li>
<li>按F12选U盘启动进入大白菜后, 用DiskGenius工具重新分区, 必须将BIOS+MBR格式转UEFI+GPT格式. 分区表格式为GUID而不是MBR, window上管EFI分区叫ESP/MSR分区</li>
<li><p>注意, 不要修改推荐的卷标, 这个卷就是指向的ESP/MSR分区.</p>
<h2 id="安装win10后"><a href="#安装win10后" class="headerlink" title="安装win10后"></a>安装win10后</h2><p>安装win10后需要将禁用掉快速启动, 否则会造成按F12无法选择U盘启动. 菜单路径为: “设置 -&gt; 系统 -&gt; 电源与睡眠 -&gt; 其他电源设置 -&gt; 选择电源按钮的功能 -&gt; 更改当前不可用的设置 -&gt; 启动快速启动”</p>
<h2 id="安装ubuntu-18-04"><a href="#安装ubuntu-18-04" class="headerlink" title="安装ubuntu 18.04"></a>安装ubuntu 18.04</h2><p>像安装win10一样, 一样要注意重要一点, 需创建大概300M左右的UEF分区, 另外, 还可以创建一个根分区和一个备份文件用的bak分区.<br>注意, windows非常霸道, 它是总修改bios里的启动顺序, 将它的”Windows boot Manager”放在”ubuntu grub”的前面, 可以在bios里锁定启动顺序</p>
<h2 id="安装win7"><a href="#安装win7" class="headerlink" title="安装win7"></a>安装win7</h2><p>win7若没有sata的驱动, 所以得先改回IDE, 装完win7之后再改回AHCI, 否则也容易挂在启动界面不动了.<br>注: 我未遇到以上问题, 可能因为我装的win7并不是原版的, 已经带了sata驱动</p>
<h2 id="加装SSD"><a href="#加装SSD" class="headerlink" title="加装SSD"></a>加装SSD</h2><p>如果加装了SSD之后呢? 那得注意:</p>
</li>
<li><p>装win10时同样需要进大白菜或老毛桃后用DiskGenius在SSD上划分ESP/MSR分区</p>
</li>
<li>装ubuntu时, 分区处也要创建EFI分区, 同时grub设置安装在SSD上, 相当于: grub-install /dev/sdX.</li>
<li>bios里选择哪块硬盘启动. 其实在SSD上安装grub后, 这个grub会连HDD上原先的win10与ubuntu一起放在启动列表里. 注意, windows非常霸道, 它是总修改bios里的启动顺序, 将它的”Windows boot Manager”放在”ubuntu grub”的前面, 可以在bios里锁定启动顺序</li>
<li><p>有时候需要对ssd优化, 例如不要将swap分区放在ssd以延长寿命, 如更改i/o调度策略为noop, 如使用bcache</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>装完之后进入win10发现thinkpad小红点左键失灵, 再切换进ubuntu发现小红点左键正常(实际上, 5次大概有一次有问题, 只是登录界面左键与右键似乎混乱了, 登录之后就正常了. 再换PE进系统发现小红点左键依然有问题. 所以基本断定和硬件没有关系, 应该是win10上的小红点驱动有问题.<br>但搜索了很多帖子, 没一个能解决问题的, 联想的小红点win10驱动做得太烂了. 所以决定回到win7, 回到win7之后该问题解决. 另外, PE回到win7的过程中不会伤害之前SSD上安装的ubuntu系统, 也不会伤害原HDD里的双系统.</p>
<h2 id="数据迁移"><a href="#数据迁移" class="headerlink" title="数据迁移"></a>数据迁移</h2><p>现在在笔记本x220t上装了win10, 也装了ubuntu 18.04, 但是如何将工作机t440p的根分区迁移到x220t的根分区呢? 因为我们已经在x220t上安装了ubuntu 18.04, 这样省去了采用命令划分EFI分区, 以及最后填充EFI分区的步骤. 现在将精力集中在如何快速迁移根分区上.</p>
</li>
<li><p>目的机x220t因为有写操作, 故要以livecd启动, 启动ssh server, 并将根分区加载到/mnt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo -i</span><br><span class="line">apt install openssh-server</span><br><span class="line">passwd</span><br><span class="line">echo &apos;PermitRootLogin yes&apos; &gt;&gt; /etc/ssh/sshd_config</span><br><span class="line">service ssh restart</span><br><span class="line">fdisk -l</span><br><span class="line">mount /dev/sdb8 /mnt</span><br></pre></td></tr></table></figure>
</li>
<li><p>源机t440p最好也以livecd启动, 注意: 例如源机上有一个软链指向了/bak分区, 但因为此时没有挂载/bak分区, 所以在rsync命令迁移时会报错退出. 人工删除该软链重新运行即可.  且需要注意 rsync命令中的/mnt/后应该有/, 否则会将mnt目录迁移到根分区的mnt目录下.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo -i</span><br><span class="line">fdisk -l</span><br><span class="line">mount /dev/sda9 /mnt</span><br><span class="line"></span><br><span class="line"># rsync will now copy all files, directories, permissions and owners over to the destination machine.</span><br><span class="line"># It also skips all files and directories that are not on the root filesystem, like /dev/, /sys/, /proc/.</span><br><span class="line"># If there are filesystems that are mounted separately on the source machine and your want those copied too, use rsync again on those mountpoints too.</span><br><span class="line">rsync -xavP --numeric-ids --exclude=&apos;tmp&apos; /mnt root@192.168.99.128:/</span><br><span class="line">#rsync -xavP --numeric-ids --exclude=&apos;tmp&apos; --exclude=&apos;/nas&apos; /mnt/ root@192.168.99.128:/mnt/</span><br></pre></td></tr></table></figure>
</li>
<li><p>更新grub, 此时会报”canot find EFI directory”, 这样会导致这时生成grub时无法找到原HDD中的双系统, 不要紧, 只要找到目前SSD中的双系统即可. 呆会下一步再运行一下grub命令即可解决</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mount /dev/sdb8 /mnts</span><br><span class="line">for d in dev sys proc; do mount --bind /$d /mnt/$d; done</span><br><span class="line">chroot /mnt/ grub-install /dev/sdb   # canot find EFI directory</span><br><span class="line">chroot /mnt/ update-grub</span><br></pre></td></tr></table></figure>
</li>
<li><p>修复fstab, 之前运行上述迁移命令前忘了备份x220t上的fstab系统, 导致它被覆盖, OK, 我们修复它.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">blkid</span><br><span class="line">e2label /dev/sdb8 &quot;ROOT_SSD&quot;</span><br><span class="line">tee &quot;/mnt/etc/fstab&quot; &lt;&lt;EOF</span><br><span class="line">#UUID can be found via blkid command</span><br><span class="line">#LABEL=boot /boot ext2 sync 0 2</span><br><span class="line">#UUID=735b3be3-779c-4d21-a944-b033225f3ab4 none   swap    sw      0       0</span><br><span class="line">#LABEL=SWAP none swap sw 0 0</span><br><span class="line">UUID=9401-D2EA /boot/efi vfat defaults 0 2</span><br><span class="line">LABEL=ROOT_SSD / ext4 errors=remount-ro 0 1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
</li>
<li><p>这时重启系统, 就可以以grub选择启动SSD上的双系统了, 如果还想把HDD的原有的双系统也加到grub的话, 那进ubuntu系统后再执行一次update-grub命令即可.</p>
</li>
<li>这种迁移方式效果非常好, 各种工作软件不需要再重装了. 呵呵<h2 id="调整分区"><a href="#调整分区" class="headerlink" title="调整分区"></a>调整分区</h2>一个分区不够用时, 可以使用gpartd合并相邻的空闲分区.注意一点, 要合并的分区必须是umount状态时才能合并.<h2 id="SSD优化"><a href="#SSD优化" class="headerlink" title="SSD优化"></a>SSD优化</h2></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># disable scanning for btrfs filesystems when boot</span><br><span class="line">sudo apt-get purge btrfs-tools</span><br><span class="line">sudo update-initramfs -ukall</span><br><span class="line"></span><br><span class="line"># enable TRIM feature by adding discard option</span><br><span class="line"># what&apos;s TRIM - https://blog.csdn.net/quqi99/article/details/50963308</span><br><span class="line"># the option noatime is used to disable access time for a file</span><br><span class="line">sudo hdparm -I /dev/sdb |grep TRIM</span><br><span class="line">vi /etc/fstab</span><br><span class="line">LABEL=ROOT_SSD /               ext4    noatime,discard,errors=remount-ro 0       1</span><br><span class="line">sudo mount -o remount /dev/sdb8</span><br><span class="line">sudo mount |grep sdb8 |grep discard</span><br><span class="line"></span><br><span class="line"># Try not to use swap space unless it&apos;s running out of memory.</span><br><span class="line">echo 1 &gt; /proc/sys/vm/swappiness</span><br><span class="line"></span><br><span class="line"># avoid visiting ssd by using ramdisk for /tmp instead of tmpfs</span><br><span class="line">vim /etc/fstab</span><br><span class="line">tmpfs /tmp tmpfs defaults,noatime,mode=1777 0 0</span><br><span class="line">tmpfs /var/tmp tmpfs defaults,noatime,mode=1777 0 0</span><br><span class="line">tmpfs /var/log tmpfs defaults,noatime,mode=1777 0 0</span><br><span class="line">sudo mount -o remount /</span><br><span class="line"></span><br><span class="line"># Set chrome to use ramdisk cache</span><br><span class="line">cd ~/.cache/google-chrome/Default</span><br><span class="line">rm -rf Cache</span><br><span class="line">sudo ln -s /tmp Cache</span><br><span class="line">rm -rf Media\ Cache/</span><br><span class="line">sudo ln -s /tmp Media\ Cache</span><br><span class="line"></span><br><span class="line"># Use noop for I/O elevator</span><br><span class="line">cat /sys/block/sda/queue/scheduler</span><br><span class="line">sudo vi /etc/default/grub</span><br><span class="line">GRUB_CMDLINE_LINUX_DEFAULT=&quot;elevator=noop&quot;</span><br><span class="line">sudo update-grub</span><br><span class="line"></span><br><span class="line"># Test SSD speed</span><br><span class="line">$ sudo hdparm -Tt /dev/sdb</span><br><span class="line">/dev/sdb:</span><br><span class="line"> Timing cached reads:   9128 MB in  2.00 seconds = 4569.28 MB/sec</span><br><span class="line"> Timing buffered disk reads: 818 MB in  3.01 seconds = 272.07 MB/sec</span><br><span class="line"></span><br><span class="line"># Make sure 4K align</span><br><span class="line">$ sudo fdisk -lu |grep sdb |grep sectors</span><br><span class="line">Disk /dev/sdb: 232.9 GiB, 250059350016 bytes, 488397168 sectors</span><br><span class="line"></span><br><span class="line"># Health check</span><br><span class="line">$ sudo smartctl -s on -a /dev/sdb |grep PASSED</span><br><span class="line">SMART overall-health self-assessment test result: PASSED</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/03/也谈wifi断流问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/03/也谈wifi断流问题/" itemprop="url">也谈wifi断流问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-03T12:02:42+08:00">
                2018-09-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (作者：张华 发表于：2018-09-03)</strong></p>
<h2 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h2><p>现实生活中的悖论真多, 本来pmtud是设计用来在mtu不一致的情况下协商mss值的, 结果很多服务端或者中间路由器会错误地禁用掉icmp-type=3或者icmp=type=4, 于是ptmud不可用, 于是很多路由器中的clamp-mss-to-pmtu设置(iptables -t mangle -A POSTROUTING -p tcp –tcp-flags SYN,RST SYN -j TCPMSS –clamp-mss-to-pmtu)也失效, 这样tcp访问某些特定mtu值不一致的网站时就会出现各种莫名其妙的问题.<br>另一方面, 对于udp, 因为无连接, 所以无法协商mss, 这样有些网络设备(eg: Nokia7)会设置禁止pmtud协商(disable DF bit in the IP header of the sender, ip_no_pmtu_disc=1), 从而也会反过来造成路由器中的clamp-mss-to-pmtu失效.</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>笔者最近应该是遇到了常听大家说起的wifi断流问题, 新入一款安卓原生系统手机, 但是在使用wifi上网时会感觉到某些APP上网不流畅, 尤其是使用京东APP搜索商品时会总说找不着网络, 但此时显然是有网络的. 为此, 笔者先做了一系统排除性实验:</p>
<ul>
<li>排除法测试, 使用京东APP搜索商品时说找不着网络, 但使用京东APP的其他功能没有问题, 并且使用京东以外的其他APP也没问题</li>
<li>排除法测试, 切换为4G网络使用京东APP搜索商品正常, 仅仅只是使用WIFI网络时才会出问题.</li>
<li>排除法测试, 去麦当劳使用WIFI确认无问题, 但速度也不快, 但能打开页面.</li>
<li>排除法测试, 难道是家里的WIFI有问题吗? 但换个手机型号使用京东APP搜索商品却又正常.</li>
<li>排除法测试, 难道是VPN的问题? 关掉VPN, 恢复DNS国内设置依然有问题.</li>
<li>排除法测试, 继续换一个没有VPN的干净的OpenWRT路由器依然有问题, 可惜家里没有Non-OpenWRT路由器可供测试.</li>
<li>排除法测试, 路由器上修改802.11g, 802.11n, 802.11ac等设置后问题依旧.</li>
<li>排除法测试, 检查了路由器上的MAC地址是否与其他机器重复, 未发现异常</li>
<li>排除法测试, 使用114.114.114.114作为DNS, 问题依旧</li>
<li>排除法测试, OpenWRT路由器使用tcpdump抓包, 干扰条目过多, 未深入</li>
<li>排除法测试, 现在问题看起来只是发生在这款特定手机型号与特定的OpenWRT路由器与特定的某些APP如京东, 手机刷机到android 8.1与7.1两个版本问题依旧.</li>
<li>排除法测试, google搜索大量京东或别的某些应用在各种手机型号上出问题的帖子, 试着更改帖子中的各种切换手机配置的操作, 如不对京东使用电源优化,问题依旧.</li>
<li><p>排除法测试, 绝大多数时候打不开京东的这个搜索商品的功能, 但极少数情况又能打开, 但非常慢, 使用别家的wifi网络时也是非常慢, 很难说清楚现象.</p>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>上述一系列排除性测试让我相信该问题仅和我使用特定的手机型号, 使用特定的OpenWRT路由器, 使用特定的某些APP如京东有关.<br>京东APP, 一个上层应用而已, 理论上只有下列几个因素会影响到上层应用:</p>
</li>
<li><p>DNS</p>
</li>
<li>IPv6/IPv4 fallback</li>
<li>MTU<br>理论让我将目光回到MTU, 修改OpenWRT路由器WAN口的MTU=1492后问题依旧.继续深挖:</li>
<li>路由器背后的手机操作系统应该有/proc/sys/net/ipv4/ip_no_pmtu_disc=0让手机可以根据pmtu来确实应用所需的mss值. 遗憾地是, 手机没有root, 无法检查此项值. 参考: Note7 WiFi问题技术分析及给三星的解决方案 <a href="http://tieba.baidu.com/p/4793087714" target="_blank" rel="external">http://tieba.baidu.com/p/4793087714</a> , 参考: <a href="https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt" target="_blank" rel="external">https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt</a> , 参考: <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=5d424d5a674f782d0659a3b66d951f412901faee" target="_blank" rel="external">https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=5d424d5a674f782d0659a3b66d951f412901faee</a></li>
<li>OpenWRT路由器tcpdump抓包, 看到的mss值确实不小. 既然无root权限无法修改手机的ip_no_pmtu_disc参数, 那有没有方法直接修改OpenWRT路由器强迫修改mss值呢?<br>OK, 在路由器上添加如下两个命令, 问题就这么解决了:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#iptables -A FORWARD -j ACCEPT</span><br><span class="line">iptables -t mangle -A FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line"></span><br><span class="line">root@OpenWrt:~# iptables-save |grep mss</span><br><span class="line">:mssfix - [0:0]</span><br><span class="line">-A FORWARD -j mssfix</span><br><span class="line">-A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line">-A mssfix -o pppoe-wan -p tcp -m tcp --tcp-flags SYN,RST SYN -m comment --comment &quot;wan (mtu_fix)&quot; -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line"></span><br><span class="line">root@OpenWrt:~# tcpdump -ni br-lan src host 192.168.99.194 and dst host 111.13.24.129 and dst port 443</span><br><span class="line">06:37:42.085674 IP 192.168.99.194.39494 &gt; 111.13.24.129.443: Flags [S], seq 140081180, win 65535, options [mss 1460,sackOK,TS val 8629819 ecr 0,nop,wscale 8], length 0</span><br><span class="line">06:37:42.092397 IP 192.168.99.194.39494 &gt; 111.13.24.129.443: Flags [.], ack 2370816066, win 343, length 0</span><br><span class="line">06:37:42.095245 IP 192.168.99.194.39494 &gt; 111.13.24.129.443: Flags [P.], seq 0:173, ack 1, win 343, length 173</span><br><span class="line">06:37:42.141194 IP 192.168.99.194.39494 &gt; 111.13.24.129.443: Flags [.], ack 1453, win 354, length 0</span><br><span class="line">06:37:42.141396 IP 192.168.99.194.39494 &gt; 111.13.24.129.443: Flags [.], ack 2905, win 365, length 0</span><br><span class="line">06:37:42.141536 IP 192.168.99.194.39494 &gt; 111.13.24.129.443: Flags [.], ack 3472, win 377, length 0</span><br><span class="line">06:37:42.147373 IP 192.168.99.194.39494 &gt; 111.13.24.129.443: Flags [P.], seq 173:491, ack 3472, win 377, length 318</span><br><span class="line">06:37:42.185607 IP 192.168.99.194.39494 &gt; 111.13.24.129.443: Flags [P.], seq 491:1736, ack 3714, win 388, length 1245</span><br><span class="line">06:37:42.194932 IP 192.168.99.194.39494 &gt; 111.13.24.129.443: Flags [P.], seq 1736:1767, ack 4065, win 388, length 31</span><br><span class="line">06:37:42.195258 IP 192.168.99.194.39494 &gt; 111.13.24.129.443: Flags [R.], seq 1767, ack 4065, win 388, length 0</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这款手机的操作系统没有设置ip_no_pmtu_disc参数去协商mss值, 而OpenWRT路由器刚好缺一条iptables rule (iptables -t mangle -A FORWARD -p tcp –tcp-flags SYN,RST SYN -j TCPMSS –clamp-mss-to-pmtu), 这样遭遇了pppoe的1492 MTU问题.<br>换句话说, 当我外出时, 如果所连的路由器没有加这条设置, 那么这个问题仍然又遇到. 手机操作系统ip_no_pmtu_disc设置才能彻底解决某些应用wifi网络不能上网的问题.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iptables -t mangle -I POSTROUTING -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line">iptables -t mangle -I OUTPUT -o pppoe-wan -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line">iptables -t mangle -I FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line">ip6tables -t mangle -I POSTROUTING -o pppoe-wan -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line">ip6tables -t mangle -I OUTPUT -o pppoe-wan -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line">ip6tables -t mangle -I FORWARD -o pppoe-wan -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br></pre></td></tr></table></figure></p>
<p>另外, 要想clamp-mss-to-pmtu生效, 需要设置充许设计DF=1来禁用分片从而启用pmtud协议.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 0 &gt;/proc/sys/net/ipv4/ip_no_pmtu_disc</span><br></pre></td></tr></table></figure></p>
<p>但设置”echo 0 &gt;/proc/sys/net/ipv4/ip_no_pmtu_disc”后之后, 对于无连接的udp包, 由于无法协商mss值, 当大分片的udp包到达路由器后, 路由器会简单地丢弃它.</p>
<h2 id="另一个案例"><a href="#另一个案例" class="headerlink" title="另一个案例"></a>另一个案例</h2><p>openstack虚机上再创建docker容器, 因为是vxlan网络虚机的mtu=1450, docker0的mtu如果为1500时将导致docker container无法通信.<br>当mtu配置不正确时此时需要依赖mss, 此时计算节点充当的是路由器它发现此情况(不能将docker container发出的mtu=1500的包发出时)应该发ICMP error到docker container, 或者openstack vrouter应当做这件事情.</p>
<p>docker container错误的具体表现是无法下载, 此时, 最可能的情况时, 最大的包到达openstack vrouter后, 它从external interface收到mtu=1500的包并尝试分片发给虚机, 失败后将发ICMP error到外部下载服务器. 此时:</p>
<p>1, 当non-DVR时, l3-agent上的snat-xxx确实向外部下载服务器发出了ICMP error:</p>
<p>11:45:04.873959 fa:16:3e:c5:b3:ed &gt; fe:54:00:36:ab:b4, ethertype IPv4 (0x0800), length 590: 100.64.1.1 &gt; 120.146.233.220: ICMP 103.245.215.9 unreachable - need to frag (mtu 1450), length 556</p>
<p>2, 当DVR时, FIP移到了计算节点上, 此时是qrouter-xxx, 但是这个ns没有default route,<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">root@maas-node02:~# ip -n qrouter-1752c73a-be9f-4326-97cc-99dbe0988b3c rule show</span><br><span class="line">0:	from all lookup local</span><br><span class="line">32766:	from all lookup main</span><br><span class="line">32767:	from all lookup default</span><br><span class="line">57481:	from 103.245.215.14 lookup 16</span><br><span class="line">80000:	from 103.245.215.0/28 lookup 16</span><br><span class="line"></span><br><span class="line">root@maas-node02:~# ip -n qrouter-1752c73a-be9f-4326-97cc-99dbe0988b3c route show table 16</span><br><span class="line">default via 169.254.106.115 dev rfp-1752c73a-b</span><br><span class="line"></span><br><span class="line">root@maas-node02:~# ip -n qrouter-1752c73a-be9f-4326-97cc-99dbe0988b3c route show</span><br><span class="line">103.245.215.0/28 dev qr-ec03268e-fb proto kernel scope link src 103.245.215.1</span><br><span class="line">169.254.106.114/31 dev rfp-1752c73a-b proto kernel scope link src 169.254.106.114</span><br></pre></td></tr></table></figure></p>
<p>只有再添加下列默认路由之后, DVR qrouter-xxx’s才能将ICMP error发出去, 这样才可能去使用mss,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@maas-node02:~# ip -n qrouter-1752c73a-be9f-4326-97cc-99dbe0988b3c route add default via 169.254.106.115 dev rfp-1752c73a-b</span><br></pre></td></tr></table></figure>
<p>所以这样造成的问题就是, non-DVR虚机上运行docker container没问题, 而DVR虚机上运行docker container有问题. 解决办法有三个:</p>
<p>1, 修改docker0的mtu=1450, 我们不能修改bridge的mtu, 但可以往docker0里再添加一个tap, 这样bridge的mtu将取决于tap mtu的最小值.<br>2, 计算节点上运行:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t mangle -A FORWARD -o ens3 -p tcp -m tcp --tcp-flags SYN,RST SYN -m tcpmss --mss 1400:65495 -j TCPMSS --clamp-mss-to-pmtu</span><br></pre></td></tr></table></figure></p>
<p>3, to increase the global-physnet-mtu to 1550 to allow the real tenant network MTU to be 1500.<br>注: 沿路的交换机也要配置相应的MTU, 特别是如果交换机配置的MTU过小, 那么ICMP error直接就没有提示就被drop( 千万注意: 过来的包的MTU如果大于设备的MTU值才会分片分段, 如果是小于的话直接就DROP掉了, 这个参数net.ipv4.tcp_mtu_probing也可以探测这种情况)了这样导致MTU没配对之后也无法利用MSS, 这种问题更不好查. 另外, 也发现IPv6情况下的ICMPv6没有被ip6table rule允许的情况.</p>
<h2 id="另一个问题-ssh连接时不时中断"><a href="#另一个问题-ssh连接时不时中断" class="headerlink" title="另一个问题 - ssh连接时不时中断"></a>另一个问题 - ssh连接时不时中断</h2><p>我的mtu是1484=1456+28(IP header is 20 byptes, ICMP header is 8 bytes), gavin的mtu是1492. 这个网页(<a href="https://ubuntuforums.org/showthread.php?t=2341699)说" target="_blank" rel="external">https://ubuntuforums.org/showthread.php?t=2341699)说</a>:<br>we are using a PPPoE connection the highest MTU we can get is 1492. And your ISP might not even go that high. The highest I could get mine was 1484. 这个网页(<a href="https://www.gargoyle-router.com/phpbb/viewtopic.php?t=8787)也提到了pppoe-wan口的mtu有时总比设置的要少8" target="_blank" rel="external">https://www.gargoyle-router.com/phpbb/viewtopic.php?t=8787)也提到了pppoe-wan口的mtu有时总比设置的要少8</a> byptes.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root@OpenWrt:~# ip addr show pppoe-wan | grep mtu</span><br><span class="line">608: pppoe-wan: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1484 qdisc fq_codel state UNKNOWN group default qlen 3</span><br><span class="line"></span><br><span class="line">hua@t440p:~$ ping -M do -i 1 -c 2 -s 1456 vps</span><br><span class="line">1464 bytes from vps (xx.xx.xx.xx.xx): icmp_seq=1 ttl=57 time=209 ms</span><br><span class="line">hua@t440p:~$ ping -M do -i 1 -c 2 -s 1457 vps</span><br><span class="line">ping: local error: Message too long, mtu=1484</span><br><span class="line"></span><br><span class="line">ubuntu@gavin-P70:~$ ping -M do -i 1 -c 2 -s 1464 vps</span><br><span class="line">1472 bytes from xx.xx.xx.xx.xx: icmp_seq=1 ttl=58 time=48.5 ms</span><br><span class="line">ubuntu@gavin-P70:~$ ping -M do -i 1 -c 2 -s 1465 vps</span><br><span class="line">ping: local error: Message too long, mtu=1492</span><br></pre></td></tr></table></figure></p>
<p>抓包看到的gavin的mss值是1452(1492-40), 我的mss值却是1460.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hua@t440p:~$ sudo tcpdump -s0 -p -ni eth0 src host 192.168.99.135 and dst host xx.xx.xx.xx and dst port 22 and &apos;(ip and ip[20+13] &amp; tcp-syn != 0)&apos;</span><br><span class="line">08:00:45.771287 IP 192.168.99.135.35172 &gt; xx.xx.xx.xx.22: Flags [S], seq 1056384328, win 29200, options [mss 1460,sackOK,TS val 92453102 ecr 0,nop,wscale 7], length 0</span><br><span class="line"></span><br><span class="line">hua@t440p:~$ sudo tcpdump -s0 -p -ni eth0 src host xx.xx.xx.xx and &apos;(ip and ip[20+13] &amp; tcp-syn != 0)&apos;</span><br><span class="line">08:01:31.252313 IP xx.xx.xx.xx.22 &gt; 192.168.99.135.35182: Flags [S.], seq 1913298511, ack 4239124226, win 28960, options [mss 1452,sackOK,TS val 3432015413 ecr 92498502,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure></p>
<p>为什么我的mss值却是1460呢? 可能是根据t440p client上的mtu 1500来设置的, 却没有参考路由器上的1484. 查看了路由器的设置是使用了”–clamp-mss-to-pmtu”, 如果对端如gavin家不支持pmtu协商呢(如他家的tplink路由器什么的没有设置ip_no_pmtu_disc=0).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@OpenWrt:~# iptables-save |grep mss</span><br><span class="line">:mssfix - [0:0]</span><br><span class="line">-A FORWARD -j mssfix</span><br><span class="line">-A mssfix -o pppoe-wan -p tcp -m tcp --tcp-flags SYN,RST SYN -m comment --comment &quot;wan (mtu_fix)&quot; -j TCPMSS --clamp-mss-to-pmtu</span><br></pre></td></tr></table></figure>
<p>只有继续添加了下列针对POSTROUTING的clamp-mss-to-pmtu才变回1444<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@OpenWrt:~# iptables -t mangle -A POSTROUTING -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line"></span><br><span class="line">hua@t440p:~$ sudo tcpdump -s0 -p -ni eth0 src host xx.xx.xx.xx and &apos;(ip and ip[20+13] &amp; tcp-syn != 0)&apos;</span><br><span class="line">08:51:53.831560 IP xx.xx.xx.xx.22 &gt; 192.168.99.135.36554: Flags [S.], seq 3638522412, ack 307088327, win 28960, options [mss 1444,sackOK,TS val 3432771057 ecr 95521097,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure></p>
<p>所以在路由器上设置最终为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iptables -t mangle -A FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line">iptables -t mangle -A OUTPUT -o pppoe-wan -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line">iptables -t mangle -A POSTROUTING -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line">ip6tables -t mangle -A FORWARD -o pppoe-wan -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line">ip6tables -t mangle -A OUTPUT -o pppoe-wan -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line">ip6tables -t mangle -A POSTROUTING -o pppoe-wan -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br></pre></td></tr></table></figure></p>
<p>改了之后, 从路由器的<strong>pppoe-wan口(出是1444, 进是1452)</strong>和t440p的<strong>eth0口(出是1444, 进是1460)</strong>看到的mss值是不一样的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root@OpenWrt:~# tcpdump -s0 -p -ni pppoe-wan src host xx.xx.xx.xx and &apos;(ip and ip[20+13] &amp; tcp-syn != 0)&apos;</span><br><span class="line">01:45:59.915458 IP xx.xx.xx.xx.22 &gt; 10.2.156.119.37466: Flags [S.], seq 2070487858, ack 2656232535, win 28960, options [mss 1452,sackOK,TS val 3433582582 ecr 98767213,nop,wscale 7], length 0</span><br><span class="line"></span><br><span class="line">root@OpenWrt:~# tcpdump -s0 -p -ni eth0 src host 192.168.99.135 and dst host xx.xx.xx.xx and dst port 22 and &apos;(ip and ip[20+13] &amp; tcp-syn != 0)&apos;</span><br><span class="line">01:45:59.835236 IP 10.2.156.119.37466 &gt; xx.xx.xx.xx.22: Flags [S], seq 2656232534, win 29200, options [mss 1444,sackOK,TS val 98767213 ecr 0,nop,wscale 7], length 0</span><br><span class="line"></span><br><span class="line">hua@t440p:~$ sudo tcpdump -s0 -p -ni eth0 src host xx.xx.xx.xx and &apos;(ip and ip[20+13] &amp; tcp-syn != 0)&apos;</span><br><span class="line">09:45:59.925637 IP xx.xx.xx.xx.22 &gt; 192.168.99.135.37466: Flags [S.], seq 2070487858, ack 2656232535, win 28960, options [mss 1444,sackOK,TS val 3433582582 ecr 98767213,nop,wscale 7], length 0</span><br><span class="line"></span><br><span class="line">hua@t440p:~$ sudo tcpdump -s0 -p -ni eth0 dst host xx.xx.xx.xx and dst port 22 and &apos;(ip and ip[20+13] &amp; tcp-syn != 0)&apos;</span><br><span class="line">09:45:59.842593 IP 192.168.99.135.37466 &gt; xx.xx.xx.xx.22: Flags [S], seq 2656232534, win 29200, options [mss 1460,sackOK,TS val 98767213 ecr 0,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure></p>
<p>如果不在路由器, 改在t440p上单为ssh设置的话可以:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># https://blog.cloudflare.com/path-mtu-discovery-in-practice/</span><br><span class="line"># sudo ip route change default via &lt;&gt; advmss 1444</span><br><span class="line">sudo iptables -t mangle -A OUTPUT -p tcp --dport 22 -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --set-mss 1444</span><br></pre></td></tr></table></figure></p>
<p>另外, 也可以将t440p上的mtu设置为1484, 这样路由器和t440p上进和出的mss值全统一到1444了.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ifconfig eth0 mtu 1484</span><br></pre></td></tr></table></figure></p>
<p>只有两端的话可以通过mss协商解决, 但两端之间还有一系统路由器交换机什么的就需要pmtud来协商mss值了. pmtud的原理就是设置df=1禁止分片但又传大分片给路由器, 路由器根据df=1应该丢弃掉包并返回 icmp unreachable messages, 但有些路由器的icmp规则设置的过于严格(<strong>远端机器无法ping, “sudo mtr xxx -r –no-dns -P 22”的输出的最后一跳丢包率是100%, 100%的丢包率不一定是真的丢包, 而是icmp限速了或者禁用了</strong>) 可以会丢弃icmp unreachable messages. 至少应该设置:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># https://www.iana.org/assignments/icmp-parameters/icmp-parameters.xhtml</span><br><span class="line">iptables -A INPUT -p icmp -m icmp --icmp-type 3 -j ACCEPT</span><br><span class="line">iptables -A INPUT -p icmp -m icmp --icmp-type 4 -j ACCEPT</span><br><span class="line">iptables -A INPUT -p icmp -m icmp --icmp-type 11 -j ACCEPT</span><br></pre></td></tr></table></figure></p>
<h2 id="20190426更新"><a href="#20190426更新" class="headerlink" title="20190426更新"></a>20190426更新</h2><p>如此好的一篇文章 (<a href="https://www.zeitgeist.se/2013/11/26/mtu-woes-in-ipsec-tunnels-how-to-fix/" target="_blank" rel="external">https://www.zeitgeist.se/2013/11/26/mtu-woes-in-ipsec-tunnels-how-to-fix/</a> ), 相见恨晚. 这篇文章中提到了两点和我之前长时间痛苦摸索的基本一致:</p>
<ul>
<li>文章说pmtud理论很饱满, 但现实很残酷, 很多网站或路由器很粗鲁地将icmp-type 3 and 4禁用掉了, 从而导致ptmud失效.</li>
<li>在ptmud失效的情况下, 对于tcp, 在server端运行” iptables -t mangle -A FORWARD -o eth0 -p tcp -m tcp –tcp-flags SYN,RST SYN -s 172.16.16.0/24 -m tcpmss –mss 1361:1536 -j TCPMSS –set-mss 1360”  或者 “sudo iptables -t mangle -A FORWARD -p tcp –dport 22 -m tcp –tcp-flags SYN,RST SYN -j TCPMSS –set-mss 1444” 来动态设置mss是一个好办法.</li>
<li>在ptmud失效的情况下, 对于udp, udp是无连接的, 无法协商mss值, 在server端运行 “echo 1 &gt;/proc/sys/net/ipv4/ip_no_pmtu_disc” (The only solution to guarantee that UDP works is to disable the Don’t Fragment (DF) bit in the IP header of the sender. This will allow our VPN server to fragment any UDP packet). 注意: <strong>ip_no_pmtu_disc=1也意味着禁止pmtud协商了哦</strong>.</li>
</ul>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>减少client端的mss值, 并查看mss值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># https://blog.cloudflare.com/path-mtu-discovery-in-practice/</span><br><span class="line"># sudo ip route change default via &lt;&gt; advmss 1400</span><br><span class="line"># sudo iptables -t mangle -A OUTPUT -p tcp --dport 22 -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --set-mss 1444</span><br><span class="line">hua@t440p:~$ sudo iptables-save |grep mss</span><br><span class="line">-A OUTPUT -p tcp -m tcp --dport 22 -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --set-mss 1444</span><br><span class="line"></span><br><span class="line">hua@t440p:~$ sudo tcpdump -s0 -p -ni eth0 src host 192.168.99.135 and dst host xxx and dst port 22 and &apos;(ip and ip[20+13] &amp; tcp-syn != 0)&apos;</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">22:05:34.295036 IP 192.168.99.135.55502 &gt; xxx.22: Flags [S], seq 2332107979, win 29200, options [mss 1444,sackOK,TS val 56741396 ecr 0,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure>
<p>Enable smart MTU black hole detection. RFC4821 proposes a mechanism to detect ICMP black holes and tries to adjust the path MTU in a smart way. To enable this on Linux type:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /proc/sys/net/ipv4/tcp_mtu_probing</span><br><span class="line">echo 1024 &gt; /proc/sys/net/ipv4/tcp_base_mss</span><br></pre></td></tr></table></figure></p>
<p>检测丢包(使用ping不准的):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hua@t440p:~$ netstat -s -p|grep -i segments</span><br><span class="line">    11519400 segments received</span><br><span class="line">    11624725 segments sent out</span><br><span class="line">    84339 segments retransmitted</span><br><span class="line">    980 bad segments received</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/03/IPv6来啦/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/03/IPv6来啦/" itemprop="url">IPv6来啦</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-03T16:01:05+08:00">
                2018-08-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (作者：张华 发表于：2018-08-03)</strong></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>家里用的是中国移动的宽带, 一直挺稳定的, 而且昨天发现ISP下发了IPv6地址(不是子网, 形如: 2409:8a00:7805:xxxx:xxxx:xxxx:xxxx:xxxx/64, 打xxxx的部分是动态变化的). 好吧, 咱就用用.</p>
<p><strong>20200814更新</strong><br>联通宽带在安装的时候要告诉师傅将光猫改桥接(后台改), 改桥接之后，会有公网IP及IPv6 (TL-WDR7660路由器需要在IPv6也设置为像IPv6一样宽带联网才能使用IPv6的）．<br>即使改桥接后得到的公网IP也是作了限制的，并且所有端口全是封的，给联通打电话时，他们会装作不慬公网，不懂端口，一问三不知．总之，最后他们还是偷偷地给改好了．<br>但是WDR7660下接的WNDR4300 openwrt路由器却无法获取IPv6地址，估计没有安装odhcpv6的原因吧，待确定．</p>
<h2 id="路由器配置"><a href="#路由器配置" class="headerlink" title="路由器配置"></a>路由器配置</h2><ul>
<li><p>配置/etc/config/network使用’option ip6assign ‘64’</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config interface &apos;lan&apos;                    </span><br><span class="line">        ...      </span><br><span class="line">        option ip6assign &apos;64&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置/etc/confignetwork删除config globals ‘globals’段中和 IPv6 ULA-Prefix相关的配置 (注: 使用IPv6 ULA-Prefix的话, LAN中机器就会得到一个以它打头的IPv6地址, 但如何从外网访问这个地址呢? 有三种方式: 一是使用我们现在使用的relay方式得到是ISP提供的全球可路由的IPv6地址；二是配置ULA-Prefix=2409:8a00:7805:1::/80之后再使用下列的neigh proxy的方法解决, 但这种一般是子网长度80比64大, 但ISP并没有给我们分配subnet, 只是分配了IPv6地址, 所以无法保证2409:8a00:7805:1::/80这个前缀与ISP分配的2409:8a00:7805:xxxx:xxxx:xxxx:xxxx:xxxx/64一致；三是ULA-Prefix配置一个与ISP分配的不同的路由然后通过配置路由的方式但前提是也得有全球可路由的IPv6 subnet啊. 所以这里我们选择了relay模式来实现外网访问内网的目的.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">config globals &apos;globals&apos;</span><br><span class="line">        option ula_prefix &apos;fd89:714c:1c30::/48&apos;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv6.conf.all.proxy_ndp=1</span><br><span class="line">ip -6 neigh add proxy 2409:8a00:7805:1::430 dev pppoe-wan</span><br><span class="line">ip -6 neigh show proxy</span><br></pre></td></tr></table></figure>
<ul>
<li>配置/etc/config/dhcp使用relay模式, relay模式意味着openwrt通过默认的odhcpd作为中继自动为LAN的其他机器配置ISP的IPv6地址<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">config dhcp &apos;wan&apos;                                                                                                                       </span><br><span class="line">        option interface &apos;wan&apos;                                                                                                          </span><br><span class="line">        option ignore &apos;1&apos;                                                                                                               </span><br><span class="line">        option dhcpv6 &apos;disabled&apos;                                                                                                        </span><br><span class="line">        option ndp &apos;relay&apos;                                                                                                              </span><br><span class="line">        option ra &apos;relay&apos;                                                                                                               </span><br><span class="line">        option master &apos;1&apos;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </span><br><span class="line">config dhcp &apos;lan&apos;                                                                                                                       </span><br><span class="line">        option interface &apos;lan&apos;                                                                                                          </span><br><span class="line">        option start &apos;100&apos;                                                                                                              </span><br><span class="line">        option limit &apos;150&apos;                                                                                                              </span><br><span class="line">        option leasetime &apos;12h&apos;                                                                                                          </span><br><span class="line">        list dhcp_option &apos;6,192.168.99.1&apos;                                                                                               </span><br><span class="line">        option ndp &apos;relay&apos;                                                                                                              </span><br><span class="line">        option ra &apos;relay&apos;                                                                                                               </span><br><span class="line">        option dhcpv6 &apos;relay&apos;</span><br><span class="line"></span><br><span class="line"># actually I use 60, not 64</span><br><span class="line">cat /etc/config/network</span><br><span class="line">config interface &apos;lan&apos;</span><br><span class="line">	...</span><br><span class="line">	option ip6assign &apos;60&apos;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>配置完之后就可以通过下列命令重启路由器服务就可以在br-lan与pppoe-wan上获得ISP分配的IPv6地址了.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/network restart</span><br><span class="line">/etc/init.d/odhcpd restart</span><br></pre></td></tr></table></figure></p>
<p>内网机器直接通过’sudo /etc/init.d/network-manager restart’重启网络也会获取ISP分配的IPv6地址.</p>
<h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><ul>
<li>内网机器访问br-lan内网网关, 能通是因为下列路由的功能:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hua@t440p:~$ ip -6 route list |grep 2409:8a00:7805:b7df::/64</span><br><span class="line">2409:8a00:7805:b7df::/64 dev eth0  proto ra  metric 100  pref medium</span><br><span class="line"></span><br><span class="line">root@OpenWrt:~# route -A inet6 |grep ::/0</span><br><span class="line">::/0                                        fe80::200:5eff:fe00:134                 UG    1024   0        0</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>注: 后面我们会看到::/0的默认路由会造成很多问题.</p>
<ul>
<li><p>内网机器访问pppoe-wan外网网关<br>对于relay模式由于内网机器拿到的本来就是ISP分配的可路由的IP所以自然能通. 但对于nat模式由于内网机器分配的是和ISP不同网段的IP, 所以需要在路由器上做NAT6, 如:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#ip6tables -t nat -I POSTROUTING -o pppoe-wan -j MASQUERADE</span><br><span class="line">ip6tables -t nat -I POSTROUTING -s `uci get network.globals.ula_prefix` -j MASQUERADE &gt; /dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure>
</li>
<li><p>外网访问内网机器<br>如果使用relay由于内网机器使用的是全球可路由的IPv6地址, 外网直接就是可以访问内网机器的；但如果是不同子网需要做路由；如果只是前缀相同只是子网长度不同可以做neigh proxy</p>
</li>
<li>但是此时我们发现内网机器无法访问外网(如ping6 ipv6.baidu.com), 但此时在路由器上却是可以访问外网的. 原因就在于上面使用::/0的默认路由似乎有问题(报这个错: Destination unreachable: Unknown code 5), 改成了2000::/3就好了:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">route -A inet6 add 2000::/3 `route -A inet6 | grep ::/0 | awk &apos;NR==1&#123;print&quot;gw &quot;$2&quot; dev &quot;$7&#125;&apos;`</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>或者:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip -6 route add default from 2409:8a00:7805::48 dev pppoe-wan</span><br></pre></td></tr></table></figure></p>
<p>但上面两种方法仍然遇到了不稳定的问题, 感觉是openwrt的odhcpd不稳定, 照下列方法更换为6relayd之后仍然不好使.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">wget https://www.shintaku.cc/files/6relayd_2013-10-21_ar71xx.ipk</span><br><span class="line">opkg install 6relayd_2013-10-21_ar71xx.ipk</span><br><span class="line">vi /etc/config/6relayd</span><br><span class="line">config relay</span><br><span class="line">        option master &apos;wan&apos;</span><br><span class="line">        option network &apos;lan&apos;</span><br><span class="line">        option rd &apos;relay&apos;</span><br><span class="line">        option dhcpv6 &apos;relay&apos;</span><br><span class="line">        option ndp &apos;relay&apos;</span><br><span class="line">/etc/init.d/odhcpd disable</span><br><span class="line">/etc/init.d/odhcpd stop</span><br><span class="line">/etc/init.d/6relayd enable</span><br><span class="line">/etc/init.d/6relayd start</span><br></pre></td></tr></table></figure></p>
<p>接着使用tcpdump查看好像是DHCPv6 reply包从pppoe-wan过不来br-lan口:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">08:22:53.810272 IP6 2400:da00:2::29 &gt; 2409:8a00:7805:b7df:8d79:6c9:315a:9ca3: ICMP6, echo reply, seq 7, length 64</span><br></pre></td></tr></table></figure></p>
<p>所以接着, 在/etc/config/firewall文件的 Allow-ICMPv6-Forward项中添加了下列行后确认已经有了129(129就是reply)相关的iptables rules之后仍然不好使.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">        list icmp_type &apos;router-solicitation&apos;    </span><br><span class="line">        list icmp_type &apos;neighbour-solicitation&apos; </span><br><span class="line">        list icmp_type &apos;router-advertisement&apos;   </span><br><span class="line">        list icmp_type &apos;neighbour-advertisement&apos;</span><br><span class="line">        </span><br><span class="line">root@OpenWrt:~# ip6tables-save |grep Allow-ICMPv6-Forward |grep 129</span><br><span class="line">-A zone_wan_forward -p ipv6-icmp -m icmp6 --icmpv6-type 129 -m limit --limit 1000/sec -m comment --comment Allow-ICMPv6-Forward -j ACCEPT</span><br></pre></td></tr></table></figure></p>
<p>google查到的和这个bug相同(<a href="https://github.com/openwrt/odhcpd/issues/37" target="_blank" rel="external">https://github.com/openwrt/odhcpd/issues/37</a>), 但里面的所有方法都试过了不成功.<br>似乎是ISP使用的是Statefull DHCPV6的方式, 6relayd可以把ra信息relay过来，但LAN端机器似乎无法跟DHCPV6服务器通信。</p>
<h2 id="2018-0805更新"><a href="#2018-0805更新" class="headerlink" title="2018-0805更新"></a>2018-0805更新</h2><p>今天通过这个帖子(<a href="https://bbs.pku.edu.cn/v2/post-read.php?bid=35&amp;threadid=15501646)解决了上面的问题" target="_blank" rel="external">https://bbs.pku.edu.cn/v2/post-read.php?bid=35&amp;threadid=15501646)解决了上面的问题</a>:<br>OpenWRT默认是在wan口使用DHCPv6 Client, 在LAN口使用odhcpd开启RA和DHCPv6. 这个默认配置适用于国外主流ISP, 因为他们DHCPv6-PD (prefix delegation)把一个至少/64地址段分配给客户使用(还有的使用小于64的地址段给客户分配静态IP).<br>不过中国移动给客户分配的是SLAAC地址, 没有使用DHCPv6, 也就没使用DHCPv6-PD, 这样拿不到前缀(ISP分配的2409:8a00:7805:xxx::/64地址的第4段总是变化的), 所以odhcpd也就无法根据这个前缀设置路由, 所以我们需要手工设置确保可以在OpenWrt上ping通内网机器, 这样才能保证reply消息到达br-lan之后能到达内网机器, 如:<br>route -A inet6 add 2409:8a00:7805:d9b1::/64 dev br-lan<br>所以最终添加在/etc/firewall.user的内容如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># make ipv6 relay to work</span><br><span class="line">PREFIX=`ip -6 route list dev br-lan |grep 2409:8a00 |awk -F &apos;::/&apos; &apos;&#123;print $1&quot;::/64&quot;&#125;&apos; |uniq`</span><br><span class="line">ip -6 route del $PREFIX dev br-lan &gt; /dev/null 2&gt;&amp;1</span><br><span class="line"># NOTE: too too important, we must add &apos;metric&apos; option, or this route will have expire time, or use &apos;ip&apos; to add route</span><br><span class="line">route -A inet6 add $PREFIX dev br-lan metric 1 &gt; /dev/null 2&gt;&amp;1                                </span><br><span class="line">                                                                                                               </span><br><span class="line"># make ipv6 nat to work                                                                                        </span><br><span class="line">#ip -6 route add default from $PREFIX dev pppoe-wan &gt; /dev/null 2&gt;&amp;1                                           </span><br><span class="line">#route -A inet6 add 2000::/3 `route -A inet6 | grep ::/0 | awk &apos;NR==1&#123;print&quot;gw &quot;$2&quot; dev &quot;$7&#125;&apos;` &gt; /dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure></p>
<p>再就是得配置replay消息能从pppoe-wan到达br-lan, 所以最终使用下列配置(也记得去掉IPv6 ULA-Prefix):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">config dhcp &apos;wan&apos;                                </span><br><span class="line">        option interface &apos;wan&apos;                   </span><br><span class="line">        option ignore &apos;1&apos;                        </span><br><span class="line">        option ndp &apos;relay&apos;                       </span><br><span class="line">        option ra &apos;relay&apos;                        </span><br><span class="line">        option master &apos;1&apos;                                      </span><br><span class="line">config dhcp &apos;lan&apos;             </span><br><span class="line">        option interface &apos;lan&apos;</span><br><span class="line">        option start &apos;100&apos;    </span><br><span class="line">        option limit &apos;150&apos;    </span><br><span class="line">        option leasetime &apos;12h&apos;</span><br><span class="line">        list dhcp_option &apos;6,192.168.99.1&apos;</span><br><span class="line">        option ndp &apos;relay&apos;               </span><br><span class="line">        option ra &apos;relay&apos;</span><br></pre></td></tr></table></figure></p>
<p>可新问题又来了, 这条路由总是过期, 如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2409:8a00:7805:da41::/64 dev br-lan  proto kernel  metric 256  expires 259019sec</span><br></pre></td></tr></table></figure></p>
<p>重新执行一下上面的命令又能恢复, 太不稳定了, 还是转回ipv6 NAT模式吧.</p>
<h2 id="20180902更新"><a href="#20180902更新" class="headerlink" title="20180902更新"></a>20180902更新</h2><p>上述静态路由过期的问题原因找到, 原因是需要添加metric</p>
<ul>
<li>route -A inet6 add 2409:8a00:7805:d9b1::/64 dev br-lan               # 会有过期时间 </li>
<li>route -A inet6 add 2409:8a00:7805:d9b1::/64 dev br-lan metric 1 # 无过期时间<br>或者使用ip命令它添加的无过期时间 - ip -6 route del 2409:8a00:7805:d9b1::/64 dev br-lan</li>
</ul>
<h2 id="转试NAT6"><a href="#转试NAT6" class="headerlink" title="转试NAT6"></a>转试NAT6</h2><p>上面使用replay时的bug解不了, 无奈之下, 只好使用NAT6, 外面访问不了内网就访问不了吧, 起码可以内网访问外网啊. </p>
<ul>
<li><p>在/etc/config/network中配置了ula_prefix=2001:192:168:99::/64, 这时路由器上的br-lan除了ISP分配的IP之外, 也会多时我们这个自己配置的地址: 2001:192:168:99:0:0:0:1/64</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">config globals &apos;globals&apos;</span><br><span class="line">        option ula_prefix &apos;2001:192:168:99::/64&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改/etc/config/dhcp将relay模式改到server模式即NAT模式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">config dhcp &apos;lan&apos;                                </span><br><span class="line">        option interface &apos;lan&apos;                   </span><br><span class="line">        option start &apos;100&apos;                       </span><br><span class="line">        option limit &apos;150&apos;                       </span><br><span class="line">        option leasetime &apos;12h&apos;                   </span><br><span class="line">        list dhcp_option &apos;6,192.168.99.1&apos;        </span><br><span class="line">        option dhcpv6 &apos;server&apos;                   </span><br><span class="line">        option ra_management &apos;2&apos;                 </span><br><span class="line">        option ra &apos;server&apos;                       </span><br><span class="line">        option ra_default &apos;1&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在/etc/firewall.user中添加下列SNAT规则</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip6tables -t nat -I POSTROUTING -s `uci get network.globals.ula_prefix` -j MASQUERADE &gt; /dev/null 2&gt;&amp;1 </span><br><span class="line">route -A inet6 add 2000::/3 `route -A inet6 | grep ::/0 | awk &apos;NR==1&#123;print&quot;gw &quot;$2&quot; dev &quot;$7&#125;&apos;` &gt; /dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>之后, 内网机器随便手动配置一个IP如2001:192:168:99:0:0:0:3/64并添加默认路由之后就可以访问外网了.<br>如果想外网访问2001:192:168:99::3/64, 是不能够使用下面的neigh proxy方式的, 因为网段和ISP分配的可路由网段根据就不一样嘛. 唯一的办法其实就是做DNAT<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv6.conf.all.proxy_ndp=1</span><br><span class="line">ip -6 neigh add proxy 2001:192:168:99:0:0:0:3 dev pppoe-wan</span><br><span class="line">ip -6 neigh show proxy</span><br></pre></td></tr></table></figure></p>
<h2 id="VPS不支持IPv6时如何使用IPv6"><a href="#VPS不支持IPv6时如何使用IPv6" class="headerlink" title="VPS不支持IPv6时如何使用IPv6"></a>VPS不支持IPv6时如何使用IPv6</h2><p>VPS若不支持IPv6, 可以通过tunnelbroker来配置6in4隧道支持IPv6, 但前提是VPS要能支持配置允许proto-41流量通过 (iptables -A INPUT -p 41 -j ACCEPT), 目前google cloud VPS是无法配置这个的.<br>若您的VPS支持这个, 可以继续. 先在<a href="https://www.tunnelbroker.net/" target="_blank" rel="external">https://www.tunnelbroker.net/</a> 登录后在’User Functions -&gt; Create Regular Tunnel’菜单创建 Create Regular Tunnel, 然后:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF | sudo tee -a /etc/network/interfaces</span><br><span class="line">auto he-ipv6</span><br><span class="line">iface he-ipv6 inet6 v4tunnel</span><br><span class="line">        address 2001:470:a:xx::2</span><br><span class="line">        netmask 64</span><br><span class="line">        endpoint 216.218.226.xx</span><br><span class="line">        local 162.xx.xx.xx</span><br><span class="line">        ttl 255</span><br><span class="line">        gateway 2001:470:a:4c4::1</span><br><span class="line">EOF</span><br><span class="line">cat &lt;&lt; EOF | sudo tee -a /etc/sysctl.conf</span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 0</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 0</span><br><span class="line">net.ipv6.conf.lo.disable_ipv6 = 0</span><br><span class="line">EOF</span><br><span class="line">sudo sysctl -p</span><br><span class="line">sudo apt install -y ifupdown</span><br><span class="line">sudo ifup he-ipv6</span><br><span class="line"># can&apos;t do tunnelbroker as 6in4 is unsupported (NAT/gateways won&apos;t pass proto-41)</span><br><span class="line">#sudo iptables -t nat -A PREROUTING -p 41 -d &lt;VPS-IP&gt; -j DNAT --to-destination &lt;tunnelbroker-ip&gt;</span><br><span class="line">#sudo iptables -t nat -A POSTROUTING -p 41 -d &lt;tunnelbroker-ip&gt; -j SNAT --to-source &lt;VPS-IP&gt;</span><br><span class="line">#sudo iptables -A INPUT -p 41 -j ACCEPT</span><br></pre></td></tr></table></figure>
<p>使用tunnelbroker需要VPS有公网IPv4地址, 若没有, 可以使用miredo(sudo apt-get install miredo), 但前提也是要防火墙允许proto-41的流量</p>
<h2 id="附件-OpenWRT自动恢复网络脚本"><a href="#附件-OpenWRT自动恢复网络脚本" class="headerlink" title="附件 - OpenWRT自动恢复网络脚本"></a>附件 - OpenWRT自动恢复网络脚本</h2><p>1, /etc/init.d/cron enable<br>2, crontab -e<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">*/4 * * * * /root/auto_restart.sh &gt;&gt; /var/log/cron.log 2&gt;&amp;1</span><br><span class="line">0 1 * * * /root/auto_restart_haproxy.sh &gt;&gt; /var/log/cron.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure></p>
<p>3, auto_restart.sh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">root@OpenWrt:~# cat /root/auto_restart.sh </span><br><span class="line">#! /bin/sh</span><br><span class="line"># /etc/init.d/cron start</span><br><span class="line"># /etc/init.d/cron enable</span><br><span class="line"># crontab -e</span><br><span class="line"># */5 * * * * /root/auto_restart.sh &gt;&gt; /var/log/cron.log 2&gt;&amp;1</span><br><span class="line">EXIST=$(ip route list 0.0.0.0/0)</span><br><span class="line">if test -z &quot;$EXIST&quot;</span><br><span class="line">then</span><br><span class="line">   echo $(date) &apos;default route disappears, adding it now ...&apos;</span><br><span class="line">   route add default dev pppoe-wan</span><br><span class="line">   sleep 5</span><br><span class="line">   echo $(date) &apos;check network connection ...&apos;</span><br><span class="line">   #ping -c 1 114.114.114.114 &amp;&gt; /dev/null &amp;&amp; echo &apos;success&apos; || /etc/init.d/network restart;</span><br><span class="line">   if ping -c 1 114.114.114.114 &amp;&gt; /dev/null</span><br><span class="line">   then</span><br><span class="line">      echo &apos;ping success&apos;</span><br><span class="line">   else</span><br><span class="line">      echo &apos;ping failed&apos;</span><br><span class="line">      /etc/init.d/network restart</span><br><span class="line">      # for ipv6</span><br><span class="line">      /etc/init.d/odhcpd restart</span><br><span class="line">      sleep 5</span><br><span class="line">      PREFIX=`ip -6 route list dev br-lan |grep 2409:8a00 |awk -F &apos;::/&apos; &apos;&#123;print $1&quot;::/64&quot;&#125;&apos; |uniq`</span><br><span class="line">      ip -6 route del $PREFIX dev br-lan &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">      # NOTE: too too important, we must add &apos;metric&apos; option, or this route will have expire time, or use &apos;ip&apos; to add route</span><br><span class="line">      route -A inet6 add $PREFIX dev br-lan metric 1 &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">   fi</span><br><span class="line">fi</span><br><span class="line">root@OpenWrt:~# cat /root/auto_restart_haproxy.sh </span><br><span class="line">#! /bin/sh</span><br><span class="line">echo $(date) &apos;restarting haproxy ...&apos;</span><br><span class="line">/etc/init.d/haproxy restart</span><br></pre></td></tr></table></figure></p>
<h2 id="20190119更新-最终的设置"><a href="#20190119更新-最终的设置" class="headerlink" title="20190119更新 - 最终的设置"></a>20190119更新 - 最终的设置</h2><p>使用relay模式即使按上面说的加metric之后有缓解但还是容易时不是断线, 不清楚什么原因, 这里一个android关于忽略RA的帖子, 估计有关吧. - <a href="https://issuetracker.google.com/issues/36949115" target="_blank" rel="external">https://issuetracker.google.com/issues/36949115</a><br>换回stateful IPv6还需要继续在openwrt上配置DHCPv6  Server的, 并且android等某些系统并不支持stateful IPv6<br>换回stateless IPv6也有问题, 在ubuntu系统上可以通过命令”echo ‘precedence ::ffff:0:0/96 100’ &gt;&gt; /etc/gai.conf”配置IPv4优先, 但android系统下对4G网络可以配置是否使用IPv4/IPv6, 但对于wifi则没这个配置项. 而stateless IPv6会给android分配2001:192:168:99::1的DHCPv6, 而在openwrt上要pdnsd支持IPv6的话还得再编译, 也是麻烦.<br>所以最后退回为单纯的SLAAC配置, 仅是通过openwrt路由器拿RA, 并不配置DHCPv6. 所以最终的配置为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">config dhcp &apos;lan&apos;                                </span><br><span class="line">        option interface &apos;lan&apos;                   </span><br><span class="line">        option start &apos;100&apos;                       </span><br><span class="line">        option limit &apos;150&apos;                       </span><br><span class="line">        option leasetime &apos;12h&apos;                   </span><br><span class="line">        list dhcp_option &apos;6,192.168.99.1&apos;        </span><br><span class="line">        option ra &apos;server&apos;</span><br></pre></td></tr></table></figure></p>
<p>并且禁用掉用于提供DHCPv6的odhcpd服务.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/odhcpd disable</span><br><span class="line">/etc/init.d/odhcpd stop</span><br></pre></td></tr></table></figure></p>
<p>看来要想支持IPv6好, 需要全生态链的所有软件都能支持IPv6好并且方便啊, 目前看到的android, openwrt与pdnsd在这方面都就有所欠缺.<br>但上面改了之后, 是不再给client发dhcp信息了, 但同样网卡上也没有配置IPv6地址(静态指定的除外), 原因仍然是openwrt上的bug, /etc/init.d/radvd脚本没有根据/etc/config/radvd里的配置生成/var/etc/radvd.conf, 那我们人工生成一个配置文件:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root@OpenWrt:~# cat /var/etc/radvd2.conf </span><br><span class="line">interface br-lan</span><br><span class="line">&#123;</span><br><span class="line">   AdvSendAdvert on;</span><br><span class="line">   prefix 2001:192:168:99::/64</span><br><span class="line">   &#123;</span><br><span class="line">       AdvOnLink on;</span><br><span class="line">       AdvAutonomous on;</span><br><span class="line">   &#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>然后修改/etc/init.d/radvd中的下列三行即可:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#[ -z &quot;$RADVD_CONFIG_FILE&quot; ] &amp;&amp; return 1</span><br><span class="line">#service_start /usr/sbin/radvd -C &quot;$RADVD_CONFIG_FILE&quot; -m stderr_syslog -p /var/run/radvd.pid</span><br><span class="line">service_start /usr/sbin/radvd -C /var/etc/radvd2.conf -m stderr_syslog -p /var/run/radvd.pid</span><br></pre></td></tr></table></figure></p>
<p>即使如此, 仍然有一些软件如和包之类的无法在IPv6与IPv4之间做fallback.<br>好吧, 看来IPv6的生态链太有问题了, 还是故意再把radvd恢复成不可用状态吧. 这样android手机就不会配置IPv6地址也不会配置DHCPv6从页可以正常使用和包之类的其他对IPv6支持不大好的软件, 同时ubuntu上想要使用IPv6的话就做静态配置指定IPv6地址吧.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/13/用OpenSSL做自签名的证书/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/13/用OpenSSL做自签名的证书/" itemprop="url">用OpenSSL做自签名的证书</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-13T13:10:13+08:00">
                2018-07-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华  发表于：2014-04-18<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>(<a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )<br>加密技术回顾<br>非对称加密算法如RSA的特点如下:<br>1, 公钥加密私钥解密, 大家都可以用我的公钥给我发加密的数据了, 因为只有我有私钥才能解密.<br>2, 私钥加密公钥解密叫数字签名(例如所谓的UEFI secure boot就是在主板硬件里集成一些操作系统的公钥，由主板硬件去校验操作系统是法合法，但关键是微软把持了公钥的申请，主板硬件厂商没有提供界面让用法自定义公钥，尤其在移动领域很多win8的硬件根本不提供关闭secure boot的选项这样就造成只能安装win8一种系统）, 大家收到我用私钥加密后的数据, 看用公钥能不能打得开, 能打开说明这数据确实是由我所发的, 因为别人没有我的私钥不可能伪造这些数据.<br>非对称加密去处很费时间, 我们一般采用对称密钥算法如DES来加密, 但对称密钥的保存是一个问题.<br>所以我们可以采用非对称加密算法来加密先协商交换对称密钥, 这就叫SSL. 假设客户端A的公私钥对是(P1,V1), 服务端B的公私钥对是(P2,V2), A需要确认和它通信的是B, 那么SSL的过程是:<br>首先, A和B都持有对方的公钥.<br>step1, A-&gt;B: hello 是step2, B-&gt;A: 用V2加密过的P1（即用户证书，A就用P2解密出P1, 这种数字签名方式让A确定了和它通信的是B）<br>step3, A-&gt;B: ok<br>step4, B-&gt;A: 用V1加密的一段信息<br>step5, A-&gt;B: 用P1加密一个自动生成的对称密钥K（用之前的P1解密成功这段信息则认为B是可信的了）<br>step6, B-&gt;A: 用K加密的数据（之后两对密钥功能结束，由K来加解密数据）<br>总结一下, 这里(P2,V2)就是certificate authority (CA)用来给客户签名用的公私钥。<br>(P1,V1)是客户自己的公私钥，提交给CA，CA所做的事情就是上述step2用(P2,V2)来给客户的(P1,V1)签名，简单吧？<br>V2是CA公司要保密的，而P2就是公用CA证书要安装到客户端</p>
<p>用V2加密过（签名过）的P1，称为用户证书，和用户私钥V1连起一个文件后, 一般被安装在服务器端。</p>
<p>X.509证书是一些标准字段的集合, 是包含有关用户或设备及其相应公钥信息的一种非常通用的证书格式, 目前版本是3. 必要字段包括:<br>1, 版本号<br>2, 由CA给每一个证书分配的序列号;<br>3, 证书使用的签名算法<br>4, 证书的认证机构<br>5, 证书的有效日期<br>6, 证书的所有人的唯一标识<br>7, 认证机构使用私钥的数字签名<br>8, 公钥信息<br>不同于PGP证书任何人都可以扮演认证者的角色, X.509证书的认证者只能是CA或由CA指定的人.要获得一份X.509证书，必须请求CA发给你证书。用户提供自己的公钥，证明自己拥有相应的私钥，并提供有关自己的某些特定信息。然后在这些信息上数字签名，并将整个数据包(称为证书请求)发给CA。CA做一些努力来验证用户提供的信息是正确的，然后就生成证书并返回给用户。<br>OpenSSL对X.509的支持如下:<br>(1) 证书请求管理<br>(2) 证书生成<br>(3) 证书吊销及CRL管理<br>(4) X509名字管理<br>(5) 属性管理<br>(6) 扩展管理<br>(7) 验证及信任管理</p>
<p>用OpenSSL做自签名的证书(pem格式)步骤:<br>1, 先生成CA的公私钥<br>   mkdir CA &amp; cd CA<br>   mkdir newcerts private<br>   echo ‘01’ &gt; serial #会生成以为个数字为名字的pem文件, 且每个数字自增1<br>   touch index.txt #生成记录数据库<br>   使用配置文件, 由于openssl命令行参数太多, 为避免写太多, 就使用一个配置文件代替, 如<a href="https://github.com/openstack/nova/blob/master/nova/CA/openssl.cnf.tmpl" target="_blank" rel="external">https://github.com/openstack/nova/blob/master/nova/CA/openssl.cnf.tmpl</a><br>   生成(P2,V2), 这时候P2=cacert.pem, V2=private/cakey.pem<br>   openssl req -new -x509 -extensions v3_ca -keyout private/cakey.pem -out cacert.pem -days 365 -config ./openssl.cnf -batch -nodes<br>   查看证书信息, openssl x509 -in cacert.pem -noout -text<br>2, 生成<p1,v1>,即Certificate signing Reqeust(CSR), P1=req.pem, V1=key.pem<br>   openssl req -new -nodes -out req.pem -config ./openssl.cnf<br>3, 用CA的私钥V2为P1签名, 即在newcerts目录生成用户证书cert.pem, 并更新数据库文件index.txt及serail文件<br>   openssl ca -out cert.pem -config ./openssl.cnf -infiles req.pem<br>   查看证书信息, openssl x509 -in cert.pem -noout -text<br>4, 安装证书<br>   用户私钥key.pem(V1)和用V2加密过的用户公钥(cert.pem)安装到服务端(有的服务器碉要把这两个文件连成一个,可以执行: cat key.pem cert.pem &gt; key-cert.pem), 如:<br>   /home/httpd/ssl/cert.pem Site certificate<br>   /home/httpd/ssl/key.pem Site private key<br>   最后将CA的公钥P2=cacert.pem安装到客户端</p1,v1></p>
<p>在OpenStack PKI认证中：<br>1, Keystone产生了CA公私钥: CA.pem, CA.key<br>2, Keystone产生了用户公私钥: keystone.pub, keystone.key<br>3, Keystone产生了用户证书: keystone.pem (即使用CA.key对keystone.pub进行了签名)<br>假如nova要使用PKI认证的话：<br>1, CA端，即keystone端，安装有: CA.pem, CA.key, keystone.key, keystone.pem<br>2, 用户端，即nova端，安装有：keystone.pem<br>过程：<br>1, 用户拿用户名和密码去keystone认证，keystone将用户信息通过keystone.key进行签名后作为token返回用户<br>2, 用户用这一token去访问nova, nova拿到token后，使用keystone.pem解密。（而原来的UUID方式nova还得再拿token去keystone那边验证一下是否有效，所以使用PKI方式能减轻keystone的压力。</p>
<p>再举个例子，如在安装openconnect时生成证书：</p>
<p>sudo apt-get -y install build-essential pkg-config libgnutls28-dev libreadline-dev libseccomp-dev libwrap0-dev libnl-nf-3-dev liblz4-dev gnutls-bin</p>
<p>#Create CA certificate<br>mkdir -p /tmp/cert &amp;&amp; cd /tmp/cert<br>cat &gt; /tmp/cert/ca.tmpl &lt;&lt; EOF<br>cn = “sts CA”<br>organization = “sts CA”<br>serial = 1<br>expiration_days = 3650<br>ca<br>signing_key<br>cert_signing_key<br>crl_signing_key<br>EOF</p>
<p>#Generate CA secret KEY: V2<br>certtool –generate-privkey –outfile CA.key</p>
<p>#Generate CA certifice: P2 signed by V2<br>certtool –generate-self-signed –load-privkey CA.key –template ca.tmpl –outfile CA.pem</p>
<p>#Create User certificate (here is for VPN server)<br>cat &gt; /tmp/cert/vpnserver.tmpl &lt;&lt; EOF<br>cn = “sts vpn server”<br>organization = “sts”<br>expiration_days = 3650<br>signing_key<br>encryption_key<br>tls_www_server<br>EOF</p>
<p>#Generate User secret KEY: V1<br>certtool –generate-privkey –outfile vpnserver.key</p>
<p>#Generate User certificate: <p1 signed="" by="" v2=""><br>certtool –generate-certificate –load-privkey vpnserver.key –load-ca-certificate CA.pem –load-ca-privkey CA.key –template vpnserver.tmpl –outfile vpnserver.pem</p1></p>
<p>#CA.pem,vpnserver,pem,vpnserver.key need to be installed in vpnserver<br>sudo cp CA.pem /etc/ssl/certs/CA.pem<br>sudo cp vpnserver.pem /etc/ssl/private/vpnserver.pem<br>sudo cp vpnserver.key /etc/ssl/private/vpnserver.key<br>OpenStack创建CA的方法：</p>
<p>openssl genrsa -out /etc/keystone/ssl/private/cakey.pem 1024<br>openssl req -new -x509 -extensions v3_ca -key /etc/keystone/ssl/private/cakey.pem -out /etc/keystone/ssl/certs/ca.pem -days 3650 -config /etc/keystone/ssl/certs/openssl.conf -subj /C=US/ST=Unset/L=Unset/O=Unset/CN=localhost<br>openssl genrsa -out /etc/keystone/ssl/private/keystonekey.pem 1024<br>openssl req -key /etc/keystone/ssl/private/keystonekey.pem -new -out /etc/keystone/ssl/certs/req.pem -config /etc/keystone/ssl/certs/openssl.conf -subj /C=US/ST=Unset/L=Unset/O=Unset/CN=localhost<br>openssl ca -batch -out /etc/keystone/ssl/certs/keystone.pem -config /etc/keystone/ssl/certs/openssl.conf -days 3650d -cert /etc/keystone/ssl/certs/ca.pem -keyfile /etc/keystone/ssl/private/cakey.pem -infiles /etc/keystone/ssl/certs/req.pem</p>
<p>再看一个使用easy-rsa为openvpn生成证书的实例：</p>
<p>sudo apt-get install easy-rsa openssl<br>sudo cp -r /usr/share/easy-rsa/ /etc/openvpn<br>cd /etc/openvpn/easy-rsa<br>sudo chown -R <code>whoami</code>:root /etc/openvpn<br>mkdir /etc/openvpn/easy-rsa/keys<br>source ./vars<br>export KEY_COUNTRY=CN<br>export KEY_PROVINCE=BJ<br>export KEY_CITY=BJ<br>export KEY_ORG=sts<br>export KEY_OU=sts<br>export KEY_NAME=sts<br>export KEY_EMAIL=root@sts<br>export KEY_NAME=”server”<br>./clean-all<br>./build-ca<br>$ ls keys/<br>ca.crt  ca.key  index.txt  serial<br>./build-key-server server<br>$ ls keys/<br>01.pem  ca.key     index.txt.attr  serial      server.crt  server.key<br>ca.crt  index.txt  index.txt.old   serial.old  server.csr<br>cp /etc/openvpn/easy-rsa/keys/{server.crt,server.key,ca.crt} /etc/openvpn</p>
<p>#It’s ideal for each client connecting to the VPN to have its own unique certificate and key.</p>
<p>#This is preferable to generating one general certificate and key to use among all client devices.<br>./build-key client1<br>scp ubuntu@<server>:/etc/openvpn/easy-rsa/keys/client1.crt /etc/openvpn/<br>scp ubuntu@<server>:/etc/openvpn/easy-rsa/keys/client1.key /etc/openvpn/<br>scp ubuntu@<server>:/etc/openvpn/easy-rsa/keys/client.ovpn /etc/openvpn/<br>scp ubuntu@<server>:/etc/openvpn/easy-rsa/keys/ca.crt /etc/openvpn/</server></server></server></server></p>
<p>常见证书格式及转换</p>
<p>PKCS(Public-Key Cryptography Standards), 是由RSA实验室与其他安全系统开发商共同制定的一个公钥密码标准<br>X.509是常用的通用的证书格式, 所有的证书都符合PKI(Public Key Infrastructure)制定的的ITU-T X509国际标准<br>.cer/.crt是用于存储证书, 以二进制形式存储, 不含私钥<br>.pem跟.cer/.crt的区别是它以ascii来表示<br>pfx/p12用于存放个人证书/私钥, 他通常包含保护密码, 二进制存储, 转换如:openssl pkcs12 -export -clcerts -in server-cert.cer -inkey server-key.key -out server.p12</p>
<p>JKS和JCEKS是Java密钥库(KeyStore)的两种比较常见类型, 可以使用java提供的证书工具keytool(openssl和keytool都是可以用来管理证书的工具而已)进行转换(如:keytool -import -v -trustcacerts -storepass 123456 -alias server -file cacert.pem -keystore server.jks)</p>
<p>例如： k8s中的dashboard若不在浏览器里导入p12证书在采用RBAC授权时就会什么也看不到：</p>
<h1 id="generate-client-certificate-data"><a href="#generate-client-certificate-data" class="headerlink" title="generate client-certificate-data"></a>generate client-certificate-data</h1><p>grep ‘client-certificate-data’ /var/run/kubernetes/admin.kubeconfig | head -n 1 | awk ‘{print $2}’ | base64 -d &gt;&gt; kubecfg.crt</p>
<h1 id="generate-client-key-data"><a href="#generate-client-key-data" class="headerlink" title="generate client-key-data"></a>generate client-key-data</h1><p>grep ‘client-key-data’ /var/run/kubernetes/admin.kubeconfig | head -n 1 | awk ‘{print $2}’ | base64 -d &gt;&gt; kubecfg.key</p>
<h1 id="generate-p12"><a href="#generate-p12" class="headerlink" title="generate p12"></a>generate p12</h1><p>openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name “kubernetes-client”</p>
<p>非对称算法可以使用开源的GPG工具，可参考文档： <a href="http://wenku.baidu.com/link?url=d5fWoN46-y7581212dDUEt7dkUfFkriW0bKy_gwUjps4zpKH64jytimWDm-yfuKIwAtu0jFoWW_ocVTJhSeRb2_QQLUz8oIBQulU6jSI133" target="_blank" rel="external">http://wenku.baidu.com/link?url=d5fWoN46-y7581212dDUEt7dkUfFkriW0bKy_gwUjps4zpKH64jytimWDm-yfuKIwAtu0jFoWW_ocVTJhSeRb2_QQLUz8oIBQulU6jSI133</a></p>
<p>及：<br><a href="https://help.ubuntu.com/community/GnuPrivacyGuardHowto" target="_blank" rel="external">https://help.ubuntu.com/community/GnuPrivacyGuardHowto</a></p>
<p>sudo apt-get install rng-tools<br>sudo rngd -r /dev/urandom</p>
<p>sudo apt-get install gnupg-agent<br>killall -q gpg-agent<br>eval $(gpg-agent –daemon)</p>
<p>创建密钥对：gpg –gen-key， 如创建了：”Zhang Hua (zhhuabj) <a href="&#109;&#x61;&#105;&#108;&#x74;&#111;&#x3a;&#118;&#x65;&#114;&#x79;&#x68;&#117;&#x61;&#50;&#48;&#48;&#x36;&#64;&#103;&#109;&#97;&#x69;&#x6c;&#46;&#99;&#111;&#109;">&#118;&#x65;&#114;&#x79;&#x68;&#117;&#x61;&#50;&#48;&#48;&#x36;&#64;&#103;&#109;&#97;&#x69;&#x6c;&#46;&#99;&#111;&#109;</a>“</p>
<pre><code>export GPGKEY=A24B36AE
</code></pre><p>查看公钥：gpg –list-public<br>查看私钥：gpg –list-secret-key<br>查看签名：gpg –list-sig<br>查看公钥指纹：gpg –fingerprint $GPGKEY<br>提取公钥：gpg –armor –output public.key –export $GPGKEY  或者： gpg –export -a $GPGKEY &gt; public.key</p>
<p>提取私钥：gpg -a –export-secret-keys $KEYID &gt; customer-mirror.key<br>生成公钥回收证书，当私钥出问题时可将它上传密钥服务器声明公钥作废:<br>  gpg –output revoke.asc –gen-revoke $GPGKEY<br>  声明作废：gpg –keyserver Server Address –send-keys $GPGKEY</p>
<p>迁移KEY</p>
<p>gpg –output mygpgkey_pub.gpg –armor –export  $GPGKEY<br>gpg –output mygpgkey_sec.gpg –armor –export-secret-key $GPGKEY</p>
<p>gpg –import mygpgkey_pub.gpg<br>gpg –allow-secret-key-import –import mygpgkey_sec.gpg</p>
<p>上传公钥到密钥服务器，如：gpg –send-keys –keyserver keyserver.ubuntu.com $GPGKEY 或把公钥导成文本之后直接在<a href="http://keyserver.ubuntu.com/这里提交公钥。" target="_blank" rel="external">http://keyserver.ubuntu.com/这里提交公钥。</a></p>
<p>查询:</p>
<p><a href="https://keyserver.ubuntu.com/pks/lookup?fingerprint=on&amp;op=index&amp;search=0x" target="_blank" rel="external">https://keyserver.ubuntu.com/pks/lookup?fingerprint=on&amp;op=index&amp;search=0x</a><public-key-long-format></public-key-long-format></p>
<p>目前只能用指纹：</p>
<p>gpg –keyid-format long –list-keys xx.xxx@xx.com<br>gpg –with-colons –fingerprint E6A84FFFCF31A67F | grep fpr | cut -d ‘:’ -f 10<br>gpg –recv-keys C7100A4CD0D8F3AE44212746E6A84FFFCF31A67F</p>
<p>交互命令窗口：gpg –cert-digest-algo=SHA256 –edit-key $GPGKEY</p>
<p>给自己加密文件，加密是用公钥，gpg –encrypt -r veryhua2006@gmail.com test.txt, 会生成名为test.txt.gpg的加密文件<br>给自己解决文件，gpg –decrypt test.txt.gpg &gt; test.txt</p>
<p>给别人加密文件当然要先导入别人的公钥：gpg –import otherpublic.key<br>核对对方的公钥指纹：gpg –fingerprint other@gmail.com<br>为别人加密文件: gpg –encrypt –recipient other@gmail.com test.txt<br>对别人的公钥进行签名，这样别人知道是你发的： gpg –sign-key other@gmail.com</p>
<p>对文件进行签名： gpg –clearsign file<br>验证签名是否完整： gpg –verity file.asc</p>
<p>OpenPGP能用于加密邮件，将GPG指纹注册到launchpad中(<a href="https://launchpad.net/~zhhuabj)，这样launchpad将给你发签名或加密过的邮件，然后再用openPGP" target="_blank" rel="external">https://launchpad.net/~zhhuabj)，这样launchpad将给你发签名或加密过的邮件，然后再用openPGP</a> enabled的邮件客户端如thunderbird来接收解密邮件和验证签名。<br>thunderbird通过enigmail插件来支持OpenPGP, Configure OpenPGP support in Thunderbird under Enigmail-&gt;Preferences and add under GnuPG executable path. The path for GnuPG is /usr/bin/gpg.<br>如果不想用邮件客户端，直接用firefox来访问如gmail等webmail的话，安装firegpg插件即可。chrome不需要装插件直接支持pgp解密。</p>
<p>将GPG指纹（gpg –fingerprint)注册到launchpad中(<a href="https://launchpad.net/~zhhuabj)后会收到一封邮件，内容类似如下，将其存成一个文件，再解密(gpg" target="_blank" rel="external">https://launchpad.net/~zhhuabj)后会收到一封邮件，内容类似如下，将其存成一个文件，再解密(gpg</a> –decrypt file.txt)后就生成了一个验证链接如<a href="https://launchpad.net/token/Hc7J9x7kF55CHTZJs，点击验证即可。" target="_blank" rel="external">https://launchpad.net/token/Hc7J9x7kF55CHTZJs，点击验证即可。</a><br>—–BEGIN PGP MESSAGE—–<br>Version: GnuPG v1.4.11 (GNU/Linux)<br>…….<br>52gY/bZADAl0xhScHvvuYquGS3oApfgtNM3UJWXa<br>=ZgnD<br>—–END PGP MESSAGE—–</p>
<p>Signed Ubuntu Code of Conduct in <a href="https://launchpad.net/~zhhuabj，" target="_blank" rel="external">https://launchpad.net/~zhhuabj，</a><br>1, 先下载UbuntuCodeofConduct-2.0.txt, <a href="https://launchpad.net/codeofconduct/2.0/+download" target="_blank" rel="external">https://launchpad.net/codeofconduct/2.0/+download</a><br>2, gpg –clearsign UbuntuCodeofConduct-2.0.txt<br>3, 将生成的UbuntuCodeofConduct-2.0.txt.asc文件再上传至 <a href="https://launchpad.net/codeofconduct/2.0/+sign即可。" target="_blank" rel="external">https://launchpad.net/codeofconduct/2.0/+sign即可。</a></p>
<p>2014-5-23日添加，配置使用Google Authenticator服务</p>
<p>Google帐户支持密码+临时验证码的两阶段验证方式。<br>临时验证码也支持直接短信发到手机上，也可以在Android手机上安装Google Authenticator服务来接收临时验证码。<br>具体先在<a href="https://accounts.google.com/b/0/SmsAuthConfig页面配置，并先生成google服务端为安装了Google" target="_blank" rel="external">https://accounts.google.com/b/0/SmsAuthConfig页面配置，并先生成google服务端为安装了Google</a> Authenticator服务的客户端生成的密钥。然后再在Google Authenticator里输入这个密钥就可以实现一次一密了。</p>
<p>2020-05-21更新　－　产生密钥;</p>
<p>#<a href="https://www.dazhuanlan.com/2019/12/15/5df63aed10999" target="_blank" rel="external">https://www.dazhuanlan.com/2019/12/15/5df63aed10999</a></p>
<p>#generate ca key pairs<br>mkdir -p ca/{private,certs,newcerts} &amp;&amp; cd ca<br>openssl genrsa -aes256 -passout pass:password -out private/ca.key.pem 4096<br>chmod 400 private/ca.key.pem<br>wget <a href="https://jamielinux.com/docs/openssl-certificate-authority/_downloads/root-config.txt" target="_blank" rel="external">https://jamielinux.com/docs/openssl-certificate-authority/_downloads/root-config.txt</a> -O openssl.cnf<br>sed -i “s,/root/ca,.,g” openssl.cnf<br>openssl req -config ./openssl.cnf -key private/ca.key.pem -new -x509 -days 7300 -sha256 -extensions v3_ca -passin pass:password \<br>     -subj “/C=CN/ST=BJ/O=STS/OU=quqi.com/CN=quqi.com.rootca/emailAddress=quqi@mail.com” -out certs/ca.cert.pem<br>chmod 444 certs/ca.cert.pem<br>openssl x509 -noout -text -in certs/ca.cert.pem #verify</p>
<p>#generate intermediate key pairs<br>mkdir -p intermediate/{certs,crl,csr,newcerts,private}<br>chmod 744 intermediate/private<br>touch index.txt &amp;&amp; echo 1000 &gt; serial &amp;&amp; echo 1000 &gt; crlnumber<br>openssl genrsa -aes256 -passout pass:password -out intermediate/private/intermediate.key.pem 4096<br>chmod 400 intermediate/private/intermediate.key.pem<br>cp ./openssl.cnf ./openssl-im.cnf</p>
<p>#modify the following section of openssl-im.cnf file<br>[ CA_default ]<br>dir             = .<br>private_key     = $dir/private/intermediate.key.pem<br>certificate     = $dir/certs/intermediate.cert.pem<br>crl             = $dir/crl/intermediate.crl.pem<br>policy          = policy_loose<br>openssl req -config ./openssl-im.cnf -new -sha256 -passin pass:password \<br>     -subj “/C=CN/ST=BJ/O=STS/OU=quqi.com/CN=quqi.com.imca/emailAddress=quqi@mail.com” \<br>     -key intermediate/private/intermediate.key.pem -out intermediate/csr/intermediate.csr.pem<br>openssl ca -config ./openssl.cnf -extensions v3_intermediate_ca -days 3650 -notext -md sha256 -passin pass:password \<br>     -in intermediate/csr/intermediate.csr.pem -out intermediate/certs/intermediate.cert.pem<br>chmod 444 intermediate/certs/intermediate.cert.pem<br>openssl x509 -noout -text -in intermediate/certs/intermediate.cert.pem<br>openssl verify -CAfile certs/ca.cert.pem intermediate/certs/intermediate.cert.pem</p>
<p>#generate certificate chain<br>cat intermediate/certs/intermediate.cert.pem certs/ca.cert.pem &gt; intermediate/certs/ca-chain.cert.pem<br>chmod 444 intermediate/certs/ca-chain.cert.pem</p>
<p>#generate clinet.quqi.com key pairs<br>openssl genrsa -out intermediate/private/client.quqi.com.key.pem 2048<br>chmod 444 intermediate/private/client.quqi.com.key.pem<br>openssl req -config ./openssl-im.cnf -key intermediate/private/client.quqi.com.key.pem \<br>     -subj “/C=CN/ST=BJ/O=STS/OU=quqi.com/CN=client.quqi.com/emailAddress=quqi@mail.com” \<br>     -new -sha256 -out intermediate/csr/client.quqi.com.csr.pem<br>openssl ca -config ./openssl.cnf -extensions server_cert -days 3650 -notext -md sha256 -passin pass:password\<br>     -in intermediate/csr/client.quqi.com.csr.pem -out intermediate/certs/client.quqi.com.cert.pem<br>chmod 444 intermediate/certs/client.quqi.com.cert.pem<br>openssl x509 -noout -text -in intermediate/certs/client.quqi.com.cert.pem<br>openssl verify -CAfile intermediate/certs/ca-chain.cert.pem intermediate/certs/client.quqi.com.cert.pem</p>
<p>参考:<br><a href="http://www.blogjava.net/alwayscy/archive/2006/12/01/84852.html" target="_blank" rel="external">http://www.blogjava.net/alwayscy/archive/2006/12/01/84852.html</a><br><a href="http://blog.sina.com.cn/s/blog_9e9d2211010199yj.html" target="_blank" rel="external">http://blog.sina.com.cn/s/blog_9e9d2211010199yj.html</a></p>
<p><a href="http://lingxiankong.github.io/blog/2014/02/06/openstack-ssl/" target="_blank" rel="external">http://lingxiankong.github.io/blog/2014/02/06/openstack-ssl/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/10/Set-up-k8s-development-env-by-quqi99/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/10/Set-up-k8s-development-env-by-quqi99/" itemprop="url">Set up k8s development env (by quqi99)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-10T17:47:57+08:00">
                2018-07-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (作者：张华 发表于：2018-07-10)</strong></p>
<h2 id="Sign-the-CLA"><a href="#Sign-the-CLA" class="headerlink" title="Sign the CLA"></a>Sign the CLA</h2><p>Sign via Hellosign - <a href="https://github.com/kubernetes/community/blob/master/CLA.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/CLA.md</a><br>then set email for github - <a href="https://github.com/settings/emails" target="_blank" rel="external">https://github.com/settings/emails</a><br>git config –global user.email “xxx@gmail.com”</p>
<h2 id="Run-local-k8s-via-source-code"><a href="#Run-local-k8s-via-source-code" class="headerlink" title="Run local k8s  via source code"></a>Run local k8s  via source code</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># Install some packages</span><br><span class="line">sudo apt install -y gcc make socat git build-essential</span><br><span class="line"></span><br><span class="line"># Install docker</span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br><span class="line">sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt-cache policy docker-ce</span><br><span class="line">sudo apt install docker-ce</span><br><span class="line"></span><br><span class="line"># Change default location of docker image</span><br><span class="line">service docker stop</span><br><span class="line">rsync -aXS /var/lib/docker/* /bak/.docker/</span><br><span class="line">rm -rf /var/lib/docker/*</span><br><span class="line">echo /bak/.docker/ /var/lib/docker none bind 0 0 &gt;&gt; /etc/fstab</span><br><span class="line">mount –a</span><br><span class="line">service docker start</span><br><span class="line"></span><br><span class="line"># Install etcd &gt; 3.2.13</span><br><span class="line">ETCD_VER=v3.2.18</span><br><span class="line">DOWNLOAD_URL=&quot;https://github.com/coreos/etcd/releases/download&quot;</span><br><span class="line">curl -L $&#123;DOWNLOAD_URL&#125;/$&#123;ETCD_VER&#125;/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz -o /tmp/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz</span><br><span class="line">tar xzvf /tmp/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz</span><br><span class="line">sudo /bin/cp -f etcd-$&#123;ETCD_VER&#125;-linux-amd64/&#123;etcd,etcdctl&#125; /usr/bin</span><br><span class="line">rm -rf /tmp/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz etcd-$&#123;ETCD_VER&#125;-linux-amd64</span><br><span class="line"></span><br><span class="line"># Install golang &gt; 1.10.2</span><br><span class="line">wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz</span><br><span class="line">sudo rm -rf /usr/lib/go &amp;&amp; sudo tar -C /usr/lib -xzf go1.10.3.linux-amd64.tar.gz</span><br><span class="line">export GOROOT=/usr/lib/go</span><br><span class="line">export GOPATH=/bak/golang</span><br><span class="line">export PATH=$GOROOT/bin:$GOPATH/bin:$PATH</span><br><span class="line"></span><br><span class="line"># Install and run kubernetes in local env - https://www.cnblogs.com/edisonxiang/p/6951787.html</span><br><span class="line">mkdir -p $GOPATH/src/k8s.io</span><br><span class="line">#go get -d k8s.io/kubernetes</span><br><span class="line">git clone git@github.com:zhhuabj/kubernetes.git $GOPATH/src/k8s.io/kubernetes</span><br><span class="line">#make GOGCFLAGS=&quot;-N -l&quot;  #Debug it</span><br><span class="line">sudo usermod -a -G docker $&#123;USER&#125;</span><br><span class="line">sudo systemctl restart docker.service</span><br><span class="line">sudo systemctl disable kubelet.service</span><br><span class="line">sudo systemctl stop kubelet.service</span><br><span class="line"></span><br><span class="line">#注意：一直不成功的原因是需要用小写true，它是区分大小写的</span><br><span class="line">KUBE_ENABLE_CLUSTER_DASHBOARD=true ./hack/local-up-cluster.sh</span><br><span class="line">#注意：加GO_OUT可避免再次编译</span><br><span class="line">GO_OUT=/bak/golang/src/k8s.io/kubernetes/_output/bin</span><br><span class="line">KUBE_ENABLE_CLUSTER_DASHBOARD=true ./hack/local-up-cluster.sh</span><br><span class="line"></span><br><span class="line"># Test local env</span><br><span class="line">export KUBECONFIG=/var/run/kubernetes/admin.kubeconfig</span><br><span class="line">cluster/kubectl.sh get pods --all-namespaces</span><br></pre></td></tr></table></figure>
<h2 id="github-process"><a href="#github-process" class="headerlink" title="github process"></a>github process</h2><p><a href="https://github.com/kubernetes/community/blob/master/contributors/guide/github-workflow.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/contributors/guide/github-workflow.md</a><br>k8s与openstack不一样，openstack使用gerrit来review code, 但是k8s使用github的PR机制。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">kubernetes提交PR的流程可以采用pull模型（Shared Repository Model，https://gist.github.com/seshness/3943237），也可以采用fork模型（https://www.cnblogs.com/edisonxiang/p/6951787.html）。我们采用fork模型：</span><br><span class="line"></span><br><span class="line"># Click &apos;Fork&apos; button to fork your own branch - https://github.com/kubernetes/kubernetes, then we have https://github.com/zhhuabj/kubernetes</span><br><span class="line">mkdir -p $GOPATH/src/k8s.io</span><br><span class="line">git clone git@github.com:zhhuabj/kubernetes.git $GOPATH/src/k8s.io/kubernetes</span><br><span class="line">cd kubernetes</span><br><span class="line">hack/local-up-cluster.sh</span><br><span class="line"></span><br><span class="line"># set up upstream branch</span><br><span class="line">git remote add upstream https://github.com/kubernetes/kubernetes.git</span><br><span class="line">git remote set-url --push upstream no_push</span><br><span class="line">git remote -v</span><br><span class="line"></span><br><span class="line"># Update our branch</span><br><span class="line">git fetch upstream</span><br><span class="line">git checkout master</span><br><span class="line">git rebase upstream/master</span><br><span class="line">#git pull upstream master</span><br><span class="line"></span><br><span class="line"># Add new branch myfeature</span><br><span class="line">git checkout -b myfeature</span><br><span class="line">git config --global user.email &quot;veryhua2006@gmail.com&quot;</span><br><span class="line">git config --global user.name &quot;zhhuabj&quot;</span><br><span class="line"># Add or Modify files</span><br><span class="line">...</span><br><span class="line">git add .</span><br><span class="line">git commit -a -F ./msg</span><br><span class="line">git commit --amend -a -F ./message</span><br><span class="line">git commit -m &quot;update&quot;</span><br><span class="line">git push origin myfeature</span><br><span class="line">git push origin :myfeature  #delete remote branch</span><br><span class="line"></span><br><span class="line"># Rebase unmerged PR into our repo</span><br><span class="line">git fetch upstream pull/56136/head:BRANCHNAME</span><br><span class="line"></span><br><span class="line"># Merge multiple local commits into a full commit by using &apos;git squash&apos;</span><br><span class="line">git log</span><br><span class="line">git rebase -i HEAD~6 把顶部的六个版本聚到一起进入编辑页面</span><br><span class="line">　　把需要压缩的日志前面的pick都改为s（squash的缩写）</span><br><span class="line">　　注意必须保留一个pick，如果将所有的pick都改为了s那就没有合并的载体了就会报如下错误</span><br><span class="line">　　依次输入CTRL+X Y ENTER三个命令完成编辑。</span><br><span class="line">　　最后Git Push orgin branchname</span><br><span class="line"></span><br><span class="line"># Pull Request - https://github.com/zhhuabj/kubernetes, 在新上传的Branch上，点击Compare &amp; Pull Request按钮创建一个Pull Requst</span><br><span class="line"></span><br><span class="line"># 最后https://github.com/kubernetes/kubernetes/pulls就可以找到刚刚提交的Pull Request。</span><br></pre></td></tr></table></figure></p>
<h2 id="Review-process"><a href="#Review-process" class="headerlink" title="Review process"></a>Review process</h2><p><a href="https://github.com/kubernetes/community/blob/master/contributors/guide/pull-requests.md#the-testing-and-merge-workflow" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/contributors/guide/pull-requests.md#the-testing-and-merge-workflow</a><br> openstack社区更开放，使用gerrit机制，新人都能review代码，并能+1。<br>但k8s使用github的PR，相对封闭一些，新人是不能review代码的，新人的角色叫contributor，可以修改issue (在issue上回复/assign)并提交代码。<br>只有每个子模块下OWNERS文件定义的reviewer, approver角色的人员(<a href="https://github.com/kubernetes/community/blob/master/community-membership.md)才能review代码(reviewer可以LDTM(looks" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/community-membership.md)才能review代码(reviewer可以LDTM(looks</a> good to me, +1), approver可以+2，一个+1一个+2就可以进代码，但openstack中是只能两个+2才可以)</p>
<p>如果要为某个issue创建PR, 需要在PR的描述里填写fixes #issue_num 。这样PR在 merge后issue会“自动”关闭。PR创建后，k8s机器人会做以下几件事：<br>在相应OWNER列表里选取一个人做为reviewer<br>如果是kubernetes member，则启动CI来检查PR，例如UT, e2e test；如果不是kuberentes member ，则需要一个member帮忙启动相应ci<br>待CI没有问题后，可以ping相应的reviewers来检查代码了</p>
<p>在reviewer认为可以后，需要标lgtm (look go to me) 标签；同时需要该模块的approver标记approve标签。两个标签都有了以后，就可以等待合并了。代码的合并也是由k8s机器人完成的，可以在 <a href="http://submit-queue.k8s.io/#/queue" target="_blank" rel="external">http://submit-queue.k8s.io/#/queue</a> 看到等待合并的PR。在合并之前，k8s机器人也会自动重新跑ci以保证代码没有问题。<br>以上三步差不多就可以将typo提交到主干上。其中大部分工作都有k8s机器人自动完成，比如分配reviewer。<br>  Bot命令如下：</p>
<ul>
<li>Jenkins verification: @k8s-bot verify test this</li>
<li>GCE E2E: @k8s-bot cvm gce e2e test this</li>
<li>Test all: @k8s-bot test this please, issue #IGNORE</li>
<li>CRI test: @k8s-bot cri test this.</li>
<li>Verity test: @k8s-bot verify test this</li>
<li>LGTM (only applied if you are one of assignees):: /lgtm</li>
<li>LGTM cancel: /lgtm cancel<br>更多命令见 <a href="https://prow.k8s.io/command-help" target="_blank" rel="external">https://prow.k8s.io/command-help</a><h2 id="How-to-do-test"><a href="#How-to-do-test" class="headerlink" title="How to do test"></a>How to do test</h2><a href="https://github.com/kubernetes/community/blob/master/contributors/devel/testing.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/contributors/devel/testing.md</a><br>make verify<br>make test<br>make test-integration<h2 id="How-to-debug-k8s"><a href="#How-to-debug-k8s" class="headerlink" title="How to debug k8s"></a>How to debug k8s</h2>local-up-cluster.sh是通过_output/local/bin/linux/amd64/hyperkube在容器里启动k8s各服务的，那样是不方便使用(dlv exec ${OUTDIR}/bin/kubelet – $kubelet_flags)来调试基于B/S的k8s服务的，那首先将k8s各服务以本地进程的形式启动，这样调试k8s服务就变得像调试openstack服务一样。</li>
</ul>
<p>1， 第一步创建systemd启动配置文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line">$ cat /lib/systemd/system/kube-etcd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kube-etcd Service</span><br><span class="line">After=network.target</span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">ExecStart=/usr/bin/etcd -name etcd -data-dir /var/lib/etcd \</span><br><span class="line">          -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 \</span><br><span class="line">          -advertise-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001</span><br><span class="line">Restart=always</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=default.target</span><br><span class="line"></span><br><span class="line">$ cat /lib/systemd/system/kube-apiserver.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kube-apiserver Service</span><br><span class="line">After=network.target</span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">ExecStart=/bak/golang/src/k8s.io/kubernetes/_output/bin/kube-apiserver \</span><br><span class="line">            --admission-control=NamespaceAutoProvision,LimitRanger,SecurityContextDeny \</span><br><span class="line">            --apiserver-count=1 \</span><br><span class="line">            --cors-allowed-origins=.* \</span><br><span class="line">            --enable-garbage-collector=false \</span><br><span class="line">            --etcd-servers=http://127.0.0.1:2379 \</span><br><span class="line">            --insecure-bind-address=0.0.0.0 \</span><br><span class="line">            --insecure-port=8080 \</span><br><span class="line">            --log-dir=~/.kube/log/kube-apiserver \</span><br><span class="line">            --logtostderr=false \</span><br><span class="line">            --service-cluster-ip-range=10.0.0.0/16 \</span><br><span class="line">            --v=5 \</span><br><span class="line">Restart=always</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=default.target</span><br><span class="line"></span><br><span class="line">$ cat /lib/systemd/system/kube-controller-manager.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kube-controller-manager Service</span><br><span class="line">After=network.target</span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">ExecStart=/bak/golang/src/k8s.io/kubernetes/_output/bin/kube-controller-manager \</span><br><span class="line">          --enable-garbage-collector=false \</span><br><span class="line">          --logtostderr=false \</span><br><span class="line">          --log-dir=~/.kube/log/kube-controller-manager \</span><br><span class="line">          --pod-eviction-timeout=5m0s \</span><br><span class="line">          --master=http://0.0.0.0:8080 \</span><br><span class="line">          --node-monitor-grace-period=40s \</span><br><span class="line">          --terminated-pod-gc-threshold=12500 \</span><br><span class="line">          --leader-elect=true \</span><br><span class="line">          --v=4 \</span><br><span class="line">Restart=always</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=default.target</span><br><span class="line"></span><br><span class="line">$ cat /lib/systemd/system/kube-scheduler.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kube-scheduler Service</span><br><span class="line">After=network.target</span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">ExecStart=/bak/golang/src/k8s.io/kubernetes/_output/bin/kube-scheduler \</span><br><span class="line">            --log-dir=~/.k8s/log/kube-scheduler \</span><br><span class="line">            --logtostderr=false \</span><br><span class="line">            --master=http://0.0.0.0:8080 \</span><br><span class="line">            --leader-elect=true \</span><br><span class="line">            --v=5 \</span><br><span class="line">Restart=always</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=default.target</span><br><span class="line"></span><br><span class="line"># prepare kubelet.kubeconfig and kube-proxy.kubeconfig</span><br><span class="line">export KUBE_APISERVER=&quot;http://127.0.0.1:8080&quot;</span><br><span class="line">./_output/bin/kubectl config set-cluster myk8s --server=$&#123;KUBE_APISERVER&#125; --kubeconfig=kubelet.kubeconfig</span><br><span class="line">./_output/bin/kubectl config set-credentials kubelet --kubeconfig=kubelet.kubeconfig</span><br><span class="line">./_output/bin/kubectl config set-context myk8s-context --cluster=myk8s --user=kubelet --kubeconfig=kubelet.kubeconfig</span><br><span class="line">./_output/bin/kubectl config use-context myk8s-context --kubeconfig=kubelet.kubeconfig</span><br><span class="line"></span><br><span class="line">./_output/bin/kubectl config set-cluster myk8s --server=$&#123;KUBE_APISERVER&#125; --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line">./_output/bin/kubectl config set-credentials kube-proxy --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line">./_output/bin/kubectl config set-context myk8s-context --cluster=myk8s --user=kube-proxy --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line">./_output/bin/kubectl config use-context myk8s-context --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line">cp *.kubeconfig /home/hua/.kube/</span><br><span class="line"></span><br><span class="line">$ cat /lib/systemd/system/kubelet.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=kubelet: The Kubernetes Node Agent</span><br><span class="line">Documentation=http://kubernetes.io/docs/</span><br><span class="line">[Service]</span><br><span class="line">ExecStart=/bak/golang/src/k8s.io/kubernetes/_output/bin/kubelet \</span><br><span class="line">          --address=127.0.0.1 --port=10250 --hostname-override=127.0.0.1 \</span><br><span class="line">          --pod-infra-container-image=docker.io/kubernetes/pause \</span><br><span class="line">          --fail-swap-on=false --cgroup-driver=cgroupfs \</span><br><span class="line">          --kubeconfig=/home/hua/.kube/kubelet.kubeconfig \</span><br><span class="line">          --runtime-cgroups=/systemd/system.slice \</span><br><span class="line">          --kubelet-cgroups=/systemd/system.slice \</span><br><span class="line">          --eviction-hard=&apos;nodefs.available&lt;1%&apos; \</span><br><span class="line">          --logtostderr=false --log-dir=~/.kube/log/kubelet --v=4</span><br><span class="line">Restart=always</span><br><span class="line">StartLimitInterval=0</span><br><span class="line">RestartSec=10</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line">$ cat /lib/systemd/system/kube-proxy.service[Unit]</span><br><span class="line">Description=Kube-proxy Service</span><br><span class="line">After=network.target</span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">ExecStart=/bak/golang/src/k8s.io/kubernetes/_output/bin/kube-proxy \</span><br><span class="line">            --log-dir=~/.k8s/log/kube-proxy \</span><br><span class="line">            --logtostderr=false \</span><br><span class="line">            --master=http://0.0.0.0:8080 \</span><br><span class="line">            --kubeconfig=/home/hua/.kube/kube-proxy.kubeconfig \</span><br><span class="line">            --proxy-mode=userspace \</span><br><span class="line">            --v=5</span><br><span class="line">Restart=always</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=default.target</span><br></pre></td></tr></table></figure></p>
<p>2， 第二步，启动各服务:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl --system daemon-reload</span><br><span class="line">sudo systemctl start kube-etcd.service</span><br><span class="line">etcdctl -C http://localhost:4001 cluster-health</span><br><span class="line">sudo systemctl start kube-apiserver.service</span><br><span class="line">sudo systemctl start kube-controller-manager.service</span><br><span class="line">sudo systemctl start kube-scheduler.service</span><br><span class="line">sudo systemctl start kubelet.service</span><br></pre></td></tr></table></figure></p>
<p>3, 第二步，验证安装是否正确：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$ /bak/golang/src/k8s.io/kubernetes/_output/bin/kubectl -s http://127.0.0.1:8080 get componentstatus</span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">controller-manager   Healthy   ok</span><br><span class="line">scheduler            Healthy   ok</span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class="line"></span><br><span class="line">/bak/golang/src/k8s.io/kubernetes/_output/bin/kubectl config set-cluster myk8s --server=http://127.0.0.1:8080</span><br><span class="line">/bak/golang/src/k8s.io/kubernetes/_output/bin/kubectl config set-context myk8s-context --cluster=myk8s --namespace=default --user=client</span><br><span class="line">/bak/golang/src/k8s.io/kubernetes/_output/bin/kubectl config use-context myk8s-context</span><br><span class="line">/bak/golang/src/k8s.io/kubernetes/_output/bin/kubectl config set preferences.colors true</span><br><span class="line">$ cat ~/.kube/config</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    server: http://127.0.0.1:8080</span><br><span class="line">  name: myk8s</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: myk8s</span><br><span class="line">    namespace: default</span><br><span class="line">    user: client</span><br><span class="line">  name: myk8s-context</span><br><span class="line">current-context: myk8s-context</span><br><span class="line">kind: Config</span><br><span class="line">preferences:</span><br><span class="line">  colors: true</span><br><span class="line">users: []</span><br><span class="line"></span><br><span class="line">$ /bak/golang/src/k8s.io/kubernetes/_output/bin/kubectl get componentstatus</span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">controller-manager   Healthy   ok</span><br><span class="line">scheduler            Healthy   ok</span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class="line">$ ./_output/bin/kubectl get nodes</span><br><span class="line">NAME        STATUS    ROLES     AGE       VERSION</span><br><span class="line">127.0.0.1   Ready     &lt;none&gt;    11m       v1.12.0-alpha.0.1999+32dc6cc08aa034-dirty</span><br><span class="line">$ ./_output/bin/kubectl get events</span><br></pre></td></tr></table></figure></p>
<p>4，第四步，例如要调试kubelet服务的话，先停止该服务(sudo systemctl stop kubelet)，然后使用(dlv exec ${OUTDIR}/bin/kubelet – $kubelet_flags)命令启动，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo /bak/golang/bin/dlv --headless -l 127.0.0.1:1234 exec /bak/golang/src/k8s.io/kubernetes/_output/bin/kubelet -- --fail-swap-on=False --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice  --v=4</span><br><span class="line">API server listening at: 127.0.0.1:1234</span><br><span class="line"></span><br><span class="line">$ sudo /bak/golang/bin/dlv connect 127.0.0.1:1234</span><br><span class="line">Type &apos;help&apos; for list of commands.</span><br><span class="line">(dlv) b main.main</span><br><span class="line">Breakpoint 1 set at 0x2d08348 for main.main() ./_output/local/go/src/k8s.io/kubernetes/cmd/kubelet/kubelet.go:36</span><br><span class="line">(dlv) c</span><br></pre></td></tr></table></figure></p>
<h2 id="安装dashboard"><a href="#安装dashboard" class="headerlink" title="安装dashboard"></a>安装dashboard</h2><p>该命令(KUBE_ENABLE_CLUSTER_DASHBOARD=true ./hack/local-up-cluster.sh）会自动安装dashboard。<br>注意：如果不成功原因是需要用小写true，它是区分大小写的。</p>
<p>安装成功后使用命令（cluster/kubectl.sh cluster-info）查看它的访问地址如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://localhost:6443//api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</span><br></pre></td></tr></table></figure></p>
<p>这个链接有点问题，在api处有两个斜线会造成看不到UI，改成如下的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://localhost:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</span><br></pre></td></tr></table></figure></p>
<p>也可运行命令（kubectl proxy –port=8001 –kubeconfig=/var/run/kubernetes/admin.kubeconfig –accept-hosts=’^*$’）访问：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</span><br></pre></td></tr></table></figure></p>
<p>上面的’–accept-hosts’用于在非本机外部访问，但Dashboard只允许localhost和127.0.0.1使用HTTP连接进行访问，而其它地址只允许使用HTTPS。因此，如果需要在非本机访问Dashboard的话，只能采用NodePort:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n kube-system edit service kubernetes-dashboard</span><br><span class="line">$ kubectl -n kube-system get service kubernetes-dashboard</span><br><span class="line">NAME                   TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kubernetes-dashboard   NodePort   10.0.0.232   &lt;none&gt;        443:31050/TCP   1h</span><br><span class="line">visit: https://192.168.99.216:31050/</span><br></pre></td></tr></table></figure></p>
<p>这时访问dashboard仍然有下列问题:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;message&quot;: &quot;services \&quot;https:kubernetes-dashboard:\&quot; is forbidden: User \&quot;system:anonymous\&quot; cannot get services/proxy in the namespace \&quot;kube-system\&quot;: no RBAC policy matched&quot;,</span><br></pre></td></tr></table></figure></p>
<p>这是因为最新版的k8s默认启用了RBAC(–authorization-mode=Node,RBAC)，并为未认证用户赋予了一个默认的身份：anonymous<br>对于API Server来说，它是使用证书进行认证的，我们需要先创建一个证书：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># generate client-certificate-data</span><br><span class="line">grep &apos;client-certificate-data&apos; /var/run/kubernetes/admin.kubeconfig | head -n 1 | awk &apos;&#123;print $2&#125;&apos; | base64 -d &gt;&gt; kubecfg.crt</span><br><span class="line"># generate client-key-data</span><br><span class="line">grep &apos;client-key-data&apos; /var/run/kubernetes/admin.kubeconfig | head -n 1 | awk &apos;&#123;print $2&#125;&apos; | base64 -d &gt;&gt; kubecfg.key</span><br><span class="line"># generate p12</span><br><span class="line">openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name &quot;kubernetes-client&quot;</span><br></pre></td></tr></table></figure></p>
<p>然后将该p12证书导入到浏览器即可。此时默认的anonymous身份的token可以这样获取:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cluster/kubectl.sh get secret -n kube-system | grep dashboard</span><br><span class="line">cluster/kubectl.sh -n kube-system  get secret kubernetes-dashboard-token-kglhd -o jsonpath=&#123;.data.token&#125;| base64 -d</span><br></pre></td></tr></table></figure></p>
<p>anonymous身份可能看不到很多东西，所以我们再在kube-system名空间下再创建一个admin用户并和cluster-admin角色关联：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /tmp/admin-user.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line">cat &gt; /tmp/admin-user-role-binding.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line">cluster/kubectl.sh create -f /tmp/admin-user.yaml</span><br><span class="line">cluster/kubectl.sh create -f /tmp/admin-user-role-binding.yaml</span><br><span class="line">cluster/kubectl.sh -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin | awk &apos;&#123;print $1&#125;&apos;)</span><br></pre></td></tr></table></figure>
<p>##直接修改local-up-cluster.sh代替hyperkube用本地进程启动 ##<br>或者直接修改脚本去掉hyperkube, 然后运行ENABLE_CLUSTER_DASHBOARD=True ./hack/local-up-cluster.sh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">diff --git a/hack/local-up-cluster.sh b/hack/local-up-cluster.sh</span><br><span class="line">index 3b688d3..95de0df 100755</span><br><span class="line">--- a/hack/local-up-cluster.sh</span><br><span class="line">+++ b/hack/local-up-cluster.sh</span><br><span class="line">@@ -202,7 +202,8 @@ do</span><br><span class="line"> done</span><br><span class="line"></span><br><span class="line"> if [ &quot;x$GO_OUT&quot; == &quot;x&quot; ]; then</span><br><span class="line">-    make -C &quot;$&#123;KUBE_ROOT&#125;&quot; WHAT=&quot;cmd/kubectl cmd/hyperkube&quot;</span><br><span class="line">+    #make -C &quot;$&#123;KUBE_ROOT&#125;&quot; WHAT=&quot;cmd/kubectl cmd/hyperkube&quot;</span><br><span class="line">+    make -C &quot;$&#123;KUBE_ROOT&#125;&quot; GOGCFLAGS=&quot;-N -l&quot; WHAT=&quot;cmd/kubelet cmd/kube-proxy cmd/kube-apiserver cmd/kube-controller-manager cmd/cloud-controller-manager cmd/kube-scheduler cmd/kubectl&quot;</span><br><span class="line"> else</span><br><span class="line">     echo &quot;skipped the build.&quot;</span><br><span class="line"> fi</span><br><span class="line">@@ -578,7 +579,7 @@ function start_apiserver &#123;</span><br><span class="line">     fi</span><br><span class="line"></span><br><span class="line">     APISERVER_LOG=$&#123;LOG_DIR&#125;/kube-apiserver.log</span><br><span class="line">-    $&#123;CONTROLPLANE_SUDO&#125; &quot;$&#123;GO_OUT&#125;/hyperkube&quot; apiserver $&#123;swagger_arg&#125; $&#123;audit_arg&#125; $&#123;authorizer_arg&#125; $&#123;priv_arg&#125; $&#123;runtime_config&#125; \</span><br><span class="line">+    $&#123;CONTROLPLANE_SUDO&#125; &quot;$&#123;GO_OUT&#125;/kube-apiserver&quot; $&#123;swagger_arg&#125; $&#123;audit_arg&#125; $&#123;authorizer_arg&#125; $&#123;priv_arg&#125; $&#123;runtime_config&#125; \</span><br><span class="line">       $&#123;cloud_config_arg&#125; \</span><br><span class="line">       $&#123;advertise_address&#125; \</span><br><span class="line">       $&#123;node_port_range&#125; \</span><br><span class="line">@@ -650,7 +651,7 @@ function start_controller_manager &#123;</span><br><span class="line">     fi</span><br><span class="line"></span><br><span class="line">     CTLRMGR_LOG=$&#123;LOG_DIR&#125;/kube-controller-manager.log</span><br><span class="line">-    $&#123;CONTROLPLANE_SUDO&#125; &quot;$&#123;GO_OUT&#125;/hyperkube&quot; controller-manager \</span><br><span class="line">+    $&#123;CONTROLPLANE_SUDO&#125; &quot;$&#123;GO_OUT&#125;/kube-controller-manager&quot; \</span><br><span class="line">       --v=$&#123;LOG_LEVEL&#125; \</span><br><span class="line">       --vmodule=&quot;$&#123;LOG_SPEC&#125;&quot; \</span><br><span class="line">       --service-account-private-key-file=&quot;$&#123;SERVICE_ACCOUNT_KEY&#125;&quot; \</span><br><span class="line">@@ -685,7 +686,7 @@ function start_cloud_controller_manager &#123;</span><br><span class="line">     fi</span><br><span class="line"></span><br><span class="line">     CLOUD_CTLRMGR_LOG=$&#123;LOG_DIR&#125;/cloud-controller-manager.log</span><br><span class="line">-    $&#123;CONTROLPLANE_SUDO&#125; $&#123;EXTERNAL_CLOUD_PROVIDER_BINARY:-&quot;$&#123;GO_OUT&#125;/hyperkube&quot; cloud-controller-manager&#125; \</span><br><span class="line">+    $&#123;CONTROLPLANE_SUDO&#125; $&#123;EXTERNAL_CLOUD_PROVIDER_BINARY:-&quot;$&#123;GO_OUT&#125;/cloud-controller-manager&quot;&#125; \</span><br><span class="line">       --v=$&#123;LOG_LEVEL&#125; \</span><br><span class="line">       --vmodule=&quot;$&#123;LOG_SPEC&#125;&quot; \</span><br><span class="line">       $&#123;node_cidr_args&#125; \</span><br><span class="line">@@ -791,7 +792,7 @@ function start_kubelet &#123;</span><br><span class="line">     )</span><br><span class="line"></span><br><span class="line">     if [[ -z &quot;$&#123;DOCKERIZE_KUBELET&#125;&quot; ]]; then</span><br><span class="line">-      sudo -E &quot;$&#123;GO_OUT&#125;/hyperkube&quot; kubelet &quot;$&#123;all_kubelet_flags[@]&#125;&quot; &gt;&quot;$&#123;KUBELET_LOG&#125;&quot; 2&gt;&amp;1 &amp;</span><br><span class="line">+      sudo -E &quot;$&#123;GO_OUT&#125;/kubelet&quot; &quot;$&#123;all_kubelet_flags[@]&#125;&quot; &gt;&quot;$&#123;KUBELET_LOG&#125;&quot; 2&gt;&amp;1 &amp;</span><br><span class="line">       KUBELET_PID=$!</span><br><span class="line">     else</span><br><span class="line"></span><br><span class="line">@@ -889,14 +890,14 @@ EOF</span><br><span class="line">       done</span><br><span class="line">     fi &gt;&gt;/tmp/kube-proxy.yaml</span><br><span class="line"></span><br><span class="line">-    sudo &quot;$&#123;GO_OUT&#125;/hyperkube&quot; proxy \</span><br><span class="line">+    sudo &quot;$&#123;GO_OUT&#125;/kube-proxy&quot; \</span><br><span class="line">       --v=$&#123;LOG_LEVEL&#125; \</span><br><span class="line">       --config=/tmp/kube-proxy.yaml \</span><br><span class="line">       --master=&quot;https://$&#123;API_HOST&#125;:$&#123;API_SECURE_PORT&#125;&quot; &gt;&quot;$&#123;PROXY_LOG&#125;&quot; 2&gt;&amp;1 &amp;</span><br><span class="line">     PROXY_PID=$!</span><br><span class="line"></span><br><span class="line">     SCHEDULER_LOG=$&#123;LOG_DIR&#125;/kube-scheduler.log</span><br><span class="line">-    $&#123;CONTROLPLANE_SUDO&#125; &quot;$&#123;GO_OUT&#125;/hyperkube&quot; scheduler \</span><br><span class="line">+    $&#123;CONTROLPLANE_SUDO&#125; &quot;$&#123;GO_OUT&#125;/kube-scheduler&quot; \</span><br><span class="line">       --v=$&#123;LOG_LEVEL&#125; \</span><br><span class="line">       --kubeconfig &quot;$CERT_DIR&quot;/scheduler.kubeconfig \</span><br><span class="line">       --feature-gates=&quot;$&#123;FEATURE_GATES&#125;&quot; \</span><br></pre></td></tr></table></figure></p>
<h2 id="How-to-read-source-code"><a href="#How-to-read-source-code" class="headerlink" title="How to read source code"></a>How to read source code</h2><p><a href="http://dockone.io/article/895" target="_blank" rel="external">http://dockone.io/article/895</a></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://kubernetes.io/docs/imported/community/devel/" target="_blank" rel="external">https://kubernetes.io/docs/imported/community/devel/</a><br>[2] <a href="https://github.com/kubernetes/community/tree/master/contributors/devel" target="_blank" rel="external">https://github.com/kubernetes/community/tree/master/contributors/devel</a><br>[3] Bug - <a href="https://github.com/kubernetes/community/issues" target="_blank" rel="external">https://github.com/kubernetes/community/issues</a><br>[4] Submit code review - <a href="https://github.com/kubernetes/community/blob/master/contributors/guide/pull-requests.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/contributors/guide/pull-requests.md</a><br>[5] Membership - <a href="https://github.com/kubernetes/community/blob/master/community-membership.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/community-membership.md</a><br>[6] CONTRIBUTING - <a href="https://github.com/kubernetes/community/blob/master/CONTRIBUTING.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/CONTRIBUTING.md</a><br>[7] Code format - <a href="https://github.com/golang/go/wiki/CodeReviewComments" target="_blank" rel="external">https://github.com/golang/go/wiki/CodeReviewComments</a><br>[8] Slack - <a href="https://kubernetes.slack.com/messages" target="_blank" rel="external">https://kubernetes.slack.com/messages</a><br>[9] Mail-list - <a href="https://groups.google.com/forum/#!forum/kubernetes-dev" target="_blank" rel="external">https://groups.google.com/forum/#!forum/kubernetes-dev</a><br>[10] SIG-list (Special Interest Groups) - <a href="https://github.com/kubernetes/community/blob/master/sig-list.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/sig-list.md</a><br>[11] open-bug - <a href="https://github.com/kubernetes/community/blob/master/contributors/guide/issue-triage.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/contributors/guide/issue-triage.md</a><br>[12] BP - <a href="https://github.com/kubernetes/community/tree/master/contributors/design-proposals" target="_blank" rel="external">https://github.com/kubernetes/community/tree/master/contributors/design-proposals</a><br>[13] <a href="https://github.com/kubernetes/community/blob/master/community-membership.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/community-membership.md</a><br>[14] test - <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/testing.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/contributors/devel/testing.md</a><br>[15] <a href="https://ress.infoq.com/minibooks/Kubernetes-handbook/zh/pdf/kubernetes.pdf" target="_blank" rel="external">https://ress.infoq.com/minibooks/Kubernetes-handbook/zh/pdf/kubernetes.pdf</a><br>[16] <a href="https://kubernetes.io/docs/home/" target="_blank" rel="external">https://kubernetes.io/docs/home/</a><br>[17] <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way" target="_blank" rel="external">https://github.com/kelseyhightower/kubernetes-the-hard-way</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/10/温故OpenStack中的测试-by-Joshua/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/10/温故OpenStack中的测试-by-Joshua/" itemprop="url">温故OpenStack中的测试(by Joshua)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-10T16:28:15+08:00">
                2018-07-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (作者：张华 发表于：2018-03-15)</strong></p>
<ol>
<li><p>沿用tox调用virtualenv自动创建的虚拟环境(virtualenv -p python3.5 .tox/py35)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source .tox/py35/bin/activate</span><br><span class="line">sudo pip install --upgrade -r requirements.txt</span><br><span class="line">sudo pip install --upgrade -r test-requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用unittest和nose运行测试。nose是对unittest的扩展，使得python的测试更加简单，nose自动发现测试代码并执行，nose提供了大量的插件，比如覆盖报表等。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m unittest -v unit_tests.test_neutron_utils.TestNeutronUtils.test_get_packages_ovs_newton</span><br><span class="line">.tox/py35/bin/python nosetests -v unit_tests/test_neutron_utils.py:TestNeutronUtils.test_get_packages_ovs_newton</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>注意：上面采用nosetests运行时会报错，因为我们的测试采用了python3, 所以需要在安装了python3-nose之后（sudo apt-get install python3-nose python3-mock）再采用下列三种方式之一运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nosetests3 -v unit_tests/test_neutron_utils.py:TestNeutronUtils.test_restart_map_ovs_odl</span><br><span class="line">/bak/work/charms/neutron-gateway/.tox/py35/bin/python /usr/local/bin/nosetests -v unit_tests/test_neutron_utils.py:TestNeutronUtils.test_get_packages_ovs_newton</span><br><span class="line">python -m nose unit_tests/test_neutron_utils.py:TestNeutronUtils.test_restart_map_ovs_odl</span><br></pre></td></tr></table></figure></p>
<p>但实际上仍然找不找nose模块，那是因为nose与virtualenv结合地不大好，在这个网页找着了答案(<a href="https://stackoverflow.com/questions/864956/problems-using-nose-in-a-virtualenv" target="_blank" rel="external">https://stackoverflow.com/questions/864956/problems-using-nose-in-a-virtualenv</a>) - You need to have a copy of nose installed in the virtual environment. In order to force installation of nose into the virtualenv, even though it is already installed in the global site-packages, run pip install with the -I flag: pip install nose -I</p>
<ol>
<li><p>上面使用unittest与nose运行测试的方式只是将结果输出到stdout，不便于分析。所以可以使用python-subunit模块来运行测试，并将测试结果通过subunit协议输出到文件中便于日后分析。因为subunit是基于二进制的不便于人眼看，所以可使用subunit2pyunit工具将其人类可读化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python -m subunit.run discover |subunit2pyunit</span><br><span class="line">python -m subunit.run discover -t ./ ./unit_tests |subunit2pyunit</span><br><span class="line">python -m subunit.run unit_tests.test_neutron_utils.TestNeutronUtils. |subunit2pyunit</span><br></pre></td></tr></table></figure>
</li>
<li><p>在大型应用中分析测试结果很重要，testrepository可以调用subunit来用python-subunit模块来运行测试，并将测试结果通过subunit协议输出到文件中，然后testrepository在些基础上有更多的分析，如分析哪些用例运行的时间最长，如显示失败的用例，如仅运行上次运行失败的用例。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">testr init</span><br><span class="line">testr run</span><br><span class="line">testr run --parallel</span><br><span class="line">$ cat .testr.conf</span><br><span class="line">[DEFAULT]</span><br><span class="line">test_command=OS_STDOUT_CAPTURE=$&#123;OS_STDOUT_CAPTURE:-1&#125; \</span><br><span class="line">             OS_STDERR_CAPTURE=$&#123;OS_STDERR_CAPTURE:-1&#125; \</span><br><span class="line">             OS_TEST_TIMEOUT=$&#123;OS_TEST_TIMEOUT:-60&#125; \</span><br><span class="line">             $&#123;PYTHON:-python&#125; -m subunit.run discover -t ./ ./unit_tests $LISTOPT $IDOPTION</span><br><span class="line">test_id_option=--load-list $IDFILE</span><br><span class="line">test_list_option=--list</span><br></pre></td></tr></table></figure>
</li>
<li><p>tox用于创建虚拟python环境，也可以集成上面的testrepository(commands = ostestr {posargs})</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$ cat tox.ini</span><br><span class="line">[tox]</span><br><span class="line">envlist = pep8,py27,py35</span><br><span class="line">skipsdist = True</span><br><span class="line"></span><br><span class="line">[testenv]</span><br><span class="line">setenv = VIRTUAL_ENV=&#123;envdir&#125;</span><br><span class="line">         PYTHONHASHSEED=0</span><br><span class="line">         CHARM_DIR=&#123;envdir&#125;</span><br><span class="line">         AMULET_SETUP_TIMEOUT=5400</span><br><span class="line">install_command =</span><br><span class="line">  pip install --allow-unverified python-apt &#123;opts&#125; &#123;packages&#125;</span><br><span class="line">commands = ostestr &#123;posargs&#125;</span><br><span class="line">whitelist_externals = juju</span><br><span class="line">passenv = HOME TERM AMULET_* CS_API_*</span><br><span class="line"></span><br><span class="line">[testenv:py27]</span><br><span class="line">basepython = python2.7</span><br><span class="line">deps = -r&#123;toxinidir&#125;/requirements.txt</span><br><span class="line">       -r&#123;toxinidir&#125;/test-requirements.txt</span><br><span class="line">commands = /bin/true</span><br><span class="line"></span><br><span class="line">[testenv:py35]</span><br><span class="line">basepython = python3.5</span><br><span class="line">deps = -r&#123;toxinidir&#125;/requirements.txt</span><br><span class="line">       -r&#123;toxinidir&#125;/test-requirements.txt</span><br><span class="line"></span><br><span class="line">[testenv:pep8]</span><br><span class="line">basepython = python2.7</span><br><span class="line">deps = -r&#123;toxinidir&#125;/requirements.txt</span><br><span class="line">       -r&#123;toxinidir&#125;/test-requirements.txt</span><br><span class="line">commands = flake8 &#123;posargs&#125; hooks unit_tests tests actions lib</span><br><span class="line">           charm-proof</span><br><span class="line"></span><br><span class="line">[flake8]</span><br><span class="line">ignore = E402,E226</span><br><span class="line">exclude = */helpers</span><br></pre></td></tr></table></figure>
</li>
<li><p>pydev使用virtualenv中的py35</p>
<p>在eclipse的”Preferences -&gt; Pydev -&gt; Interpreters -&gt; Python Interpreters”菜单中定义python35=/bak/work/charms/neutron-gateway/.tox/py35/bin/python,然后在工程上点右键从”Properties -&gt; Pydev - Interpreter/Grammar”定义使用python35。注意，需要将/bak/work/charms/neutron-gateway/.tox/py35/lib/python3.5/site-packages也选到环境变量中，否则后面会报ImportError: No module named ‘mock。<br>为一个测试类定义”Python unitest”类型的”Debug Configurations”, 也在其Interpreter选项卡中定义使用python35 (结果：eclipse似乎有bug，此处选择了python35后无法保存)<br>所以无法成功，似乎是pydev与python3协作不大好。最后还是pudb好使(sudo pip install pudb, import pudb; pdb.set_trace())</p>
</li>
<li><p>py27下的测试运行方法(如openstack), charm似乎只能用py36 (tox -r -epy36 &amp;&amp; tox -e py36).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cat tox.ini</span><br><span class="line">tox -r -epy27</span><br><span class="line">tox -e py27,pep8</span><br><span class="line">tox -e py27 neutron.tests.unit.agent.linux.test_keepalived</span><br><span class="line">tox -e py27 neutron.tests.unit.agent.linux.test_keepalived.KeepalivedInstanceTestCase.test_remove_addresses_by_interface</span><br><span class="line">mkdir /opt/stack &amp;&amp; sudo chown -R hua /opt/stack</span><br><span class="line">tox -e functional neutron.tests.functional.agent.l3.test_ha_router.L3HATestCase.test_keepalived_configuration</span><br><span class="line">tox -e dsvm-fullstack</span><br><span class="line">source .tox/functional/bin/activate</span><br><span class="line">.tox/functional/bin/pip install -r requirements.txt</span><br><span class="line">.tox/functional/bin/pip install -r test-requirements.txt</span><br><span class="line">.tox/functional/bin/pip install -r neutron/tests/functional/requirements.txt</span><br><span class="line">.tox/functional/bin/pip freeze |grep neutron</span><br><span class="line">.tox/functional/bin/pip install &apos;neutron-lib==1.13.0&apos;</span><br><span class="line">OS_SUDO_TESTING=True .tox/functional/bin/python -m unittest -v neutron.tests.functional.agent.l3.test_ha_router.L3HATestCase.test_keepalived_configuration</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>有时如mitaka已经eol了, 例如它的origain-stable-mitaka这个分支都没有了, 这会导致运行’tox -r -epy27 ‘不成功, 那么可以手工执行:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">virtualenv venv_mitaka</span><br><span class="line">. venv_mitaka/bin/activate</span><br><span class="line"></span><br><span class="line"># pg_config executable not found</span><br><span class="line"># Error: could not determine PostgreSQL version from &apos;10.5&apos;</span><br><span class="line">sudo apt install python-setuptools python-dev libpython-dev libssl-dev python-pip libmysqlclient-dev libxml2-dev libxslt-dev libxslt1-dev libpq-dev git git-review libffi-dev gettext graphviz libjpeg-dev zlib1g-dev build-essential python-nose python-mock libssl1.0</span><br><span class="line">sudo apt install python3.6 python3.6-dev python3-pip python3-dev python3-nose python3-mock</span><br><span class="line">#sudo pip install --upgrade setuptools</span><br><span class="line">#sudo pip3 install --upgrade setuptools</span><br><span class="line">#sudo pip install --upgrade --force-reinstall pip virtualenv</span><br><span class="line"></span><br><span class="line"># Failed to install Cryptography - sudo apt install libssl1.0</span><br><span class="line"># No module named dulwich - ./venv/bin/pip install dulwich</span><br><span class="line"># unittest has no attribute &apos;virt&apos; -</span><br><span class="line"># ./venv_ocata/bin/pip install -c https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt?h=stable/ocata .</span><br><span class="line">./venv_mitaka/bin/pip install -c https://git.openstack.org/cgit/openstack/requirements/plain/upper-constraints.txt?h=mitaka-eol .</span><br><span class="line">./venv_mitaka/bin/pip install -r requirements.txt</span><br><span class="line">./venv_mitaka/bin/pip install -r test-requirements.txt</span><br><span class="line"></span><br><span class="line">find . -name &quot;*.pyc&quot; -exec rm -rf &#123;&#125; \;</span><br><span class="line"></span><br><span class="line">git clone https://github.com/openstack/oslo.cache.git</span><br><span class="line">cd oslo.cache &amp;&amp; git checkout -b mitaka mitaka-eol</span><br><span class="line">../nova/venv/bin/pip install -r requirements.txt</span><br><span class="line">../nova/venv/bin/pip install -r test-requirements.txt</span><br><span class="line"></span><br><span class="line"># dogpile can&apos;t cannot import name threading - venv/bin/pip uninstall dogpile.cache dogpile &amp;&amp; venv/bin/pip install dogpile.cache</span><br><span class="line">venv_mitaka/bin/python -m subunit.run discover ./nova/tests/unit/compute |subunit2pyunit</span><br><span class="line">venv_mitaka/bin/python -m subunit.run nova.tests.unit.compute.test_compute |subunit2pyunit</span><br><span class="line">venv_mitaka/bin/python -m subunit.run nova.tests.unit.compute.test_compute.ComputeAPITestCase.test_attach_volume |subunit2pyunit</span><br><span class="line">venv_mitaka/bin/python -m unittest -v nova.tests.unit.compute.test_compute  #make sure the package nova.tests.virt exists</span><br><span class="line"></span><br><span class="line">venv_mitaka/bin/python -m unittest -v nova.tests.unit.virt.libvirt.test_driver.LibvirtConnTestCase.test_check_can_live_migrate_dest_all_pass_with_over_commit</span><br><span class="line"></span><br><span class="line">#venv/bin/pep8</span><br><span class="line">venv/bin/flake8</span><br></pre></td></tr></table></figure></p>
<p>对于horizion的测试:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://docs.openstack.org/horizon/latest/contributor/testing.html</span><br><span class="line">tox</span><br><span class="line">tox -e py27 -- openstack_dashboard.test.views:DashboardViewsTest.test_urls_ngdetails</span><br></pre></td></tr></table></figure></p>
<p>charm的unit test:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># charm unit test</span><br><span class="line">sudo pip install virtualenv</span><br><span class="line">sudo pip install --upgrade pip</span><br><span class="line">proxychains tox -r -epy36</span><br><span class="line">source .tox/py36/bin/activate</span><br><span class="line">python -m subunit.run discover -t ./ ./unit_tests |subunit2pyunit</span><br><span class="line">python -m subunit.run unit_tests.test_neutron_ovs_context.L3AgentContextTest |subunit2pyunit</span><br><span class="line">python -m subunit.run unit_tests.test_neutron_ovs_context.L3AgentContextTest.test_dvr_enabled |subunit2pyunit</span><br><span class="line">OS_SUDO_TESTING=True .tox/py36/bin/python -m unittest -v unit_tests.test_neutron_ovs_context.L3AgentContextTest.test_dvr_enabled</span><br></pre></td></tr></table></figure>
<h2 id="20200714更新"><a href="#20200714更新" class="headerlink" title="20200714更新"></a>20200714更新</h2><p>今天，发现上面的subunit的方法不work了，　例如运行下面三行报这个错＂TypeError: test_data_port_name() missing 1 required positional argument: ‘config’＂<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tox -r -epy38</span><br><span class="line">source .tox/py38/bin/activate</span><br><span class="line">python -m subunit.run unit_tests.test_neutron_ovs_context.OVSPluginContextTest.test_data_port_name</span><br></pre></td></tr></table></figure></p>
<p>估计是现在全面移到python3了，subunit和python3搭配不大好吧，　试了一下，　下面两种方法还可以用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">OS_SUDO_TESTING=True .tox/py3/bin/python -m unittest -v subunit.run unit_tests.test_neutron_ovs_context.OVSPluginContextTest.test_data_port_name</span><br><span class="line">nosetests3 unit_tests/test_neutron_ovs_context.py:OVSPluginContextTest.test_data_port_name</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">张华</p>
              <p class="site-description motion-element" itemprop="description">blog.csdn.net/quqi99</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">96</span>
                  <span class="site-state-item-name">Artikel</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张华</span>

  
</div>


  <div class="powered-by">Erstellt mit  <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>

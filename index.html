<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>技术并艺术着</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="blog.csdn.net/quqi99">
<meta property="og:type" content="website">
<meta property="og:title" content="技术并艺术着">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="技术并艺术着">
<meta property="og:description" content="blog.csdn.net/quqi99">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="技术并艺术着">
<meta name="twitter:description" content="blog.csdn.net/quqi99">
  
    <link rel="alternate" href="/atom.xml" title="技术并艺术着" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">技术并艺术着</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">张华的技术博客 - blog.csdn.net/quqi99</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Use-Octavia-to-Implement-HTTPS-Health-Monitors" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/31/Use-Octavia-to-Implement-HTTPS-Health-Monitors/" class="article-date">
  <time datetime="2018-12-31T08:29:50.000Z" itemprop="datePublished">2018-12-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/31/Use-Octavia-to-Implement-HTTPS-Health-Monitors/">Use Octavia to Implement HTTPS Health Monitors</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>采用Neutron LBaaS v2实现HTTPS Health Monitors时的配置如下(步骤见附件 - Neutron LBaaS v2)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">backend 52112201-05ce-4f4d-b5a8-9e67de2a895a</span><br><span class="line">    mode tcp</span><br><span class="line">    balance leastconn</span><br><span class="line">    timeout check 10s</span><br><span class="line">    option httpchk GET /</span><br><span class="line">    http-check expect rstatus 200</span><br><span class="line">    option ssl-hello-chk</span><br><span class="line">    server 37a1f5a8-ec7e-4208-9c96-27d2783a594f 192.168.21.13:443 weight 1 check inter 5s fall 2</span><br><span class="line">    server 8e722b4b-08b8-4089-bba5-8fa5dd26a87f 192.168.21.8:443 weight 1 check inter 5s fall 2</span><br></pre></td></tr></table></figure></p>
<p>这种配置会有一个问题, 当使用自定义签名证书时一切正常, 但使用机构颁发的证书时反而有问题. 原因在这网页(<a href="https://docs.openstack.org/octavia/latest/user/guides/basic-cookbook.html)其实已经有说明" target="_blank" rel="external">https://docs.openstack.org/octavia/latest/user/guides/basic-cookbook.html)其实已经有说明</a>, 如下:<br>HTTPS health monitors operate exactly like HTTP health monitors, but with ssl back-end servers. Unfortunately, this causes problems if the servers are performing client certificate validation, as HAProxy won’t have a valid cert. In this case, using TLS-HELLO type monitoring is an alternative.<br>TLS-HELLO health monitors simply ensure the back-end server responds to SSLv3 client hello messages. It will not check any other health metrics, like status code or body contents.</p>
<p>解决此问题有两种方法:<br>1, 当有”option httpchk”项时, 下面两个网页提出一种方法添加”check check-ssl verify none”参数禁止对客户端参数进行验证.<br><a href="https://stackoverflow.com/questions/16719388/haproxy-https-health-checks" target="_blank" rel="external">https://stackoverflow.com/questions/16719388/haproxy-https-health-checks</a><br><a href="https://serverfault.com/questions/924477/haproxy-health-check-for-https-backend" target="_blank" rel="external">https://serverfault.com/questions/924477/haproxy-health-check-for-https-backend</a><br>2, 或者去掉”option httpchk”项, 采用TLS-HELLO模式.<br>由于LBaaS v2无法对实现参数定制实现上述两种解决方案, 所以需要采用Octavia.</p>
<h2 id="安装Octavia"><a href="#安装Octavia" class="headerlink" title="安装Octavia"></a>安装Octavia</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">juju add-model bionic-barbican-octavia</span><br><span class="line">./generate-bundle.sh --series bionic --barbican</span><br><span class="line">#./generate-bundle.sh --series bionic --release rocky --barbican</span><br><span class="line">juju deploy ./b/openstack.yaml --overlay ./b/o/barbican.yaml</span><br><span class="line">#https://github.com/openstack-charmers/openstack-bundles/blob/master/stable/overlays/loadbalancer-octavia.yaml</span><br><span class="line">#NOTE: need to comment to:lxd related lines from loadbalancer-octavia.yaml, and change nova-compute num to 3</span><br><span class="line">juju deploy ./b/openstack.yaml --overlay ./overlays/loadbalancer-octavia.yaml</span><br><span class="line"></span><br><span class="line"># Or we can:</span><br><span class="line"># 2018-12-25 03:30:39 DEBUG update-status fatal error: runtime: out of memory</span><br><span class="line">juju deploy octavia --config openstack-origin=cloud:bionic:queens --constraints mem=4G</span><br><span class="line">juju deploy octavia-dashboard</span><br><span class="line">juju add-relation octavia-dashboard openstack-dashboard</span><br><span class="line">juju add-relation octavia rabbitmq-server</span><br><span class="line">juju add-relation octavia mysql</span><br><span class="line">juju add-relation octavia keystone</span><br><span class="line">juju add-relation octavia neutron-openvswitch</span><br><span class="line">juju add-relation octavia neutron-api</span><br><span class="line"></span><br><span class="line"># Initialize and unseal vault</span><br><span class="line"># https://docs.openstack.org/project-deploy-guide/charm-deployment-guide/latest/app-vault.html</span><br><span class="line"># https://lingxiankong.github.io/2018-07-16-barbican-introduction.html</span><br><span class="line"># /snap/vault/1315/bin/vault server -config /var/snap/vault/common/vault.hcl</span><br><span class="line">sudo snap install vault</span><br><span class="line">export VAULT_ADDR=&quot;http://$(juju run --unit vault/0 unit-get private-address):8200&quot;</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ vault operator init -key-shares=5 -key-threshold=3</span><br><span class="line">Unseal Key 1: UB7XDri5FRcMLirKBIysdUb2PN7Ia5EVMP0Z9wD9Hyll</span><br><span class="line">Unseal Key 2: mD8Gnr3hdB2LjjNB4ugxvvsvb8+EQQ/0AXm2p+c2qYFT</span><br><span class="line">Unseal Key 3: vymYLAdou3qky24IEKDufYsZXAIPLWtErAKy/RkfgghS</span><br><span class="line">Unseal Key 4: xOwDbqgNLLipsZbp+FAmVhBc3ZxA8CI3DchRc4AClRyQ</span><br><span class="line">Unseal Key 5: nRlZ8WX6CS9nOw2ct5U9o0Za5jlUAtjN/6XLxjf62CnR</span><br><span class="line">Initial Root Token: s.VJKGhNvIFCTgHVbQ6WvL0OLe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vault operator unseal UB7XDri5FRcMLirKBIysdUb2PN7Ia5EVMP0Z9wD9Hyll</span><br><span class="line">vault operator unseal mD8Gnr3hdB2LjjNB4ugxvvsvb8+EQQ/0AXm2p+c2qYFT</span><br><span class="line">vault operator unseal vymYLAdou3qky24IEKDufYsZXAIPLWtErAKy/RkfgghS</span><br><span class="line">export VAULT_TOKEN=s.VJKGhNvIFCTgHVbQ6WvL0OLe</span><br><span class="line">vault token create -ttl=10m</span><br><span class="line">$ vault token create -ttl=10m</span><br><span class="line">Key                  Value</span><br><span class="line">---                  -----</span><br><span class="line">token                s.7ToXh9HqE6FiiJZybFhevL9v</span><br><span class="line">token_accessor       6dPkFpsPmx4D7g8yNJXvEpKN</span><br><span class="line">token_duration       10m</span><br><span class="line">token_renewable      true</span><br><span class="line">token_policies       [&quot;root&quot;]</span><br><span class="line">identity_policies    []</span><br><span class="line">policies             [&quot;root&quot;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Authorize vault charm to use a root token to be able to create secrets storage back-ends and roles to allow other app to access vault</span><br><span class="line">juju run-action vault/0 authorize-charm token=s.7ToXh9HqE6FiiJZybFhevL9v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># upload Amphora image</span><br><span class="line">source ~/stsstack-bundles/openstack/novarc</span><br><span class="line">http_proxy=http://squid.internal:3128 wget http://tarballs.openstack.org/octavia/test-images/test-only-amphora-x64-haproxy-ubuntu-xenial.qcow2</span><br><span class="line">#openstack image create --tag octavia-amphora --disk-format=qcow2 --container-format=bare --private amphora-haproxy-xenial --file ./test-only-amphora-x64-haproxy-ubuntu-xenial.qcow2</span><br><span class="line">glance image-create --tag octavia-amphora --disk-format qcow2 --name amphora-haproxy-xenial --file ./test-only-amphora-x64-haproxy-ubuntu-xenial.qcow2 --visibility public --container-format bare --progress</span><br><span class="line"></span><br><span class="line">cd stsstack-bundles/openstack/</span><br><span class="line">./configure</span><br><span class="line">./tools/sec_groups.sh</span><br><span class="line">./tools/instance_launch.sh 2 xenial</span><br><span class="line">neutron floatingip-create ext_net</span><br><span class="line">neutron floatingip-associate $(neutron floatingip-list |grep 10.5.150.4 |awk &apos;&#123;print $2&#125;&apos;) $(neutron port-list |grep &apos;192.168.21.3&apos; |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line"></span><br><span class="line">cd ~/ca  #https://docs.openstack.org/project-deploy-guide/charm-deployment-guide/latest/app-octavia.html</span><br><span class="line">juju config octavia \</span><br><span class="line">    lb-mgmt-issuing-cacert=&quot;$(base64 controller_ca.pem)&quot; \</span><br><span class="line">    lb-mgmt-issuing-ca-private-key=&quot;$(base64 controller_ca_key.pem)&quot; \</span><br><span class="line">    lb-mgmt-issuing-ca-key-passphrase=foobar \</span><br><span class="line">    lb-mgmt-controller-cacert=&quot;$(base64 controller_ca.pem)&quot; \</span><br><span class="line">    lb-mgmt-controller-cert=&quot;$(base64 controller_cert_bundle.pem)&quot;</span><br></pre></td></tr></table></figure>
<p>配置资源:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># the code search &apos;configure_resources&apos;</span><br><span class="line">juju config octavia create-mgmt-network</span><br><span class="line">juju run-action --wait octavia/0 configure-resources</span><br><span class="line"></span><br><span class="line"># some deubg ways:</span><br><span class="line">openstack security group rule create $(openstack security group show lb-mgmt-sec-grp -f value -c id) --protocol udp --dst-port 546 --ethertype IPv6</span><br><span class="line">openstack security group rule create $(openstack security group show lb-mgmt-sec-grp -f value -c id) --protocol icmp --ethertype IPv6</span><br><span class="line">neutron security-group-rule-create --protocol icmpv6 --direction egress --ethertype IPv6 lb-mgmt-sec-grp</span><br><span class="line">neutron security-group-rule-create --protocol icmpv6 --direction ingress --ethertype IPv6 lb-mgmt-sec-grp</span><br><span class="line"></span><br><span class="line">neutron port-show octavia-health-manager-octavia-0-listen-port -f value -c status</span><br><span class="line">neutron port-update --admin-state-up True octavia-health-manager-octavia-0-listen-port</span><br><span class="line">AGENT=$(neutron l3-agent-list-hosting-router lb-mgmt -f value -c id)</span><br><span class="line">neutron l3-agent-router-remove $AGENT lb-mgmt</span><br><span class="line">neutron l3-agent-router-add $AGENT lb-mgmt</span><br></pre></td></tr></table></figure></p>
<p>上面configure-resources命令 (juju run-action –wait octavia/0 configure-resources)将会自动配置IPv6管理网段, 并且会配置一个binding:host在octavia/0节点上的名为octavia-health-manager-octavia-0-listen-port的port.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@zhhuabj-bastion:~$ neutron router-list |grep mgmt</span><br><span class="line">| 0a839377-6b19-419b-9868-616def4d749f | lb-mgmt         | null                                                                                                                                                                                    | False       | False |</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ neutron net-list |grep mgmt</span><br><span class="line">| ae580dc8-31d6-4ec3-9d44-4a9c7b9e80b6 | lb-mgmt-net | ea9c7d5c-d224-4dd3-b40c-3acae9690657 fc00:4a9c:7b9e:80b6::/64 |</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ neutron subnet-list |grep mgmt</span><br><span class="line">| ea9c7d5c-d224-4dd3-b40c-3acae9690657 | lb-mgmt-subnetv6 | fc00:4a9c:7b9e:80b6::/64 | &#123;&quot;start&quot;: &quot;fc00:4a9c:7b9e:80b6::2&quot;, &quot;end&quot;: &quot;fc00:4a9c:7b9e:80b6:ffff:ffff:ffff:ffff&quot;&#125; |</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ neutron port-list |grep fc00</span><br><span class="line">| 5cb6e3f3-ebe5-4284-9c05-ea272e8e599b |                                                      | fa:16:3e:9e:82:6a | &#123;&quot;subnet_id&quot;: &quot;ea9c7d5c-d224-4dd3-b40c-3acae9690657&quot;, &quot;ip_address&quot;: &quot;fc00:4a9c:7b9e:80b6::1&quot;&#125;                  |</span><br><span class="line">| 983c56d2-46dd-416c-abc8-5096d76f75e2 | octavia-health-manager-octavia-0-listen-port         | fa:16:3e:99:8c:ab | &#123;&quot;subnet_id&quot;: &quot;ea9c7d5c-d224-4dd3-b40c-3acae9690657&quot;, &quot;ip_address&quot;: &quot;fc00:4a9c:7b9e:80b6:f816:3eff:fe99:8cab&quot;&#125; |</span><br><span class="line">| af38a60d-a370-4ddb-80ac-517fda175535 |                                                      | fa:16:3e:5f:cd:ae | &#123;&quot;subnet_id&quot;: &quot;ea9c7d5c-d224-4dd3-b40c-3acae9690657&quot;, &quot;ip_address&quot;: &quot;fc00:4a9c:7b9e:80b6:f816:3eff:fe5f:cdae&quot;&#125; |</span><br><span class="line">| b65f90d1-2e1f-4994-a0e9-2bb13ead4cab |                                                      | fa:16:3e:10:34:84 | &#123;&quot;subnet_id&quot;: &quot;ea9c7d5c-d224-4dd3-b40c-3acae9690657&quot;, &quot;ip_address&quot;: &quot;fc00:4a9c:7b9e:80b6:f816:3eff:fe10:3484&quot;&#125; |</span><br></pre></td></tr></table></figure></p>
<p>并且在octavia/0上会创建一个名为o-hm0的接口, 此接口的IP地址与octavia-health-manager-octavia-0-listen-port port同.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@zhhuabj-bastion:~$ juju ssh octavia/0 -- ip addr show o-hm0 |grep global</span><br><span class="line">Connection to 10.5.0.110 closed.</span><br><span class="line">    inet6 fc00:4a9c:7b9e:80b6:f816:3eff:fe99:8cab/64 scope global dynamic mngtmpaddr noprefixroute</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ juju ssh octavia/0 -- sudo ovs-vsctl show</span><br><span class="line">490bbb36-1c7d-412d-8b44-31e6f796306a</span><br><span class="line">    Manager &quot;ptcp:6640:127.0.0.1&quot;</span><br><span class="line">        is_connected: true</span><br><span class="line">    Bridge br-tun</span><br><span class="line">        Controller &quot;tcp:127.0.0.1:6633&quot;</span><br><span class="line">            is_connected: true</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port &quot;gre-0a05006b&quot;</span><br><span class="line">            Interface &quot;gre-0a05006b&quot;</span><br><span class="line">                type: gre</span><br><span class="line">                options: &#123;df_default=&quot;true&quot;, in_key=flow, local_ip=&quot;10.5.0.110&quot;, out_key=flow, remote_ip=&quot;10.5.0.107&quot;&#125;</span><br><span class="line">        Port br-tun</span><br><span class="line">            Interface br-tun</span><br><span class="line">                type: internal</span><br><span class="line">        Port patch-int</span><br><span class="line">            Interface patch-int</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=patch-tun&#125;</span><br><span class="line">        Port &quot;gre-0a050016&quot;</span><br><span class="line">            Interface &quot;gre-0a050016&quot;</span><br><span class="line">                type: gre</span><br><span class="line">                options: &#123;df_default=&quot;true&quot;, in_key=flow, local_ip=&quot;10.5.0.110&quot;, out_key=flow, remote_ip=&quot;10.5.0.22&quot;&#125;</span><br><span class="line">    Bridge br-data</span><br><span class="line">        Controller &quot;tcp:127.0.0.1:6633&quot;</span><br><span class="line">            is_connected: true</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port phy-br-data</span><br><span class="line">            Interface phy-br-data</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=int-br-data&#125;</span><br><span class="line">        Port br-data</span><br><span class="line">            Interface br-data</span><br><span class="line">                type: internal</span><br><span class="line">    Bridge br-int</span><br><span class="line">        Controller &quot;tcp:127.0.0.1:6633&quot;</span><br><span class="line">            is_connected: true</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port patch-tun</span><br><span class="line">            Interface patch-tun</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=patch-int&#125;</span><br><span class="line">        Port &quot;o-hm0&quot;</span><br><span class="line">            tag: 1</span><br><span class="line">            Interface &quot;o-hm0&quot;</span><br><span class="line">                type: internal</span><br><span class="line">        Port br-int</span><br><span class="line">            Interface br-int</span><br><span class="line">                type: internal</span><br><span class="line">        Port int-br-data</span><br><span class="line">            Interface int-br-data</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=phy-br-data&#125;</span><br><span class="line">    Bridge br-ex</span><br><span class="line">        Port br-ex</span><br><span class="line">            Interface br-ex</span><br><span class="line">                type: internal</span><br><span class="line">    ovs_version: &quot;2.10.0&quot;</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ juju ssh neutron-gateway/0 -- sudo ovs-vsctl show</span><br><span class="line">ec3e2cb6-5261-4c22-8afd-5bacb0e8ce85</span><br><span class="line">    Manager &quot;ptcp:6640:127.0.0.1&quot;</span><br><span class="line">        is_connected: true</span><br><span class="line">    Bridge br-ex</span><br><span class="line">        Port br-ex</span><br><span class="line">            Interface br-ex</span><br><span class="line">                type: internal</span><br><span class="line">    Bridge br-int</span><br><span class="line">        Controller &quot;tcp:127.0.0.1:6633&quot;</span><br><span class="line">            is_connected: true</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port &quot;tap62c03d3b-b1&quot;</span><br><span class="line">            tag: 2</span><br><span class="line">            Interface &quot;tap62c03d3b-b1&quot;</span><br><span class="line">        Port &quot;tapb65f90d1-2e&quot;</span><br><span class="line">            tag: 3</span><br><span class="line">            Interface &quot;tapb65f90d1-2e&quot;</span><br><span class="line">        Port int-br-data</span><br><span class="line">            Interface int-br-data</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=phy-br-data&#125;</span><br><span class="line">        Port patch-tun</span><br><span class="line">            Interface patch-tun</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=patch-int&#125;</span><br><span class="line">        Port &quot;tap6f1478be-b1&quot;</span><br><span class="line">            tag: 1</span><br><span class="line">            Interface &quot;tap6f1478be-b1&quot;</span><br><span class="line">        Port &quot;tap01efd82b-53&quot;</span><br><span class="line">            tag: 2</span><br><span class="line">            Interface &quot;tap01efd82b-53&quot;</span><br><span class="line">        Port &quot;tap5cb6e3f3-eb&quot;</span><br><span class="line">            tag: 3</span><br><span class="line">            Interface &quot;tap5cb6e3f3-eb&quot;</span><br><span class="line">        Port br-int</span><br><span class="line">            Interface br-int</span><br><span class="line">                type: internal</span><br><span class="line">    Bridge br-data</span><br><span class="line">        Controller &quot;tcp:127.0.0.1:6633&quot;</span><br><span class="line">            is_connected: true</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port &quot;ens7&quot;</span><br><span class="line">            Interface &quot;ens7&quot;</span><br><span class="line">        Port br-data</span><br><span class="line">            Interface br-data</span><br><span class="line">                type: internal</span><br><span class="line">        Port phy-br-data</span><br><span class="line">            Interface phy-br-data</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=int-br-data&#125;</span><br><span class="line">    Bridge br-tun</span><br><span class="line">        Controller &quot;tcp:127.0.0.1:6633&quot;</span><br><span class="line">            is_connected: true</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port patch-int</span><br><span class="line">            Interface patch-int</span><br><span class="line">                type: patch</span><br><span class="line">                options: &#123;peer=patch-tun&#125;</span><br><span class="line">        Port &quot;gre-0a05007a&quot;</span><br><span class="line">            Interface &quot;gre-0a05007a&quot;</span><br><span class="line">                type: gre</span><br><span class="line">                options: &#123;df_default=&quot;true&quot;, in_key=flow, local_ip=&quot;10.5.0.22&quot;, out_key=flow, remote_ip=&quot;10.5.0.122&quot;&#125;</span><br><span class="line">        Port &quot;gre-0a05006b&quot;</span><br><span class="line">            Interface &quot;gre-0a05006b&quot;</span><br><span class="line">                type: gre</span><br><span class="line">                options: &#123;df_default=&quot;true&quot;, in_key=flow, local_ip=&quot;10.5.0.22&quot;, out_key=flow, remote_ip=&quot;10.5.0.107&quot;&#125;</span><br><span class="line">        Port br-tun</span><br><span class="line">            Interface br-tun</span><br><span class="line">                type: internal</span><br><span class="line">        Port &quot;gre-0a050079&quot;</span><br><span class="line">            Interface &quot;gre-0a050079&quot;</span><br><span class="line">                type: gre</span><br><span class="line">                options: &#123;df_default=&quot;true&quot;, in_key=flow, local_ip=&quot;10.5.0.22&quot;, out_key=flow, remote_ip=&quot;10.5.0.121&quot;&#125;</span><br><span class="line">        Port &quot;gre-0a05006e&quot;</span><br><span class="line">            Interface &quot;gre-0a05006e&quot;</span><br><span class="line">                type: gre</span><br><span class="line">                options: &#123;df_default=&quot;true&quot;, in_key=flow, local_ip=&quot;10.5.0.22&quot;, out_key=flow, remote_ip=&quot;10.5.0.110&quot;&#125;</span><br><span class="line">    ovs_version: &quot;2.10.0&quot;</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~⟫ juju ssh neutron-gateway/0 -- cat /var/lib/neutron/ra/0a839377-6b19-419b-9868-616def4d749f.radvd.conf</span><br><span class="line">interface qr-5cb6e3f3-eb</span><br><span class="line">&#123;</span><br><span class="line">   AdvSendAdvert on;</span><br><span class="line">   MinRtrAdvInterval 30;</span><br><span class="line">   MaxRtrAdvInterval 100;</span><br><span class="line">   AdvLinkMTU 1458;</span><br><span class="line">   prefix fc00:4a9c:7b9e:80b6::/64</span><br><span class="line">   &#123;</span><br><span class="line">        AdvOnLink on;</span><br><span class="line">        AdvAutonomous on;</span><br><span class="line">   &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ openstack security group rule list lb-health-mgr-sec-grp</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">| ID                                   | IP Protocol | IP Range | Port Range | Remote Security Group |</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">| 09a92cb2-9942-44d4-8a96-9449a6758967 | None        | None     |            | None                  |</span><br><span class="line">| 20daa06c-9de6-4c91-8a1e-59645f23953a | udp         | None     | 5555:5555  | None                  |</span><br><span class="line">| 8f7b9966-c255-4727-a172-60f22f0710f9 | None        | None     |            | None                  |</span><br><span class="line">| 90f86b27-12f8-4a9a-9924-37b31d26cbd8 | icmpv6      | None     |            | None                  |</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ openstack security group rule list lb-mgmt-sec-grp</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">| ID                                   | IP Protocol | IP Range | Port Range | Remote Security Group |</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line">| 54f79f92-a6c5-411d-a309-a02b39cc384b | icmpv6      | None     |            | None                  |</span><br><span class="line">| 574f595e-3d96-460e-a3f2-329818186492 | None        | None     |            | None                  |</span><br><span class="line">| 5ecb0f58-f5dd-4d52-bdfa-04fd56968bd8 | tcp         | None     | 22:22      | None                  |</span><br><span class="line">| 7ead3a3a-bc45-4434-b7a2-e2a6c0dc3ce9 | None        | None     |            | None                  |</span><br><span class="line">| cf82d108-e0f8-4916-95d4-0c816b6eb156 | tcp         | None     | 9443:9443  | None                  |</span><br><span class="line">+--------------------------------------+-------------+----------+------------+-----------------------+</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~$ source ~/novarc</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ openstack security group rule list default</span><br><span class="line">+--------------------------------------+-------------+-----------+------------+--------------------------------------+</span><br><span class="line">| ID                                   | IP Protocol | IP Range  | Port Range | Remote Security Group                |</span><br><span class="line">+--------------------------------------+-------------+-----------+------------+--------------------------------------+</span><br><span class="line">| 15b56abd-c2af-4c0a-8585-af68a8f09e3c | icmpv6      | None      |            | None                                 |</span><br><span class="line">| 2ad77fa3-32c7-4a20-a572-417bea782eff | icmp        | 0.0.0.0/0 |            | None                                 |</span><br><span class="line">| 2c2aec15-e4ad-4069-abd2-0191fe80f9bb | None        | None      |            | None                                 |</span><br><span class="line">| 3b775807-3c61-45a3-9677-aaf9631db677 | udp         | 0.0.0.0/0 | 3389:3389  | None                                 |</span><br><span class="line">| 3e9a6e7f-b9a2-47c9-97ca-042b22fbf308 | icmpv6      | None      |            | None                                 |</span><br><span class="line">| 42a3c09e-91c8-471d-b4a8-c1fe87dab066 | None        | None      |            | None                                 |</span><br><span class="line">| 47f9cec2-4bc0-4d71-9a02-3a27d46b59f8 | icmp        | None      |            | None                                 |</span><br><span class="line">| 94297175-9439-4df2-8c93-c5576e52e138 | udp         | None      | 546:546    | None                                 |</span><br><span class="line">| 9c6ac9d2-3b9e-4bab-a55a-04a1679b66be | None        | None      |            | c48a1bf5-7b7e-4337-afdf-8057ae8025af |</span><br><span class="line">| b6e95f76-1b64-4135-8b62-b058ec989f7e | None        | None      |            | c48a1bf5-7b7e-4337-afdf-8057ae8025af |</span><br><span class="line">| de5132a5-72e2-4f03-8b6a-dcbc2b7811c3 | tcp         | 0.0.0.0/0 | 3389:3389  | None                                 |</span><br><span class="line">| e72bea9f-84ce-4e3a-8597-c86d40b9b5ef | tcp         | 0.0.0.0/0 | 22:22      | None                                 |</span><br><span class="line">| ecf1415c-c6e9-4cf6-872c-4dac1353c014 | tcp         | 0.0.0.0/0 |            | None                                 |</span><br><span class="line">+--------------------------------------+-------------+-----------+------------+--------------------------------------+</span><br></pre></td></tr></table></figure></p>
<p>底层OpenStack环境(OpenStack Over Openstack)需要做 (见: <a href="https://blog.csdn.net/quqi99/article/details/78437988" target="_blank" rel="external">https://blog.csdn.net/quqi99/article/details/78437988</a> ):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack security group rule create $secgroup --protocol udp --dst-port 546 --ethertype IPv6</span><br></pre></td></tr></table></figure></p>
<p>最容易出现的问题是health-manager-octavia-0-listen-port port为DOWN, 从而o-hm0网络不通而无法从dhcp server处获得IP, 网段不通多半是br-int上的flow rules的问题, 我多次遇到这种情况, 但后来重建环境不知为什么又好了.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">root@juju-50fb86-bionic-rocky-barbican-octavia-11:~# ovs-ofctl dump-flows br-int</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=424018.932s, table=0, n_packets=978, n_bytes=76284, priority=10,icmp6,in_port=&quot;o-hm0&quot;,icmp_type=136 actions=resubmit(,24)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=424018.930s, table=0, n_packets=0, n_bytes=0, priority=10,arp,in_port=&quot;o-hm0&quot; actions=resubmit(,24)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=425788.219s, table=0, n_packets=0, n_bytes=0, priority=2,in_port=&quot;int-br-data&quot; actions=drop</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=424018.943s, table=0, n_packets=10939, n_bytes=2958167, priority=9,in_port=&quot;o-hm0&quot; actions=resubmit(,25)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=425788.898s, table=0, n_packets=10032, n_bytes=1608826, priority=0 actions=resubmit(,60)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=425788.903s, table=23, n_packets=0, n_bytes=0, priority=0 actions=drop</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=424018.940s, table=24, n_packets=675, n_bytes=52650, priority=2,icmp6,in_port=&quot;o-hm0&quot;,icmp_type=136,nd_target=fc00:4a9c:7b9e:80b6:f816:3eff:fe99:8cab actions=resubmit(,60)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=424018.938s, table=24, n_packets=0, n_bytes=0, priority=2,icmp6,in_port=&quot;o-hm0&quot;,icmp_type=136,nd_target=fe80::f816:3eff:fe99:8cab actions=resubmit(,60)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=425788.879s, table=24, n_packets=303, n_bytes=23634, priority=0 actions=drop</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=424018.951s, table=25, n_packets=10939, n_bytes=2958167, priority=2,in_port=&quot;o-hm0&quot;,dl_src=fa:16:3e:99:8c:ab actions=resubmit(,60)</span><br><span class="line"> cookie=0x5dc634635bd398eb, duration=425788.896s, table=60, n_packets=21647, n_bytes=4620009, priority=3 actions=NORMAL</span><br><span class="line"></span><br><span class="line">root@juju-50fb86-bionic-rocky-barbican-octavia-11:~# ovs-ofctl dump-flows br-data</span><br><span class="line"> cookie=0xb41c0c7781ded568, duration=426779.130s, table=0, n_packets=16816, n_bytes=3580386, priority=2,in_port=&quot;phy-br-data&quot; actions=drop</span><br><span class="line"> cookie=0xb41c0c7781ded568, duration=426779.201s, table=0, n_packets=0, n_bytes=0, priority=0 actions=NORMAL</span><br></pre></td></tr></table></figure></p>
<p>如果o-hm0总是无法获得IP, 我们也可以手工配置一个IPv4管理网段试试.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">neutron router-gateway-clear lb-mgmt</span><br><span class="line">neutron router-interface-delete lb-mgmt lb-mgmt-subnetv6</span><br><span class="line">neutron subnet-delete lb-mgmt-subnetv6</span><br><span class="line">neutron port-list |grep fc00</span><br><span class="line">#neutron port-delete 464e6d47-9830-4966-a2b7-e188c19c407a</span><br><span class="line">openstack subnet create --subnet-range 192.168.0.0/24 --allocation-pool start=192.168.0.2,end=192.168.0.200 --network lb-mgmt-net lb-mgmt-subnet</span><br><span class="line">neutron router-interface-add lb-mgmt lb-mgmt-subnet</span><br><span class="line">#neutron router-gateway-set lb-mgmt ext_net</span><br><span class="line">neutron port-list |grep 192.168.0.1</span><br><span class="line"></span><br><span class="line">#openstack security group create lb-mgmt-sec-grp --project $(openstack security group show lb-mgmt-sec-grp -f value -c project_id)</span><br><span class="line">openstack security group rule create --protocol udp --dst-port 5555 lb-mgmt-sec-grp</span><br><span class="line">openstack security group rule create --protocol tcp --dst-port 22 lb-mgmt-sec-grp</span><br><span class="line">openstack security group rule create --protocol tcp --dst-port 9443 lb-mgmt-sec-grp</span><br><span class="line">openstack security group rule create --protocol icmp lb-mgmt-sec-grp</span><br><span class="line">openstack security group show lb-mgmt-sec-grp</span><br><span class="line">openstack security group rule create --protocol udp --dst-port 5555 lb-health-mgr-sec-grp</span><br><span class="line">openstack security group rule create --protocol icmp lb-health-mgr-sec-grp</span><br><span class="line"></span><br><span class="line"># create a management port o-hm0 on octavia/0 node, first use neutron to allocate a port, then call ovs-vsctl to add-port</span><br><span class="line">LB_HOST=$(juju ssh octavia/0 -- hostname)</span><br><span class="line">juju ssh octavia/0 -- sudo ovs-vsctl del-port br-int o-hm0</span><br><span class="line"># Use LB_HOST to replace juju-70ea4e-bionic-barbican-octavia-11, don&apos;t know why it said &apos;bind failed&apos; when using $LB_HOST directly</span><br><span class="line">neutron port-create --name octavia-health-manager-octavia-0-listen-port --security-group $(openstack security group show lb-health-mgr-sec-grp -f value -c id) --device-owner Octavia:health-mgr --binding:host_id=juju-70ea4e-bionic-barbican-octavia-11 lb-mgmt-net --tenant-id $(openstack security group show lb-health-mgr-sec-grp -f value -c project_id)</span><br><span class="line"></span><br><span class="line">juju ssh octavia/0 -- sudo ovs-vsctl --may-exist add-port br-int o-hm0 -- set Interface o-hm0 type=internal -- set Interface o-hm0 external-ids:iface-status=active -- set Interface o-hm0 external-ids:attached-mac=$(neutron port-show octavia-health-manager-octavia-0-listen-port -f value -c mac_address) -- set Interface o-hm0 external-ids:iface-id=$(neutron port-show octavia-health-manager-octavia-0-listen-port -f value -c id)</span><br><span class="line">juju ssh octavia/0 -- sudo ip link set dev o-hm0 address $(neutron port-show octavia-health-manager-octavia-0-listen-port -f value -c mac_address)</span><br><span class="line">juju ssh octavia/0 -- sudo ip link set o-hm0 mtu 1458</span><br><span class="line">sudo mkdir -p /etc/octavia/dhcp</span><br><span class="line">sudo bash -c &apos;cat &gt;/etc/octavia/dhcp/dhclient.conf&apos; &lt;&lt;EOF</span><br><span class="line">request subnet-mask,broadcast-address,interface-mtu;</span><br><span class="line">do-forward-updates false;</span><br><span class="line">EOF</span><br><span class="line">#dhclient -v o-hm0 -cf /etc/octavia/dhcp/dhclient.conf</span><br><span class="line">ping 192.168.0.2</span><br></pre></td></tr></table></figure></p>
<h2 id="测试虚机中安装HTTPS测试服务"><a href="#测试虚机中安装HTTPS测试服务" class="headerlink" title="测试虚机中安装HTTPS测试服务"></a>测试虚机中安装HTTPS测试服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># Prepare CA and ssl pairs for lb server</span><br><span class="line">openssl genrsa -passout pass:password -out ca.key</span><br><span class="line">openssl req -x509 -passin pass:password -new -nodes -key ca.key -days 3650 -out ca.crt -subj &quot;/C=CN/ST=BJ/O=STS/CN=www.quqi.com&quot;</span><br><span class="line">openssl genrsa -passout pass:password -out lb.key</span><br><span class="line">openssl req -new -key lb.key -out lb.csr -subj &quot;/C=CN/ST=BJ/O=STS/CN=www.quqi.com&quot;</span><br><span class="line">openssl x509 -req -in lb.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out lb.crt -days 3650</span><br><span class="line">cat lb.crt lb.key &gt; lb.pem</span><br><span class="line">#openssl pkcs12 -export -inkey lb.key -in lb.crt -certfile ca.crt -passout pass:password -out lb.p12</span><br><span class="line"></span><br><span class="line"># Create two test servers and run</span><br><span class="line">sudo apt install python-minimal -y</span><br><span class="line">sudo bash -c &apos;cat &gt;simple-https-server.py&apos; &lt;&lt;EOF</span><br><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding=utf-8</span><br><span class="line">import BaseHTTPServer, SimpleHTTPServer</span><br><span class="line">import ssl</span><br><span class="line">httpd = BaseHTTPServer.HTTPServer((&apos;0.0.0.0&apos;, 443), SimpleHTTPServer.SimpleHTTPRequestHandler)</span><br><span class="line">httpd.socket = ssl.wrap_socket (httpd.socket, certfile=&apos;./lb.pem&apos;, server_side=True)</span><br><span class="line">httpd.serve_forever()</span><br><span class="line">EOF</span><br><span class="line">sudo bash -c &apos;cat &gt;index.html&apos; &lt;&lt;EOF</span><br><span class="line">test1</span><br><span class="line">EOF</span><br><span class="line">nohup sudo python simple-https-server.py &amp;</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ curl -k https://10.5.150.4</span><br><span class="line">test1</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ curl -k https://10.5.150.5</span><br><span class="line">test2</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ curl --cacert ~/ca/ca.crt https://10.5.150.4</span><br><span class="line">curl: (51) SSL: certificate subject name (www.quqi.com) does not match target host name &apos;10.5.150.4&apos;</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ curl --resolve www.quqi.com:443:10.5.150.4 --cacert ~/ca/ca.crt https://www.quqi.com</span><br><span class="line">test1</span><br><span class="line">ubuntu@zhhuabj-bastion:~$ curl --resolve www.quqi.com:443:10.5.150.4 -k https://www.quqi.com</span><br><span class="line">test1</span><br></pre></td></tr></table></figure>
<h2 id="How-to-ssh-into-amphora-service-vm"><a href="#How-to-ssh-into-amphora-service-vm" class="headerlink" title="How to ssh into amphora service vm"></a>How to ssh into amphora service vm</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /etc/octavia/.ssh &amp;&amp; sudo chown -R $(id -u):$(id -g) /etc/octavia/.ssh</span><br><span class="line">ssh-keygen -b 2048 -t rsa -N &quot;&quot; -f /etc/octavia/.ssh/octavia_ssh_key</span><br><span class="line">openstack user list --domain service_domain</span><br><span class="line"># NOTE: we must add &apos;--user&apos; option to avoid the error &apos;Invalid key_name provided&apos;</span><br><span class="line">nova keypair-add --pub-key=/etc/octavia/.ssh/octavia_ssh_key.pub octavia_ssh_key --user $(openstack user show octavia --domain service_domain -f value -c id)</span><br><span class="line">nova keypair-list --user $(openstack user show octavia --domain service_domain -f value -c id)</span><br><span class="line"></span><br><span class="line">vim /etc/octavia/octavia.conf</span><br><span class="line">vim /var/lib/juju/agents/unit-octavia-0/charm/templates/rocky/octavia.conf</span><br><span class="line">vim /usr/lib/python3/dist-packages/octavia/compute/drivers/nova_driver.py</span><br><span class="line">vim /usr/lib/python3/dist-packages/octavia/controller/worker/tasks/compute_tasks.py  #import pdb;pdb.set_trace()</span><br><span class="line">[controller_worker]</span><br><span class="line">amp_ssh_key_name = octavia_ssh_key</span><br><span class="line">amp_ssh_access_allowed = True</span><br><span class="line">sudo ip netns exec qrouter-0a839377-6b19-419b-9868-616def4d749f ssh -6 -i ~/octavia_ssh_key ubuntu@fc00:4a9c:7b9e:80b6:f816:3eff:fe5f:cdae</span><br></pre></td></tr></table></figure>
<h2 id="Deploy-a-non-terminated-HTTPS-load-balancer"><a href="#Deploy-a-non-terminated-HTTPS-load-balancer" class="headerlink" title="Deploy a non-terminated HTTPS load balancer"></a>Deploy a non-terminated HTTPS load balancer</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install python-octaviaclient</span><br><span class="line">openstack loadbalancer create --name lb1 --vip-subnet-id private_subnet</span><br><span class="line">#lb_vip_port_id=$(openstack loadbalancer create -f value -c vip_port_id --name lb1 --vip-subnet-id private_subnet)</span><br><span class="line"># Re-run the following until lb1 shows ACTIVE and ONLINE statuses:</span><br><span class="line">openstack loadbalancer show lb1</span><br><span class="line">nova list --all</span><br><span class="line">openstack loadbalancer listener create --name listener1 --protocol HTTPS --protocol-port 443 lb1</span><br><span class="line">openstack loadbalancer pool create --name pool1 --lb-algorithm ROUND_ROBIN --listener listener1 --protocol HTTPS</span><br><span class="line">#openstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type HTTPS --url-path / pool1</span><br><span class="line">openstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type TLS-HELLO pool1</span><br><span class="line">openstack loadbalancer member create --subnet-id private_subnet --address 192.168.21.10 --protocol-port 443 pool1</span><br><span class="line">openstack loadbalancer member create --subnet-id private_subnet --address 192.168.21.12 --protocol-port 443 pool1</span><br><span class="line">openstack loadbalancer member list pool1</span><br><span class="line"></span><br><span class="line">root@amphora-a9cf0b97-7f30-4b9c-b16a-7bc54526e0d0:~# ps -ef |grep haproxy</span><br><span class="line">root      1459     1  0 04:34 ?        00:00:00 /usr/sbin/haproxy-systemd-wrapper -f /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/haproxy.cfg -f /var/lib/octavia/haproxy-default-user-group.conf -p /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0.pid -L UlKGE8M_cxJTcktjV8M-eKJkh-g</span><br><span class="line">nobody    1677  1459  0 04:35 ?        00:00:00 /usr/sbin/haproxy -f /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/haproxy.cfg -f /var/lib/octavia/haproxy-default-user-group.conf -p /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0.pid -L UlKGE8M_cxJTcktjV8M-eKJkh-g -Ds -sf 1625</span><br><span class="line">nobody    1679  1677  0 04:35 ?        00:00:00 /usr/sbin/haproxy -f /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/haproxy.cfg -f /var/lib/octavia/haproxy-default-user-group.conf -p /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0.pid -L UlKGE8M_cxJTcktjV8M-eKJkh-g -Ds -sf 1625</span><br><span class="line">root      1701  1685  0 04:36 pts/0    00:00:00 grep --color=auto haproxy</span><br><span class="line">root@amphora-a9cf0b97-7f30-4b9c-b16a-7bc54526e0d0:~#</span><br><span class="line">root@amphora-a9cf0b97-7f30-4b9c-b16a-7bc54526e0d0:~# cat /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0/haproxy.cfg</span><br><span class="line"># Configuration for loadbalancer eda3efa5-dd91-437c-81d9-b73d28b5312f</span><br><span class="line">global</span><br><span class="line">    daemon</span><br><span class="line">    user nobody</span><br><span class="line">    log /dev/log local0</span><br><span class="line">    log /dev/log local1 notice</span><br><span class="line">    stats socket /var/lib/octavia/b9d5a192-1a6a-4df7-83d4-fe96ac9574c0.sock mode 0666 level user</span><br><span class="line">    maxconn 1000000</span><br><span class="line">defaults</span><br><span class="line">    log global</span><br><span class="line">    retries 3</span><br><span class="line">    option redispatch</span><br><span class="line">frontend b9d5a192-1a6a-4df7-83d4-fe96ac9574c0</span><br><span class="line">    option tcplog</span><br><span class="line">    maxconn 1000000</span><br><span class="line">    bind 192.168.21.16:443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend 502b6689-40ad-4201-b704-f221e0fddd58</span><br><span class="line">    timeout client 50000</span><br><span class="line">backend 502b6689-40ad-4201-b704-f221e0fddd58</span><br><span class="line">    mode tcp</span><br><span class="line">    balance roundrobin</span><br><span class="line">    timeout check 10s</span><br><span class="line">    option ssl-hello-chk</span><br><span class="line">    fullconn 1000000</span><br><span class="line">    option allbackups</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout server 50000</span><br><span class="line">    server 49f16402-69f4-49bb-8dc0-5ec13a0f1791 192.168.21.10:443 weight 1 check inter 5s fall 3 rise 4</span><br><span class="line">    server 1ab624e1-9cd8-49f3-9297-4fa031a3ca58 192.168.21.12:443 weight 1 check inter 5s fall 3 rise 4</span><br></pre></td></tr></table></figure>
<h2 id="Deploy-a-TLS-terminated-HTTPS-load-balancer"><a href="#Deploy-a-TLS-terminated-HTTPS-load-balancer" class="headerlink" title="Deploy a TLS-terminated HTTPS load balancer"></a>Deploy a TLS-terminated HTTPS load balancer</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">openssl genrsa -passout pass:password -out ca.key</span><br><span class="line">openssl req -x509 -passin pass:password -new -nodes -key ca.key -days 3650 -out ca.crt -subj &quot;/C=CN/ST=BJ/O=STS/CN=www.quqi.com&quot;</span><br><span class="line">openssl genrsa -passout pass:password -out lb.key</span><br><span class="line">openssl req -new -key lb.key -out lb.csr -subj &quot;/C=CN/ST=BJ/O=STS/CN=www.quqi.com&quot;</span><br><span class="line">openssl x509 -req -in lb.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out lb.crt -days 3650</span><br><span class="line">cat lb.crt lb.key &gt; lb.pem</span><br><span class="line"></span><br><span class="line">openssl pkcs12 -export -inkey lb.key -in lb.crt -certfile ca.crt -passout pass:password -out lb.p12</span><br><span class="line">sudo apt install python-barbicanclient</span><br><span class="line">openstack secret store --name=&apos;tls_lb_secret&apos; -t &apos;application/octet-stream&apos; -e &apos;base64&apos; --payload=&quot;$(base64 &lt; lb.p12)&quot;</span><br><span class="line">openstack secret list</span><br><span class="line">openstack loadbalancer create --name lb1 --vip-subnet-id private_subnet</span><br><span class="line"># Re-run the following until lb1 shows ACTIVE and ONLINE statuses:</span><br><span class="line">openstack loadbalancer show lb1</span><br><span class="line"></span><br><span class="line">openstack loadbalancer member list pool1</span><br><span class="line">openstack loadbalancer member delete pool1 &lt;member&gt;</span><br><span class="line">openstack loadbalancer pool delete pool1</span><br><span class="line">openstack loadbalancer listener delete listener1</span><br><span class="line">openstack loadbalancer listener create --protocol-port 443 --protocol TERMINATED_HTTPS --name listener1 --default-tls-container=$(openstack secret list | awk &apos;/ tls_lb_secret / &#123;print $2&#125;&apos;) lb1</span><br><span class="line">openstack loadbalancer pool create --name pool1 --lb-algorithm ROUND_ROBIN --listener listener1 --protocol HTTP</span><br><span class="line">openstack loadbalancer member create --subnet-id private_subnet --address 192.168.21.10 --protocol-port 80 pool1</span><br><span class="line">openstack loadbalancer member create --subnet-id private_subnet --address 192.168.21.12 --protocol-port 80 pool1</span><br></pre></td></tr></table></figure>
<p>但是出错了:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@zhhuabj-bastion:~/ca⟫ openstack loadbalancer listener create --protocol-port 443 --protocol TERMINATED_HTTPS --name listener1 --default-tls-container=$(openstack secret list | awk &apos;/ tls_lb_secret / &#123;print $2&#125;&apos;) lb1</span><br><span class="line">Could not retrieve certificate: [&apos;http://10.5.0.25:9312/v1/secrets/7c706fb2-4319-46fc-b78d-81f34393f581&apos;] (HTTP 400) (Request-ID: req-c0c0e4d5-f395-424c-9aab-5c4c4e72fb3d)</span><br></pre></td></tr></table></figure>
<h2 id="附件-Neutron-LBaaS-v2"><a href="#附件-Neutron-LBaaS-v2" class="headerlink" title="附件 - Neutron LBaaS v2"></a>附件 - Neutron LBaaS v2</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">https://docs.openstack.org/mitaka/networking-guide/config-lbaas.html</span><br><span class="line">neutron lbaas-loadbalancer-create --name test-lb private_subnet</span><br><span class="line">neutron lbaas-listener-create --name test-lb-https --loadbalancer test-lb --protocol HTTPS --protocol-port 443</span><br><span class="line">neutron lbaas-pool-create --name test-lb-pool-https --lb-algorithm LEAST_CONNECTIONS --listener test-lb-https --protocol HTTPS</span><br><span class="line">neutron lbaas-member-create --subnet private_subnet --address 192.168.21.13 --protocol-port 443 test-lb-pool-https</span><br><span class="line">neutron lbaas-member-create --subnet private_subnet --address 192.168.21.8 --protocol-port 443 test-lb-pool-https</span><br><span class="line">neutron lbaas-healthmonitor-create --delay 5 --max-retries 2 --timeout 10 --type HTTPS --pool test-lb-pool-https --name monitor1</span><br><span class="line">root@juju-c9d701-xenail-queens-184313-vrrp-5:~# ip netns exec qlbaas-84fd9a6c-24a2-4c0f-912b-eedc254ac1f4 curl -k  https://192.168.21.14</span><br><span class="line">test1</span><br><span class="line">root@juju-c9d701-xenail-queens-184313-vrrp-5:~# ip netns exec qlbaas-84fd9a6c-24a2-4c0f-912b-eedc254ac1f4 curl -k  https://192.168.21.14</span><br><span class="line">test2</span><br><span class="line">root@juju-c9d701-xenail-queens-184313-vrrp-5:~# ip netns exec qlbaas-84fd9a6c-24a2-4c0f-912b-eedc254ac1f4 curl --cacert /home/ubuntu/lb.pem  https://192.168.21.14</span><br><span class="line">curl: (51) SSL: certificate subject name (www.quqi.com) does not match target host name &apos;192.168.21.14&apos;</span><br><span class="line">root@juju-c9d701-xenail-queens-184313-vrrp-5:~# ip netns exec qlbaas-84fd9a6c-24a2-4c0f-912b-eedc254ac1f4 curl --cacert /home/ubuntu/lb.pem  --resolve www.quqi.com:443:192.168.21.14 https://www.quqi.com</span><br><span class="line">test1</span><br><span class="line"></span><br><span class="line">root@juju-c9d701-xenail-queens-184313-vrrp-5:~# echo &apos;show stat;show table&apos; | socat stdio /var/lib/neutron/lbaas/v2/84fd9a6c-24a2-4c0f-912b-eedc254ac1f4/haproxy_stats.sock</span><br><span class="line"># pxname,svname,qcur,qmax,scur,smax,slim,stot,bin,bout,dreq,dresp,ereq,econ,eresp,wretr,wredis,status,weight,act,bck,chkfail,chkdown,lastchg,downtime,qlimit,pid,iid,sid,throttle,lbtot,tracked,type,rate,rate_lim,rate_max,check_status,check_code,check_duration,hrsp_1xx,hrsp_2xx,hrsp_3xx,hrsp_4xx,hrsp_5xx,hrsp_other,hanafail,req_rate,req_rate_max,req_tot,cli_abrt,srv_abrt,comp_in,comp_out,comp_byp,comp_rsp,lastsess,last_chk,last_agt,qtime,ctime,rtime,ttime,</span><br><span class="line">c2a42906-e160-44dd-8590-968af2077b4a,FRONTEND,,,0,0,2000,0,0,0,0,0,0,,,,,OPEN,,,,,,,,,1,2,0,,,,0,0,0,0,,,,,,,,,,,0,0,0,,,0,0,0,0,,,,,,,,</span><br><span class="line">52112201-05ce-4f4d-b5a8-9e67de2a895a,37a1f5a8-ec7e-4208-9c96-27d2783a594f,0,0,0,0,,0,0,0,,0,,0,0,0,0,no check,1,1,0,,,,,,1,3,1,,0,,2,0,,0,,,,,,,,,,0,,,,0,0,,,,,-1,,,0,0,0,0,</span><br><span class="line">52112201-05ce-4f4d-b5a8-9e67de2a895a,8e722b4b-08b8-4089-bba5-8fa5dd26a87f,0,0,0,0,,0,0,0,,0,,0,0,0,0,no check,1,1,0,,,,,,1,3,2,,0,,2,0,,0,,,,,,,,,,0,,,,0,0,,,,,-1,,,0,0,0,0,</span><br><span class="line">52112201-05ce-4f4d-b5a8-9e67de2a895a,BACKEND,0,0,0,0,200,0,0,0,0,0,,0,0,0,0,UP,2,2,0,,0,117,0,,1,3,0,,0,,1,0,,0,,,,,,,,,,,,,,0,0,0,0,0,0,-1,,,0,0,0,0,</span><br><span class="line"></span><br><span class="line">root@juju-c9d701-xenail-queens-184313-vrrp-5:~# cat /var/lib/neutron/lbaas/v2/84fd9a6c-24a2-4c0f-912b-eedc254ac1f4/haproxy.conf</span><br><span class="line"># Configuration for test-lb</span><br><span class="line">global</span><br><span class="line">    daemon</span><br><span class="line">    user nobody</span><br><span class="line">    group nogroup</span><br><span class="line">    log /dev/log local0</span><br><span class="line">    log /dev/log local1 notice</span><br><span class="line">    maxconn 2000</span><br><span class="line">    stats socket /var/lib/neutron/lbaas/v2/84fd9a6c-24a2-4c0f-912b-eedc254ac1f4/haproxy_stats.sock mode 0666 level user</span><br><span class="line">defaults</span><br><span class="line">    log global</span><br><span class="line">    retries 3</span><br><span class="line">    option redispatch</span><br><span class="line">    timeout connect 5000</span><br><span class="line">    timeout client 50000</span><br><span class="line">    timeout server 50000</span><br><span class="line">frontend c2a42906-e160-44dd-8590-968af2077b4a</span><br><span class="line">    option tcplog</span><br><span class="line">    bind 192.168.21.14:443</span><br><span class="line">    mode tcp</span><br><span class="line">    default_backend 52112201-05ce-4f4d-b5a8-9e67de2a895a</span><br><span class="line">backend 52112201-05ce-4f4d-b5a8-9e67de2a895a</span><br><span class="line">    mode tcp</span><br><span class="line">    balance leastconn</span><br><span class="line">    timeout check 10s</span><br><span class="line">    option httpchk GET /</span><br><span class="line">    http-check expect rstatus 200</span><br><span class="line">    option ssl-hello-chk</span><br><span class="line">    server 37a1f5a8-ec7e-4208-9c96-27d2783a594f 192.168.21.13:443 weight 1 check inter 5s fall 2</span><br><span class="line">    server 8e722b4b-08b8-4089-bba5-8fa5dd26a87f 192.168.21.8:443 weight 1 check inter 5s fall 2</span><br><span class="line"></span><br><span class="line"># TCP monitor</span><br><span class="line">neutron lbaas-healthmonitor-delete monitor1</span><br><span class="line">neutron lbaas-healthmonitor-create --delay 5 --max-retries 2 --timeout 10 --type TCP --pool test-lb-pool-https --name monitor1 --url-path /</span><br><span class="line">backend 52112201-05ce-4f4d-b5a8-9e67de2a895a</span><br><span class="line">    mode tcp</span><br><span class="line">    balance leastconn</span><br><span class="line">    timeout check 10s</span><br><span class="line">    server 37a1f5a8-ec7e-4208-9c96-27d2783a594f 192.168.21.13:443 weight 1 check inter 5s fall 2</span><br><span class="line">    server 8e722b4b-08b8-4089-bba5-8fa5dd26a87f 192.168.21.8:443 weight 1 check inter 5s fall 2</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/31/Use-Octavia-to-Implement-HTTPS-Health-Monitors/" data-id="cjqc2cuv5000ed4bps3g0pfkg" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-为租户下的虚机提供IPv6-DNS服务" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/29/为租户下的虚机提供IPv6-DNS服务/" class="article-date">
  <time datetime="2018-10-29T09:37:02.000Z" itemprop="datePublished">2018-10-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/29/为租户下的虚机提供IPv6-DNS服务/">为租户下的虚机提供IPv6 DNS服务</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (<a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a>)</strong></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>当虚机运行下列代码时，我们需要考虑为tenant下的VM提供DNS服务:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import dns</span><br><span class="line">import dns.resolver</span><br><span class="line">answers = dns.resolver.query(&apos;node1&apos;, &apos;AAAA&apos;)</span><br><span class="line">print answers[0].address</span><br></pre></td></tr></table></figure>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>需要截获neutron port event把IP/MAC拿到写到DNS的record中去。neutron port表的fixed_ips字段（在neutron ipallocations表里）同时记录了VM的IPv4与IPv6(netaddr.IPNetwork(fixed_ip[‘ip_address’]).version == 6)地址 [1]，neutron dns_integration特性可以从这里面将IPv4与IPv6地址都取出来记录到neutron-dhcp-agent下的dnsmasq中从而实现ml2-dns内置DNS服务。</p>
<h2 id="IPv6"><a href="#IPv6" class="headerlink" title="IPv6"></a>IPv6</h2><p>如果只是让OpenStack tenant network支持IPv6的话，很简单，直接用下列命令（下列命令有两个重要属性：<strong>ipv6_address_mode 与 ipv6_ra_mode</strong>）。当然，如果是OpenStack Over Openstack环境的话，可以让底层provider network也支持IPv6。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">neutron subnet-create --ip-version=6 --name=zhhuabj_admin_subnet_v6 --ipv6-address-mode=slaac --ipv6-ra-mode=slaac zhhuabj_admin_net 2001:db8:0:1::/64</span><br><span class="line">neutron router-interface-add zhhuabj_router zhhuabj_admin_subnet_v6</span><br></pre></td></tr></table></figure>
<p>上面命令相当于：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ cat /var/lib/neutron/ra/5c33033b-a4e1-494d-ab20-e0498b423b6c.radvd.conf</span><br><span class="line">interface qr-10bb0b85-53</span><br><span class="line">&#123;</span><br><span class="line">   AdvSendAdvert on;</span><br><span class="line">   MinRtrAdvInterval 30;</span><br><span class="line">   MaxRtrAdvInterval 100;</span><br><span class="line">   AdvLinkMTU 1458;</span><br><span class="line">   prefix 2001:db8:0:1::/64</span><br><span class="line">   &#123;</span><br><span class="line">        AdvOnLink on;</span><br><span class="line">        AdvAutonomous on;</span><br><span class="line">   &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">$ sudo ip netns exec qdhcp-be442335-ec55-4df8-b68d-dd03fa6edf00 ps -ef|grep radvd</span><br><span class="line">root     16114     1  0 Nov01 ?        00:00:00 radvd -C /var/lib/neutron/ra/5c33033b-a4e1-494d-ab20-e0498b423b6c.radvd.conf -p /var/lib/neutron/external/pids/5c33033b-a4e1-494d-ab20-e0498b423b6c.pid.radvd -m syslog</span><br><span class="line">root     16115 16114  0 Nov01 ?        00:00:00 radvd -C /var/lib/neutron/ra/5c33033b-a4e1-494d-ab20-e0498b423b6c.radvd.conf -p /var/lib/neutron/external/pids/5c33033b-a4e1-494d-ab20-e0498b423b6c.pid.radvd -m syslog</span><br><span class="line">$ sudo ip netns exec qrouter-5c33033b-a4e1-494d-ab20-e0498b423b6c ip addr show qr-10bb0b85-53 |grep inet6 |grep global</span><br><span class="line">    inet6 2001:db8:0:2::1/64 scope global</span><br><span class="line">$ sudo ip netns exec qdhcp-be442335-ec55-4df8-b68d-dd03fa6edf00 ip addr show ns-af35afad-b2 |grep inet6 |grep global</span><br><span class="line">    inet6 2001:db8:0:2:f816:3eff:feef:5190/64 scope global</span><br></pre></td></tr></table></figure>
<p>如果它不work的话，多半两个原因：<br>1, 防火墙</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo ip6tables -t filter -A INPUT -p udp -m udp --sport 547 --dport 546 -j ACCEPT</span><br><span class="line">#secgroup=$(openstack security group list |grep default| awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">#openstack security group rule create $secgroup --protocol udp --dst-port 546 --ethertype IPv6</span><br></pre></td></tr></table></figure>
<p>2, radvd进程是否正常启动</p>
<p>附1： 若是juju搭建的OpenStack环境要enable IPv6支持的话直接在yaml里添加下列内容即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">overrides:</span><br><span class="line">  prefer-ipv6: true</span><br></pre></td></tr></table></figure>
<p>附2： 使用OpenStack IPv6环境时，直接设置OS_AUTH_URL环境变量指向keystone的IPv6地址即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export OS_AUTH_URL=$&#123;OS_AUTH_PROTOCOL:-http&#125;://[2001:db8:0:1:f816:3eff:fe3e:5e47]:5000/v2.0</span><br></pre></td></tr></table></figure>
<h2 id="Enable-ML2-DNS"><a href="#Enable-ML2-DNS" class="headerlink" title="Enable ML2-DNS"></a>Enable ML2-DNS</h2><p>该特性有dns_name与dns_domain两个重要的属性，dns_domain可用在network与floatingip中，dns_name可用在port和floatingip中。如创建network时指定dns_name (neutron port-create my-net –dns_name my-port), 这样该dns_name和IP会作为dns record。</p>
<p>检查OpenStack是否支持dns extention API。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">neutron ext-list |grep dns</span><br><span class="line">| dns-integration           | DNS Integration</span><br></pre></td></tr></table></figure>
<p>如果不支持，可以修改下列两个文件去支持，dns_domain相当于dnsmasq给不同组织的IP提供DNS服务时的一个区别标志。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/neutron/neutron.conf</span><br><span class="line">dns_domain = example.org.</span><br><span class="line">vi /etc/neutron/plugins/ml2/ml2_conf.ini</span><br><span class="line">[ml2]</span><br><span class="line">extension_drivers = port_security,dns</span><br></pre></td></tr></table></figure>
<p>如果OpenStack是由juju创建，直接使用下列命令即可enable上述两个配置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">juju config neutron-api enable-ml2-dns</span><br><span class="line">juju config neutron-api enable-ml2-dns=True</span><br></pre></td></tr></table></figure>
<h2 id="Test-ML2-DNS"><a href="#Test-ML2-DNS" class="headerlink" title="Test ML2-DNS"></a>Test ML2-DNS</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#sudo ip addr del 2001:db8:0:122::1/64 dev ens3</span><br><span class="line">dig google.com @&lt;DNS-SERVER&gt; -p 53 AAAA</span><br><span class="line">sudo tcpdump -ni ens3 -vv ip6</span><br><span class="line">sudo dhclient -6 -d ens3</span><br></pre></td></tr></table></figure>
<h2 id="designate"><a href="#designate" class="headerlink" title="designate"></a>designate</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/neutron/neutron.conf</span><br><span class="line">external_dns_driver = designate</span><br><span class="line"></span><br><span class="line">juju deploy queens_heat_designate_20181025.yaml #https://paste.ubuntu.com/p/zMsY8j57sG/</span><br><span class="line">juju config designate nameservers=&quot;openstack-au-east-2.oc.rabonet.com.&quot;</span><br><span class="line">./configure keystonev3</span><br><span class="line"></span><br><span class="line">#https://docs.openstack.org/python-designateclient/latest/user/shell-v2.html</span><br><span class="line">openstack zone create --email test@test.com quqi.com.</span><br><span class="line">#openstack zone create --email --email test@test.com --type SECONDARY quqi2.com. --master quqi.com.</span><br><span class="line">openstack recordset list quqi.com.</span><br><span class="line">openstack recordset create --type A --record 192.0.2.20 quqi.com. test1</span><br><span class="line">dig test1.quqi.com @10.5.0.29</span><br><span class="line"></span><br><span class="line">neutron net-update 37aaff3a-6047-45ac-bf4f-a825e56fd2b3 --dns_domain example.org.</span><br><span class="line">openstack recordset list example.org.</span><br><span class="line">neutron port-create 37aaff3a-6047-45ac-bf4f-a825e56fd2b3 --dns_name my-vm</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20181029172830827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F1cWk5OQ==,size_27,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>designate的架构如上图:</p>
<ul>
<li>designate-api, 接收来自远端用户的HTTP/HTTPS请求，通过Keystone验证远端用户的合法性，将HTTP/HTTPS请求传递给Central模块。</li>
<li>designate-sink, 监听来自Nova和Neutron的某些事件，用于自动生成域名资源记录，比如当监听到Nova的compute.instance.create.end事件通知后，自动创建一条对应于刚创建的实例的A记录；当监听到Nuetron的floatingip.update.end事件通知后，自动更新一条相应的A记录。</li>
<li>designate-central, 业务逻辑处理核心。响应API请求以及处理Sink所监听到的来自Nova和Neutron的特定通知事件。同时会存取数据库，对业务逻辑处理所产生的数据进行持久化存储。</li>
<li>designate-mdns, 实现了标准的DNS Notify和Zone Transfer的处理. designate-mdns is the service that sends DNS NOTIFY and answers zone transfer (AXFR) requests. This allows Designate to integrate with any DNS server that supports these very standard methods of communicating. designate-mdns also encapsulates all other forms of DNS protocol that Designate performs. For example, sending SOA queries to check that a change is live.</li>
<li>designate-pool-manager, 连接后端驱动，管理DNS服务器池，与MiniDNS(即designate-mdns)配合同步DNS服务器的域名以及资源记录等数据。</li>
</ul>
<p>MiniDNS(designate-mdns)：Hidden Master设计<br><a href="https://blog.csdn.net/andyron/article/details/46053241" target="_blank" rel="external">https://blog.csdn.net/andyron/article/details/46053241</a><br>Hidden Master是DNS网络安全管理系统设计中所推荐的一种最佳实践。主DNS服务器“隐藏”在内网防火墙背后，负责DNS域名资源的管理并同步变更到从DNS服务器；从DNS服务器部署在DMZ区域，对外提供DNS查询服务。由于主DNS服务器不接受DNS查询，增强了安全性。Designate MiniDNS功能模块就采用了Hidden Master的设计思想。所有托管到Designate中的DNS域都将MiniDNS视为主DNS服务器，而其被委托的DNS服务器都作为从DNS服务器。MiniDNS实现了标准的DNS Notify和Zone Transfer协议，负责同步DNS域名资源记录到从DNS服务器上。<br>其工作流程如下图:<br><img src="https://img-blog.csdnimg.cn/20181029173149275.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F1cWk5OQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li>首先，用户通过Desingate API创建一个example.com的DNS域；</li>
<li>Designate API将请求传递给Central，Central先将example.com域保存到数据库，接着发送RPC请求给Pool Manager；</li>
<li>Pool Manager收到来自Central的创建域名的请求之后，调用DNS后端驱动，在该域名被委托的服务器池中的所有服务器中创建example.com域。同时在这些服务器中，指定example.com的master服务器是MiniDNS；</li>
<li>Pool Manager完成所有从服务器上example.com域的创建之后，发送RPC请求给MiniDNS。</li>
<li>MiniDNS收到Pool Manager的RPC请求之后，向从服务器发送DNS Notify消息，告诉从服务器example.com有资源更新。</li>
<li>从服务器收到DNS Notify消息后，要求主从数据库启动Zone Transfer，域迁移的方式可以是AXFR，也可以是IXFR。</li>
<li>主服务器从数据库中读取为example.com域自动创建的SOA和NS记录，并将SOA和NS记录传送到从服务器。<br>后续任何对example.com域的变更操作都会遵循上述过程，由MiniDNS将变更同步到Designate所委派管理example.com域的DNS服务器上。</li>
</ul>
<p>看一下代码结构, designate支持很多backend(eg: bind), 安装bind服务的机器上可使用rndc命令行工具create/delete zone remotely. The traffic between rndc and bind/named(953/tcp) is authenticated with a key. designate将为每个pool生成下列配置, 这样就可以远程运行rndc命令了(rndc -s 10.5.0.29 -p 953 -k /etc/designate/rndc.key status), 其中5353是designate-mdns监听的端口:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/designate/pools.yaml</span><br><span class="line">- id: 794ccc2c-d751-44fe-b57f-8894c9f5c842</span><br><span class="line">  name: default</span><br><span class="line">  description: Pool genergated by Juju</span><br><span class="line">  ns_records:</span><br><span class="line">    - hostname: openstack-au-east-2.oc.xxx.com.</span><br><span class="line">      priority: 10</span><br><span class="line">  nameservers:</span><br><span class="line">    - host: 10.5.0.29</span><br><span class="line">      port: 53</span><br><span class="line">  targets:</span><br><span class="line">    - type: bind9</span><br><span class="line">      masters:</span><br><span class="line">        - host: 10.5.0.23</span><br><span class="line">          port: 5354</span><br><span class="line">      options:</span><br><span class="line">        host: 10.5.0.29</span><br><span class="line">        rndc_host: 10.5.0.29</span><br><span class="line">        rndc_key_file: /etc/designate/rndc.key</span><br><span class="line">  also_notifies: []</span><br></pre></td></tr></table></figure></p>
<p>在designate-bind节点上装有bind服务(运行在953端口, /usr/sbin/named -f -u bind), 需要确保bind能够访问/etc/bind/named.conf和/etc/bind/rndc.key, 并且能够接受从Pool Manager过来的rndc流量:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/bind/named.conf</span><br><span class="line">include &quot;/etc/bind/named.conf.options&quot;;</span><br><span class="line">include &quot;/etc/bind/named.conf.local&quot;;</span><br><span class="line">include &quot;/etc/bind/named.conf.default-zones&quot;;</span><br><span class="line">controls &#123;</span><br><span class="line">  inet 127.0.0.1 allow &#123;localhost;&#125;;</span><br><span class="line">  inet 10.5.0.29 allow &#123; 10.5.0.23; &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"># cat /etc/bind/named.conf.options</span><br><span class="line">options &#123;</span><br><span class="line">        directory &quot;/var/cache/bind&quot;;</span><br><span class="line">        dnssec-validation auto;</span><br><span class="line">        auth-nxdomain no;    # conform to RFC1035</span><br><span class="line">        listen-on-v6 &#123; any; &#125;;</span><br><span class="line">        allow-new-zones yes;</span><br><span class="line">        request-ixfr no;</span><br><span class="line">        recursion no;</span><br><span class="line">        statistics-file &quot;/var/cache/bind/named.stats&quot;;</span><br><span class="line">        zone-statistics yes;</span><br><span class="line">        allow-notify &#123; 10.5.0.23; &#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>rndc命令:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rndc querylog</span><br><span class="line">rndc status</span><br><span class="line">dig -t A openstack-au-east-2.oc.xxx.com@10.5.0.23</span><br><span class="line">dig -t MX openstack-au-east-2.oc.xxx.com@10.5.0.23</span><br></pre></td></tr></table></figure></p>
<p>bind DNS服务器可作为缓存服务器, 主DNS服务器和辅助DNS服务器, 配置分配如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 缓存服务器, 不负责解析，仅为加速，不需要注册</span><br><span class="line">options &#123;</span><br><span class="line">       forward only;</span><br><span class="line">       forwarders &#123;</span><br><span class="line">               168.95.1.1;</span><br><span class="line">               139.175.10.20;</span><br><span class="line">       &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"># 主DNS服务器, 负责解析本地客户端请求</span><br><span class="line">zone &quot;test.com&quot; IN &#123;</span><br><span class="line">       type master;</span><br><span class="line">       file &quot;test.com.zone&quot;;</span><br><span class="line">&#125;;</span><br><span class="line"># 辅助DNS服务器, 辅助服务器的区域数据都是从主服务器复制而来，其数据都是只读的. 根据序列号大小决定是否复制</span><br><span class="line">zone &quot;test.com&quot; IN &#123;</span><br><span class="line">       type slave;</span><br><span class="line">       masters &#123;ip;&#125;;</span><br><span class="line">       file &quot;slaves/test.com.zone&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>区域传送, 解析库文件同步的过程，即辅助DNS服务器从主DNS服务器或其他的辅助DNS服务器请求数据传输过程 。</p>
<ul>
<li>完全区域传送：传送区域的所有数据，简称AXFR</li>
<li>增量区域传送：传送区域中改变的数据部分，简称IXFR<br>bind配置中之DNS主从同步，区域安全传送<br><a href="http://www.it165.net/admin/html/201403/2548.html" target="_blank" rel="external">http://www.it165.net/admin/html/201403/2548.html</a></li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://github.com/openstack/neutron/blob/stable/pike/neutron/plugins/ml2/extensions/dns_integration.py#L285" target="_blank" rel="external">https://github.com/openstack/neutron/blob/stable/pike/neutron/plugins/ml2/extensions/dns_integration.py#L285</a><br>[2] <a href="https://docs.openstack.org/mitaka/networking-guide/config-dns-int.html" target="_blank" rel="external">https://docs.openstack.org/mitaka/networking-guide/config-dns-int.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/10/29/为租户下的虚机提供IPv6-DNS服务/" data-id="cjqc2cuvb000ld4bpuaykunjg" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/10/hello-world/" class="article-date">
  <time datetime="2018-09-10T05:57:54.513Z" itemprop="datePublished">2018-09-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/10/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/10/hello-world/" data-id="cjqc2cuv9000id4bpczzdg614" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Win-10-UEFI-Ubuntu-18-04-UEFI-双系统" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/09/Win-10-UEFI-Ubuntu-18-04-UEFI-双系统/" class="article-date">
  <time datetime="2018-09-08T17:28:15.000Z" itemprop="datePublished">2018-09-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/09/Win-10-UEFI-Ubuntu-18-04-UEFI-双系统/">Win 10 UEFI + Ubuntu 18.04 UEFI 双系统</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>本人昨天买了一块SSD, 结果后来发现原来这块SSD存在硬件质量问题, 造成了软件上的种种诡异问题, 如U盘时而识别时而不识别, 如触摸屏左键时而抽风, 如ghost安装win10时几乎到100%的进度时忽然来一个无响应, 重启系统后出现了”To interrupt normal start up, press the blue ThinkVantage button.”, 此时键盘无反应, 既进不了系统, 也进不了BIOS. 拨CMOS电源也无效. 最后发现是这块SSD有质量问题. 估计是SSD有控制器主要是软件吧, 控制器软件有bug导致运行ghost这种软件时也能导致硬件挂住.<br>也正是因为这个问题吧, 七搞八搞, 一不小心在重试的过程中将之前的一块linux分区误删了, 于是之前打算的迁移双系统的想法泡汤(当然, 那些通过分区助手或者ghost来迁移分区的网上文章照着做没一个是成功的).<br>这样, 有机会事隔多年再一次重装双系统的机会, 但是发现世道变了, 之前百试不爽的方法现在行不通了. 后经查证, 主要原因是ubuntu 18.04开始默认采用UEFI, 而win10默认仍然是MBR. 这样会导致一系列的问题, 如报错: grub-efi-amd64-signed failed to install 18.04, 统一采用UEFI安装.</p>
<h2 id="BIOS设置"><a href="#BIOS设置" class="headerlink" title="BIOS设置"></a>BIOS设置</h2><p>在BIOS中将Boot Mode设置为UEFI Only, 如果有Secure Boot选项还要disable它(不做这一步可能会造成按F12键之后无法找到U盘)<br>注: 改成UEFI only之后, 运行双系统, 四系统都没问题, 但后来进不了U盘的livecd, 报: couldn’t get UEFI db list, 所以只得改回Both, 但UEFI优先.</p>
<h2 id="安装win10"><a href="#安装win10" class="headerlink" title="安装win10"></a>安装win10</h2><ul>
<li>下载大白菜UEFI专版 - <a href="http://www.bigbaicai.com/download.html?down2" target="_blank" rel="external">http://www.bigbaicai.com/download.html?down2</a></li>
<li>下载win10 ghost - axel -n 10 <a href="http://xz.win10cjb.com/18.5/win10_64/DEEP_Win10x64_201805.rar" target="_blank" rel="external">http://xz.win10cjb.com/18.5/win10_64/DEEP_Win10x64_201805.rar</a></li>
<li>制作大白菜启动U盘, 如果界面上有UEFI字眼就点上(不记得了, 有就点上), 还要注意一点, 记得点里面的格式转换, 将FAT32格式(HDD-FAT32)转换成NTFS(HDD-NTFS)转换, 否则HDD-FAT32格式不能拷贝大于4G的ghost文件哦,</li>
<li>按F12选U盘启动进入大白菜后, 用DiskGenius工具重新分区, 必须将BIOS+MBR格式转UEFI+GPT格式. 分区表格式为GUID而不是MBR, window上管EFI分区叫ESP/MSR分区</li>
<li><p>注意, 不要修改推荐的卷标, 这个卷就是指向的ESP/MSR分区.</p>
<h2 id="安装win10后"><a href="#安装win10后" class="headerlink" title="安装win10后"></a>安装win10后</h2><p>安装win10后需要将禁用掉快速启动, 否则会造成按F12无法选择U盘启动. 菜单路径为: “设置 -&gt; 系统 -&gt; 电源与睡眠 -&gt; 其他电源设置 -&gt; 选择电源按钮的功能 -&gt; 更改当前不可用的设置 -&gt; 启动快速启动”</p>
<h2 id="安装ubuntu-18-04"><a href="#安装ubuntu-18-04" class="headerlink" title="安装ubuntu 18.04"></a>安装ubuntu 18.04</h2><p>像安装win10一样, 一样要注意重要一点, 需创建大概300M左右的UEF分区, 另外, 还可以创建一个根分区和一个备份文件用的bak分区.<br>注意, windows非常霸道, 它是总修改bios里的启动顺序, 将它的”Windows boot Manager”放在”ubuntu grub”的前面, 可以在bios里锁定启动顺序</p>
<h2 id="安装win7"><a href="#安装win7" class="headerlink" title="安装win7"></a>安装win7</h2><p>win7若没有sata的驱动, 所以得先改回IDE, 装完win7之后再改回AHCI, 否则也容易挂在启动界面不动了.<br>注: 我未遇到以上问题, 可能因为我装的win7并不是原版的, 已经带了sata驱动</p>
<h2 id="加装SSD"><a href="#加装SSD" class="headerlink" title="加装SSD"></a>加装SSD</h2><p>如果加装了SSD之后呢? 那得注意:</p>
</li>
<li><p>装win10时同样需要进大白菜或老毛桃后用DiskGenius在SSD上划分ESP/MSR分区</p>
</li>
<li>装ubuntu时, 分区处也要创建EFI分区, 同时grub设置安装在SSD上, 相当于: grub-install /dev/sdX.</li>
<li>bios里选择哪块硬盘启动. 其实在SSD上安装grub后, 这个grub会连HDD上原先的win10与ubuntu一起放在启动列表里. 注意, windows非常霸道, 它是总修改bios里的启动顺序, 将它的”Windows boot Manager”放在”ubuntu grub”的前面, 可以在bios里锁定启动顺序</li>
<li><p>有时候需要对ssd优化, 例如不要将swap分区放在ssd以延长寿命, 如更改i/o调度策略为noop, 如使用bcache</p>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><p>装完之后进入win10发现thinkpad小红点左键失灵, 再切换进ubuntu发现小红点左键正常(实际上, 5次大概有一次有问题, 只是登录界面左键与右键似乎混乱了, 登录之后就正常了. 再换PE进系统发现小红点左键依然有问题. 所以基本断定和硬件没有关系, 应该是win10上的小红点驱动有问题.<br>但搜索了很多帖子, 没一个能解决问题的, 联想的小红点win10驱动做得太烂了. 所以决定回到win7, 回到win7之后该问题解决. 另外, PE回到win7的过程中不会伤害之前SSD上安装的ubuntu系统, 也不会伤害原HDD里的双系统.</p>
<h2 id="数据迁移"><a href="#数据迁移" class="headerlink" title="数据迁移"></a>数据迁移</h2><p>现在在笔记本x220t上装了win10, 也装了ubuntu 18.04, 但是如何将工作机t440p的根分区迁移到x220t的根分区呢? 因为我们已经在x220t上安装了ubuntu 18.04, 这样省去了采用命令划分EFI分区, 以及最后填充EFI分区的步骤. 现在将精力集中在如何快速迁移根分区上.</p>
</li>
<li><p>目的机x220t因为有写操作, 故要以livecd启动, 启动ssh server, 并将根分区加载到/mnt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo -i</span><br><span class="line">apt install openssh-server</span><br><span class="line">passwd</span><br><span class="line">echo &apos;PermitRootLogin yes&apos; &gt;&gt; /etc/ssh/sshd_config</span><br><span class="line">service ssh restart</span><br><span class="line">fdisk -l</span><br><span class="line">mount /dev/sdb8 /mnt</span><br></pre></td></tr></table></figure>
</li>
<li><p>源机t440p最好也以livecd启动, 注意: 例如源机上有一个软链指向了/bak分区, 但因为此时没有挂载/bak分区, 所以在rsync命令迁移时会报错退出. 人工删除该软链重新运行即可.  且需要注意 rsync命令中的/mnt/后应该有/, 否则会将mnt目录迁移到根分区的mnt目录下.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo -i</span><br><span class="line">fdisk -l</span><br><span class="line">mount /dev/sda9 /mnt</span><br><span class="line"></span><br><span class="line"># rsync will now copy all files, directories, permissions and owners over to the destination machine.</span><br><span class="line"># It also skips all files and directories that are not on the root filesystem, like /dev/, /sys/, /proc/.</span><br><span class="line"># If there are filesystems that are mounted separately on the source machine and your want those copied too, use rsync again on those mountpoints too.</span><br><span class="line">rsync -xavP --numeric-ids --exclude=&apos;tmp&apos; /mnt root@192.168.99.128:/</span><br><span class="line">#rsync -xavP --numeric-ids --exclude=&apos;tmp&apos; --exclude=&apos;/nas&apos; /mnt/ root@192.168.99.128:/mnt/</span><br></pre></td></tr></table></figure>
</li>
<li><p>更新grub, 此时会报”canot find EFI directory”, 这样会导致这时生成grub时无法找到原HDD中的双系统, 不要紧, 只要找到目前SSD中的双系统即可. 呆会下一步再运行一下grub命令即可解决</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mount /dev/sdb8 /mnts</span><br><span class="line">for d in dev sys proc; do mount --bind /$d /mnt/$d; done</span><br><span class="line">chroot /mnt/ grub-install /dev/sdb   # canot find EFI directory</span><br><span class="line">chroot /mnt/ update-grub</span><br></pre></td></tr></table></figure>
</li>
<li><p>修复fstab, 之前运行上述迁移命令前忘了备份x220t上的fstab系统, 导致它被覆盖, OK, 我们修复它.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">blkid</span><br><span class="line">e2label /dev/sdb8 &quot;ROOT_SSD&quot;</span><br><span class="line">tee &quot;/mnt/etc/fstab&quot; &lt;&lt;EOF</span><br><span class="line">#UUID can be found via blkid command</span><br><span class="line">#LABEL=boot /boot ext2 sync 0 2</span><br><span class="line">#UUID=735b3be3-779c-4d21-a944-b033225f3ab4 none   swap    sw      0       0</span><br><span class="line">#LABEL=SWAP none swap sw 0 0</span><br><span class="line">UUID=9401-D2EA /boot/efi vfat defaults 0 2</span><br><span class="line">LABEL=ROOT_SSD / ext4 errors=remount-ro 0 1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
</li>
<li><p>这时重启系统, 就可以以grub选择启动SSD上的双系统了, 如果还想把HDD的原有的双系统也加到grub的话, 那进ubuntu系统后再执行一次update-grub命令即可.</p>
</li>
<li>这种迁移方式效果非常好, 各种工作软件不需要再重装了. 呵呵<h2 id="调整分区"><a href="#调整分区" class="headerlink" title="调整分区"></a>调整分区</h2>一个分区不够用时, 可以使用gpartd合并相邻的空闲分区.注意一点, 要合并的分区必须是umount状态时才能合并.<h2 id="SSD优化"><a href="#SSD优化" class="headerlink" title="SSD优化"></a>SSD优化</h2></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># disable scanning for btrfs filesystems when boot</span><br><span class="line">sudo apt-get purge btrfs-tools</span><br><span class="line">sudo update-initramfs -ukall</span><br><span class="line"></span><br><span class="line"># enable TRIM feature by adding discard option</span><br><span class="line"># what&apos;s TRIM - https://blog.csdn.net/quqi99/article/details/50963308</span><br><span class="line"># the option noatime is used to disable access time for a file</span><br><span class="line">sudo hdparm -I /dev/sdb |grep TRIM</span><br><span class="line">vi /etc/fstab</span><br><span class="line">LABEL=ROOT_SSD /               ext4    noatime,discard,errors=remount-ro 0       1</span><br><span class="line">sudo mount -o remount /dev/sdb8</span><br><span class="line">sudo mount |grep sdb8 |grep discard</span><br><span class="line"></span><br><span class="line"># Try not to use swap space unless it&apos;s running out of memory.</span><br><span class="line">echo 1 &gt; /proc/sys/vm/swappiness</span><br><span class="line"></span><br><span class="line"># avoid visiting ssd by using ramdisk for /tmp instead of tmpfs</span><br><span class="line">vim /etc/fstab</span><br><span class="line">tmpfs /tmp tmpfs defaults,noatime,mode=1777 0 0</span><br><span class="line">tmpfs /var/tmp tmpfs defaults,noatime,mode=1777 0 0</span><br><span class="line">tmpfs /var/log tmpfs defaults,noatime,mode=1777 0 0</span><br><span class="line">sudo mount -o remount /</span><br><span class="line"></span><br><span class="line"># Set chrome to use ramdisk cache</span><br><span class="line">cd ~/.cache/google-chrome/Default</span><br><span class="line">rm -rf Cache</span><br><span class="line">sudo ln -s /tmp Cache</span><br><span class="line">rm -rf Media\ Cache/</span><br><span class="line">sudo ln -s /tmp Media\ Cache</span><br><span class="line"></span><br><span class="line"># Use noop for I/O elevator</span><br><span class="line">cat /sys/block/sda/queue/scheduler</span><br><span class="line">sudo vi /etc/default/grub</span><br><span class="line">GRUB_CMDLINE_LINUX_DEFAULT=&quot;elevator=noop&quot;</span><br><span class="line">sudo update-grub</span><br><span class="line"></span><br><span class="line"># Test SSD speed</span><br><span class="line">$ sudo hdparm -Tt /dev/sdb</span><br><span class="line">/dev/sdb:</span><br><span class="line"> Timing cached reads:   9128 MB in  2.00 seconds = 4569.28 MB/sec</span><br><span class="line"> Timing buffered disk reads: 818 MB in  3.01 seconds = 272.07 MB/sec</span><br><span class="line"></span><br><span class="line"># Make sure 4K align</span><br><span class="line">$ sudo fdisk -lu |grep sdb |grep sectors</span><br><span class="line">Disk /dev/sdb: 232.9 GiB, 250059350016 bytes, 488397168 sectors</span><br><span class="line"></span><br><span class="line"># Health check</span><br><span class="line">$ sudo smartctl -s on -a /dev/sdb |grep PASSED</span><br><span class="line">SMART overall-health self-assessment test result: PASSED</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/09/Win-10-UEFI-Ubuntu-18-04-UEFI-双系统/" data-id="cjqc2cuvc000md4bp7o7u2ykg" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-也谈wifi断流问题" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/03/也谈wifi断流问题/" class="article-date">
  <time datetime="2018-09-03T04:02:42.000Z" itemprop="datePublished">2018-09-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/03/也谈wifi断流问题/">也谈wifi断流问题</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (作者：张华 发表于：2018-09-03)</strong></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>笔者最近应该是遇到了常听大家说起的wifi断流问题, 新入一款安卓原生系统手机, 但是在使用wifi上网时会感觉到某些APP上网不流畅, 尤其是使用京东APP搜索商品时会总说找不着网络, 但此时显然是有网络的. 为此, 笔者先做了一系统排除性实验:</p>
<ul>
<li>排除法测试, 使用京东APP搜索商品时说找不着网络, 但使用京东APP的其他功能没有问题, 并且使用京东以外的其他APP也没问题</li>
<li>排除法测试, 切换为4G网络使用京东APP搜索商品正常, 仅仅只是使用WIFI网络时才会出问题.</li>
<li>排除法测试, 去麦当劳使用WIFI确认无问题, 但速度也不快, 但能打开页面.</li>
<li>排除法测试, 难道是家里的WIFI有问题吗? 但换个手机型号使用京东APP搜索商品却又正常.</li>
<li>排除法测试, 难道是VPN的问题? 关掉VPN, 恢复DNS国内设置依然有问题. </li>
<li>排除法测试, 继续换一个没有VPN的干净的OpenWRT路由器依然有问题, 可惜家里没有Non-OpenWRT路由器可供测试.</li>
<li>排除法测试, 路由器上修改802.11g, 802.11n, 802.11ac等设置后问题依旧.</li>
<li>排除法测试, 检查了路由器上的MAC地址是否与其他机器重复, 未发现异常</li>
<li>排除法测试, 使用114.114.114.114作为DNS, 问题依旧</li>
<li>排除法测试, OpenWRT路由器使用tcpdump抓包, 干扰条目过多, 未深入</li>
<li>排除法测试, 现在问题看起来只是发生在这款特定手机型号与特定的OpenWRT路由器与特定的某些APP如京东, 手机刷机到android 8.1与7.1两个版本问题依旧.</li>
<li>排除法测试, google搜索大量京东或别的某些应用在各种手机型号上出问题的帖子, 试着更改帖子中的各种切换手机配置的操作, 如不对京东使用电源优化,问题依旧.</li>
<li>排除法测试, 绝大多数时候打不开京东的这个搜索商品的功能, 但极少数情况又能打开, 但非常慢, 使用别家的wifi网络时也是非常慢, 很难说清楚现象.<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2>上述一系列排除性测试让我相信该问题仅和我使用特定的手机型号, 使用特定的OpenWRT路由器, 使用特定的某些APP如京东有关.<br>京东APP, 一个上层应用而已, 理论上只有下列几个因素会影响到上层应用:</li>
<li>DNS</li>
<li>IPv6/IPv4 fallback</li>
<li>MTU<br>理论让我将目光回到MTU, 修改OpenWRT路由器WAN口的MTU=1492后问题依旧.继续深挖:</li>
<li>路由器背后的手机操作系统应该有/proc/sys/net/ipv4/ip_no_pmtu_disc=0让手机可以根据pmtu来确实应用所需的mss值. 遗憾地是, 手机没有root, 无法检查此项值.</li>
<li>OpenWRT路由器tcpdump抓包, 看到的mss值确实不小. 既然无root权限无法修改手机的ip_no_pmtu_disc参数, 那有没有方法直接修改OpenWRT路由器强迫修改mss值呢?<br>OK, 在路由器上添加如下两个命令, 问题就这么解决了:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#iptables -A FORWARD -j ACCEPT</span><br><span class="line">iptables -t mangle -A FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line"></span><br><span class="line">root@OpenWrt:~# iptables-save |grep mss</span><br><span class="line">:mssfix - [0:0]</span><br><span class="line">-A FORWARD -j mssfix</span><br><span class="line">-A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --clamp-mss-to-pmtu</span><br><span class="line">-A mssfix -o pppoe-wan -p tcp -m tcp --tcp-flags SYN,RST SYN -m comment --comment &quot;wan (mtu_fix)&quot; -j TCPMSS --clamp-mss-to-pmtu</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这款手机的操作系统没有设置ip_no_pmtu_disc参数去协商mss值, 而OpenWRT路由器刚好缺一条iptables rule (iptables -t mangle -A FORWARD -p tcp –tcp-flags SYN,RST SYN -j TCPMSS –clamp-mss-to-pmtu), 这样遭遇了pppoe的1492 MTU问题.<br>换句话说, 当我外出时, 如果所连的路由器没有加这条设置, 那么这个问题仍然又遇到. 手机操作系统ip_no_pmtu_disc设置才能彻底解决某些应用wifi网络不能上网的问题. </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/03/也谈wifi断流问题/" data-id="cjqc2cuva000kd4bpm3vobuwq" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-IPv6来啦" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/03/IPv6来啦/" class="article-date">
  <time datetime="2018-08-03T08:01:05.000Z" itemprop="datePublished">2018-08-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/03/IPv6来啦/">IPv6来啦</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (作者：张华 发表于：2018-08-03)</strong></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>家里用的是中国移动的宽带, 一直挺稳定的, 而且昨天发现ISP下发了IPv6地址(不是子网, 形如: 2409:8a00:7805:xxxx:xxxx:xxxx:xxxx:xxxx/64, 打xxxx的部分是动态变化的). 好吧, 咱就用用.</p>
<h2 id="路由器配置"><a href="#路由器配置" class="headerlink" title="路由器配置"></a>路由器配置</h2><ul>
<li><p>配置/etc/config/network使用’option ip6assign ‘64’</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config interface &apos;lan&apos;</span><br><span class="line">        ...</span><br><span class="line">        option ip6assign &apos;64&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置/etc/confignetwork删除config globals ‘globals’段中和 IPv6 ULA-Prefix相关的配置 (注: 使用IPv6 ULA-Prefix的话, LAN中机器就会得到一个以它打头的IPv6地址, 但如何从外网访问这个地址呢? 有三种方式: 一是使用我们现在使用的relay方式得到是ISP提供的全球可路由的IPv6地址；二是配置ULA-Prefix=2409:8a00:7805:1::/80之后再使用下列的neigh proxy的方法解决, 但这种一般是子网长度80比64大, 但ISP并没有给我们分配subnet, 只是分配了IPv6地址, 所以无法保证2409:8a00:7805:1::/80这个前缀与ISP分配的2409:8a00:7805:xxxx:xxxx:xxxx:xxxx:xxxx/64一致；三是ULA-Prefix配置一个与ISP分配的不同的路由然后通过配置路由的方式但前提是也得有全球可路由的IPv6 subnet啊. 所以这里我们选择了relay模式来实现外网访问内网的目的.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv6.conf.all.proxy_ndp=1</span><br><span class="line">ip -6 neigh add proxy 2409:8a00:7805:1::430 dev pppoe-wan</span><br><span class="line">ip -6 neigh show proxy</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置/etc/config/dhcp使用relay模式, relay模式意味着openwrt通过默认的odhcpd作为中继自动为LAN的其他机器配置ISP的IPv6地址</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">config dhcp &apos;wan&apos;</span><br><span class="line">        option interface &apos;wan&apos;</span><br><span class="line">        option ignore &apos;1&apos;</span><br><span class="line">        option dhcpv6 &apos;disabled&apos;</span><br><span class="line">        option ndp &apos;relay&apos;</span><br><span class="line">        option ra &apos;relay&apos;</span><br><span class="line">        option master &apos;1&apos;</span><br><span class="line">config dhcp &apos;lan&apos;</span><br><span class="line">        option interface &apos;lan&apos;</span><br><span class="line">        option start &apos;100&apos;</span><br><span class="line">        option limit &apos;150&apos;</span><br><span class="line">        option leasetime &apos;12h&apos;</span><br><span class="line">        list dhcp_option &apos;6,192.168.99.1&apos;</span><br><span class="line">        option ndp &apos;relay&apos;</span><br><span class="line">        option ra &apos;relay&apos;</span><br><span class="line">        option dhcpv6 &apos;relay&apos;</span><br><span class="line"></span><br><span class="line"># actually I use 60, not 64</span><br><span class="line">cat /etc/config/network</span><br><span class="line">config interface &apos;lan&apos;</span><br><span class="line">	...</span><br><span class="line">	option ip6assign &apos;60&apos;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>配置完之后就可以通过下列命令重启路由器服务就可以在br-lan与pppoe-wan上获得ISP分配的IPv6地址了.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/network restart</span><br><span class="line">/etc/init.d/odhcpd restart</span><br></pre></td></tr></table></figure></p>
<p>内网机器直接通过’sudo /etc/init.d/network-manager restart’重启网络也会获取ISP分配的IPv6地址.</p>
<h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><ul>
<li>内网机器访问br-lan内网网关, 能通是因为下列路由的功能:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hua@t440p:~$ ip -6 route list |grep 2409:8a00:7805:b7df::/64</span><br><span class="line">2409:8a00:7805:b7df::/64 dev eth0  proto ra  metric 100  pref medium</span><br><span class="line"></span><br><span class="line">root@OpenWrt:~# route -A inet6 |grep ::/0</span><br><span class="line">::/0                                        fe80::200:5eff:fe00:134                 UG    1024   0        0</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>注: 后面我们会看到::/0的默认路由会造成很多问题.</p>
<ul>
<li><p>内网机器访问pppoe-wan外网网关<br>对于relay模式由于内网机器拿到的本来就是ISP分配的可路由的IP所以自然能通. 但对于nat模式由于内网机器分配的是和ISP不同网段的IP, 所以需要在路由器上做NAT6, 如:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#ip6tables -t nat -I POSTROUTING -o pppoe-wan -j MASQUERADE</span><br><span class="line">ip6tables -t nat -I POSTROUTING -s `uci get network.globals.ula_prefix` -j MASQUERADE &gt; /dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure>
</li>
<li><p>外网访问内网机器<br>如果使用relay由于内网机器使用的是全球可路由的IPv6地址, 外网直接就是可以访问内网机器的；但如果是不同子网需要做路由；如果只是前缀相同只是子网长度不同可以做neigh proxy</p>
</li>
<li>但是此时我们发现内网机器无法访问外网(如ping6 ipv6.baidu.com), 但此时在路由器上却是可以访问外网的. 原因就在于上面使用::/0的默认路由似乎有问题(报这个错: Destination unreachable: Unknown code 5), 改成了2000::/3就好了:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">route -A inet6 add 2000::/3 `route -A inet6 | grep ::/0 | awk &apos;NR==1&#123;print&quot;gw &quot;$2&quot; dev &quot;$7&#125;&apos;`</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>或者:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip -6 route add default from 2409:8a00:7805::48 dev pppoe-wan</span><br></pre></td></tr></table></figure></p>
<p>但上面两种方法仍然遇到了不稳定的问题, 感觉是openwrt的odhcpd不稳定, 照下列方法更换为6relayd之后仍然不好使.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">wget https://www.shintaku.cc/files/6relayd_2013-10-21_ar71xx.ipk</span><br><span class="line">opkg install 6relayd_2013-10-21_ar71xx.ipk</span><br><span class="line">vi /etc/config/6relayd</span><br><span class="line">config relay</span><br><span class="line">        option master &apos;wan&apos;</span><br><span class="line">        option network &apos;lan&apos;</span><br><span class="line">        option rd &apos;relay&apos;</span><br><span class="line">        option dhcpv6 &apos;relay&apos;</span><br><span class="line">        option ndp &apos;relay&apos;</span><br><span class="line">/etc/init.d/odhcpd disable</span><br><span class="line">/etc/init.d/odhcpd stop</span><br><span class="line">/etc/init.d/6relayd enable</span><br><span class="line">/etc/init.d/6relayd start</span><br></pre></td></tr></table></figure></p>
<p>接着使用tcpdump查看好像是DHCPv6 reply包从pppoe-wan过不来br-lan口:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">08:22:53.810272 IP6 2400:da00:2::29 &gt; 2409:8a00:7805:b7df:8d79:6c9:315a:9ca3: ICMP6, echo reply, seq 7, length 64</span><br></pre></td></tr></table></figure></p>
<p>所以接着, 在/etc/config/firewall文件的 Allow-ICMPv6-Forward项中添加了下列行后确认已经有了129(129就是reply)相关的iptables rules之后仍然不好使.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">        list icmp_type &apos;router-solicitation&apos;</span><br><span class="line">        list icmp_type &apos;neighbour-solicitation&apos;</span><br><span class="line">        list icmp_type &apos;router-advertisement&apos;</span><br><span class="line">        list icmp_type &apos;neighbour-advertisement&apos;</span><br><span class="line"></span><br><span class="line">root@OpenWrt:~# ip6tables-save |grep Allow-ICMPv6-Forward |grep 129</span><br><span class="line">-A zone_wan_forward -p ipv6-icmp -m icmp6 --icmpv6-type 129 -m limit --limit 1000/sec -m comment --comment Allow-ICMPv6-Forward -j ACCEPT</span><br></pre></td></tr></table></figure></p>
<p>google查到的和这个bug相同(<a href="https://github.com/openwrt/odhcpd/issues/37" target="_blank" rel="external">https://github.com/openwrt/odhcpd/issues/37</a>), 但里面的所有方法都试过了不成功.<br>似乎是ISP使用的是Statefull DHCPV6的方式, 6relayd可以把ra信息relay过来，但LAN端机器似乎无法跟DHCPV6服务器通信。</p>
<h2 id="2018-0805更新"><a href="#2018-0805更新" class="headerlink" title="2018-0805更新"></a>2018-0805更新</h2><p>今天通过这个帖子(<a href="https://bbs.pku.edu.cn/v2/post-read.php?bid=35&amp;threadid=15501646)解决了上面的问题" target="_blank" rel="external">https://bbs.pku.edu.cn/v2/post-read.php?bid=35&amp;threadid=15501646)解决了上面的问题</a>:<br>OpenWRT默认是在wan口使用DHCPv6 Client, 在LAN口使用odhcpd开启RA和DHCPv6. 这个默认配置适用于国外主流ISP, 因为他们DHCPv6-PD (prefix delegation)把一个至少/64地址段分配给客户使用(还有的使用小于64的地址段给客户分配静态IP).<br>不过中国移动给客户分配的是SLAAC地址, 没有使用DHCPv6, 也就没使用DHCPv6-PD, 这样拿不到前缀(ISP分配的2409:8a00:7805:xxx::/64地址的第4段总是变化的), 所以odhcpd也就无法根据这个前缀设置路由, 所以我们需要手工设置确保可以在OpenWrt上ping通内网机器, 这样才能保证reply消息到达br-lan之后能到达内网机器, 如:<br>route -A inet6 add 2409:8a00:7805:d9b1::/64 dev br-lan<br>所以最终添加在/etc/firewall.user的内容如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># make ipv6 relay to work</span><br><span class="line">PREFIX=`route -A inet6 |grep lo |grep 2409:8a00:7805 |grep ::/128 |awk -F &apos;::/&apos; &apos;&#123;print $1&quot;::/64&quot;&#125;&apos; |uniq`</span><br><span class="line">ip -6 route del $PREFIX dev br-lan &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">route -A inet6 add $PREFIX dev br-lan &gt; /dev/null 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line"># make ipv6 nat to work</span><br><span class="line">#ip -6 route add default from $PREFIX dev pppoe-wan &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">#route -A inet6 add 2000::/3 `route -A inet6 | grep ::/0 | awk &apos;NR==1&#123;print&quot;gw &quot;$2&quot; dev &quot;$7&#125;&apos;` &gt; /dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure></p>
<p>再就是得配置replay消息能从pppoe-wan到达br-lan, 所以最终使用下列配置(也记得去掉IPv6 ULA-Prefix):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">config dhcp &apos;wan&apos;</span><br><span class="line">        option interface &apos;wan&apos;</span><br><span class="line">        option ignore &apos;1&apos;</span><br><span class="line">        option ndp &apos;relay&apos;</span><br><span class="line">        option ra &apos;relay&apos;</span><br><span class="line">        option master &apos;1&apos;</span><br><span class="line">config dhcp &apos;lan&apos;</span><br><span class="line">        option interface &apos;lan&apos;</span><br><span class="line">        option start &apos;100&apos;</span><br><span class="line">        option limit &apos;150&apos;</span><br><span class="line">        option leasetime &apos;12h&apos;</span><br><span class="line">        list dhcp_option &apos;6,192.168.99.1&apos;</span><br><span class="line">        option ndp &apos;relay&apos;</span><br><span class="line">        option ra &apos;relay&apos;</span><br></pre></td></tr></table></figure></p>
<p>可新问题又来了, 这条路由总是过期, 如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2409:8a00:7805:da41::/64 dev br-lan  proto kernel  metric 256  expires 259019sec</span><br></pre></td></tr></table></figure></p>
<p>重新执行一下上面的命令又能恢复, 太不稳定了, 还是转回ipv6 NAT模式吧.</p>
<h2 id="20180902更新"><a href="#20180902更新" class="headerlink" title="20180902更新"></a>20180902更新</h2><p>上述静态路由过期的问题原因找到, 原因是需要添加metric</p>
<ul>
<li>route -A inet6 add 2409:8a00:7805:d9b1::/64 dev br-lan               # 会有过期时间</li>
<li>route -A inet6 add 2409:8a00:7805:d9b1::/64 dev br-lan metric 1 # 无过期时间<br>或者使用ip命令它添加的无过期时间 - ip -6 route del 2409:8a00:7805:d9b1::/64 dev br-lan</li>
</ul>
<h2 id="转试NAT6"><a href="#转试NAT6" class="headerlink" title="转试NAT6"></a>转试NAT6</h2><p>上面使用replay时的bug解不了, 无奈之下, 只好使用NAT6, 外面访问不了内网就访问不了吧, 起码可以内网访问外网啊.</p>
<ul>
<li><p>在/etc/config/network中配置了ula_prefix=2001:192:168:99::/64, 这时路由器上的br-lan除了ISP分配的IP之外, 也会多时我们这个自己配置的地址: 2001:192:168:99:0:0:0:1/64</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">config globals &apos;globals&apos;</span><br><span class="line">        option ula_prefix &apos;2001:192:168:99::/64&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改/etc/config/dhcp将relay模式改到server模式即NAT模式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">config dhcp &apos;lan&apos;</span><br><span class="line">        option interface &apos;lan&apos;</span><br><span class="line">        option start &apos;100&apos;</span><br><span class="line">        option limit &apos;150&apos;</span><br><span class="line">        option leasetime &apos;12h&apos;</span><br><span class="line">        list dhcp_option &apos;6,192.168.99.1&apos;</span><br><span class="line">        option dhcpv6 &apos;server&apos;</span><br><span class="line">        option ra_management &apos;2&apos;</span><br><span class="line">        option ra &apos;server&apos;</span><br><span class="line">        option ra_default &apos;1&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在/etc/firewall.user中添加下列SNAT规则</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip6tables -t nat -I POSTROUTING -s `uci get network.globals.ula_prefix` -j MASQUERADE &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">route -A inet6 add 2000::/3 `route -A inet6 | grep ::/0 | awk &apos;NR==1&#123;print&quot;gw &quot;$2&quot; dev &quot;$7&#125;&apos;` &gt; /dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>之后, 内网机器随便手动配置一个IP如2001:192:168:99:0:0:0:3/64并添加默认路由之后就可以访问外网了.<br>如果想外网访问2001:192:168:99::3/64, 是不能够使用下面的neigh proxy方式的, 因为网段和ISP分配的可路由网段根据就不一样嘛. 唯一的办法其实就是做DNAT<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv6.conf.all.proxy_ndp=1</span><br><span class="line">ip -6 neigh add proxy 2001:192:168:99:0:0:0:3 dev pppoe-wan</span><br><span class="line">ip -6 neigh show proxy</span><br></pre></td></tr></table></figure></p>
<h2 id="VPS不支持IPv6时如何使用IPv6"><a href="#VPS不支持IPv6时如何使用IPv6" class="headerlink" title="VPS不支持IPv6时如何使用IPv6"></a>VPS不支持IPv6时如何使用IPv6</h2><p>VPS若不支持IPv6, 可以通过tunnelbroker来配置6in4隧道支持IPv6, 但前提是VPS要能支持配置允许proto-41流量通过 (iptables -A INPUT -p 41 -j ACCEPT), 目前google cloud VPS是无法配置这个的.<br>若您的VPS支持这个, 可以继续. 先在<a href="https://www.tunnelbroker.net/" target="_blank" rel="external">https://www.tunnelbroker.net/</a> 登录后在’User Functions -&gt; Create Regular Tunnel’菜单创建 Create Regular Tunnel, 然后:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF | sudo tee -a /etc/network/interfaces</span><br><span class="line">auto he-ipv6</span><br><span class="line">iface he-ipv6 inet6 v4tunnel</span><br><span class="line">        address 2001:470:a:xx::2</span><br><span class="line">        netmask 64</span><br><span class="line">        endpoint 216.218.226.xx</span><br><span class="line">        local 162.xx.xx.xx</span><br><span class="line">        ttl 255</span><br><span class="line">        gateway 2001:470:a:4c4::1</span><br><span class="line">EOF</span><br><span class="line">cat &lt;&lt; EOF | sudo tee -a /etc/sysctl.conf</span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 0</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 0</span><br><span class="line">net.ipv6.conf.lo.disable_ipv6 = 0</span><br><span class="line">EOF</span><br><span class="line">sudo sysctl -p</span><br><span class="line">sudo apt install -y ifupdown</span><br><span class="line">sudo ifup he-ipv6</span><br><span class="line"># can&apos;t do tunnelbroker as 6in4 is unsupported (NAT/gateways won&apos;t pass proto-41)</span><br><span class="line">#sudo iptables -t nat -A PREROUTING -p 41 -d &lt;VPS-IP&gt; -j DNAT --to-destination &lt;tunnelbroker-ip&gt;</span><br><span class="line">#sudo iptables -t nat -A POSTROUTING -p 41 -d &lt;tunnelbroker-ip&gt; -j SNAT --to-source &lt;VPS-IP&gt;</span><br><span class="line">#sudo iptables -A INPUT -p 41 -j ACCEPT</span><br></pre></td></tr></table></figure>
<p>使用tunnelbroker需要VPS有公网IPv4地址, 若没有, 可以使用miredo(sudo apt-get install miredo), 但前提也是要防火墙允许proto-41的流量</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/03/IPv6来啦/" data-id="cjqc2cuuw0002d4bpvclixoof" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Using-kubeadm-to-deploy-k8s" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/13/Using-kubeadm-to-deploy-k8s/" class="article-date">
  <time datetime="2018-07-13T07:30:57.000Z" itemprop="datePublished">2018-07-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/07/13/Using-kubeadm-to-deploy-k8s/">Using kubeadm to deploy k8s</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (作者：张华 发表于：2018-07-13)</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; |sudo tee /etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 6A030B21BA07F4FB</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install docker.io kubelet kubeadm kubectl kubernetes-cni</span><br><span class="line"></span><br><span class="line">sudo usermod -aG docker `whoami`</span><br><span class="line">sudo systemctl enable docker.service</span><br><span class="line">sudo systemctl enable kubelet.service</span><br><span class="line">echo &apos;KUBELET_EXTRA_ARGS=--fail-swap-on=false&apos; |sudo tee /etc/default/kubelet</span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">#sudo swapoff -a</span><br><span class="line"></span><br><span class="line">#before this step, kubelet can&apos;t be up because no file /var/lib/kubelet/config.yaml</span><br><span class="line">sudo kubeadm reset</span><br><span class="line">sudo kubeadm init --pod-network-cidr 10.244.0.0/16 --ignore-preflight-errors=swap --kubernetes-version=v1.11.0</span><br><span class="line">sudo systemctl status kubelet</span><br><span class="line"></span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">#https://docs.projectcalico.org/v3.1/getting-started/kubernetes/</span><br><span class="line">wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml</span><br><span class="line">sed -i &apos;s/192.168.0.0/10.244.0.0/g&apos; ./calico.yaml</span><br><span class="line">kubectl apply -f ./calico.yaml</span><br><span class="line"></span><br><span class="line">kubectl get nodes</span><br><span class="line">kubectl get pods --all-namespaces</span><br><span class="line">kubectl get services --all-namespaces</span><br><span class="line">kubectl cluster-info</span><br><span class="line"></span><br><span class="line">#test</span><br><span class="line">kubectl run nginx --image=nginx</span><br><span class="line">kubectl expose deployment nginx --port=80 --target-port=80 --name=nginx-svc</span><br><span class="line"></span><br><span class="line">git clone https://github.com/kubernetes/kubernetes.github.io</span><br><span class="line">kubectl create -f ./content/en/examples/application/guestbook/</span><br><span class="line">#kubectl delete -f ./content/en/examples/application/guestbook/</span><br><span class="line"></span><br><span class="line">#scale test</span><br><span class="line">kubectl scale rc frontend --replicas=4</span><br><span class="line">kubectl get pods --all-namespaces</span><br><span class="line"></span><br><span class="line">#rolling-update for ReplicationController and apply for Deployment</span><br><span class="line">#kubectl rolling-update frontend --update-period=10s -f ./deploymentv2</span><br><span class="line">#kubectl rolling-update frontend --rollback</span><br><span class="line">kubectl get deployment frontend</span><br><span class="line">kubectl rollout history deployment/frontend</span><br><span class="line">kubectl rollout undo deployment/frontend</span><br><span class="line"></span><br><span class="line">#the single task</span><br><span class="line">$ cat pi.yaml</span><br><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: pi</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: pi</span><br><span class="line">    spec:</span><br><span class="line">      restartPolicy: Never</span><br><span class="line">      containers:</span><br><span class="line">      - name: pi</span><br><span class="line">        image: perl</span><br><span class="line">        command: [&quot;perl&quot;, &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;]</span><br><span class="line">$ kubectl get job</span><br><span class="line">NAME      DESIRED   SUCCESSFUL   AGE</span><br><span class="line">pi        1         1            2m</span><br><span class="line">$ kubectl get pod |grep pi-</span><br><span class="line">pi-mqjz8                                           0/1       Completed     0          2m</span><br><span class="line">$ kubectl logs pi-mqjz8</span><br><span class="line">3.1415926...</span><br><span class="line"></span><br><span class="line">#the cron task</span><br><span class="line">$ cat cron-pi.yaml</span><br><span class="line">apiVersion: batch/v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: pi</span><br><span class="line">spec:</span><br><span class="line">  schedule: &quot;*/1 * * * *&quot;</span><br><span class="line">  jobTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      template:</span><br><span class="line">        metadata:</span><br><span class="line">          name: pi</span><br><span class="line">        spec:</span><br><span class="line">          restartPolicy: OnFailure</span><br><span class="line">          containers:</span><br><span class="line">          - name: pi</span><br><span class="line">            image: perl</span><br><span class="line">            command: [&quot;perl&quot;, &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;]</span><br><span class="line">$ kubectl get cronjob pi</span><br><span class="line">NAME      SCHEDULE      SUSPEND   ACTIVE    LAST SCHEDULE   AGE</span><br><span class="line">pi        */1 * * * *   False     0         &lt;none&gt;          28s</span><br><span class="line"></span><br><span class="line">#daemonSet</span><br><span class="line">$ kubectl get daemonset</span><br><span class="line">NAME                                         DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR                        AGE</span><br><span class="line">nginx-ingress-kubernetes-worker-controller   3         3         0         3            0           juju-application=kubernetes-worker   7d</span><br><span class="line"></span><br><span class="line">#storage</span><br><span class="line">$ cat hostpath.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pd</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: busybox</span><br><span class="line">    name: test-container</span><br><span class="line">    command:</span><br><span class="line">    - sleep</span><br><span class="line">    - &quot;3600&quot;</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /test-pd</span><br><span class="line">      name: test-volume</span><br><span class="line">  volumes:</span><br><span class="line">  - name: test-volume</span><br><span class="line">    hostPath:</span><br><span class="line">      # directory location on host</span><br><span class="line">      path: /tmp</span><br><span class="line">      # this field is optional</span><br><span class="line">      type: Directory</span><br><span class="line">kubectl create -f hostpath.yaml</span><br><span class="line">kubectl exec -it test-pd -- /bin/sh</span><br><span class="line">kubectl exec test-pd -- ls /test-pd</span><br><span class="line"></span><br><span class="line">#secret</span><br><span class="line">$ cat secret.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: pass</span><br><span class="line">type: Opaque</span><br><span class="line">data:</span><br><span class="line">  pass: cGFzc3dvcmQ=</span><br><span class="line">$ cat secret-pd.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: secret-pd</span><br><span class="line">spec:</span><br><span class="line">  restartPolicy: Never</span><br><span class="line">  containers:</span><br><span class="line">  - image: busybox</span><br><span class="line">    name: secret-container</span><br><span class="line">    command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]</span><br><span class="line">    env:</span><br><span class="line">      - name: DB_PASS</span><br><span class="line">        valueFrom:</span><br><span class="line">          secretKeyRef:</span><br><span class="line">            name: pass</span><br><span class="line">            key: pass</span><br><span class="line">$ kubectl logs secret-pd |grep DB_PASS</span><br><span class="line">DB_PASS=password</span><br><span class="line"></span><br><span class="line">#ingress</span><br><span class="line">$ kubectl get pods --all-namespaces |grep ingress</span><br><span class="line">default       nginx-ingress-kubernetes-worker-controller-2fkdr   1/1       Running     0          23h</span><br><span class="line">default       nginx-ingress-kubernetes-worker-controller-2khgp   1/1       Running     0          23h</span><br><span class="line">default       nginx-ingress-kubernetes-worker-controller-4c7kr   1/1       Running     0          23h</span><br><span class="line">kubectl run echoheaders --image=gcr.io/google_containers/echoserver:1.4 --replicas=1 --port=8080</span><br><span class="line">ubuntu@zhhuabj-bastion:~/work/k8s$ kubectl expose deployment echoheaders --port=80 --target-port=8080 --name=echoheaders</span><br><span class="line">$ cat ingress.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: echomap</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: foo.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /foo</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: echheaders</span><br><span class="line">          servicePort: 80</span><br><span class="line">kubectl exec -it nginx-ingress-kubernetes-worker-controller-2fkdr -- cat /etc/nginx/nginx.conf</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">$ sudo kubeadm init --pod-network-cidr 10.244.0.0/16 --ignore-preflight-errors=swap --kubernetes-version=v1.11.0</span><br><span class="line">I0713 06:47:50.169410     362 feature_gate.go:230] feature gates: &amp;&#123;map[]&#125;</span><br><span class="line">[init] using Kubernetes version: v1.11.0</span><br><span class="line">[preflight] running pre-flight checks</span><br><span class="line">	[WARNING Swap]: running with swap on is not supported. Please disable swap</span><br><span class="line">I0713 06:47:50.213902     362 kernel_validator.go:81] Validating kernel version</span><br><span class="line">I0713 06:47:50.213996     362 kernel_validator.go:96] Validating kernel config</span><br><span class="line">	[WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 17.12.1-ce. Max validated version: 17.03</span><br><span class="line">[preflight/images] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight/images] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class="line">[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[preflight] Activating the kubelet service</span><br><span class="line">[certificates] Generated ca certificate and key.</span><br><span class="line">[certificates] Generated apiserver certificate and key.</span><br><span class="line">[certificates] apiserver serving cert is signed for DNS names [voltorb kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.230.56.118]</span><br><span class="line">[certificates] Generated apiserver-kubelet-client certificate and key.</span><br><span class="line">[certificates] Generated sa key and public key.</span><br><span class="line">[certificates] Generated front-proxy-ca certificate and key.</span><br><span class="line">[certificates] Generated front-proxy-client certificate and key.</span><br><span class="line">[certificates] Generated etcd/ca certificate and key.</span><br><span class="line">[certificates] Generated etcd/server certificate and key.</span><br><span class="line">[certificates] etcd/server serving cert is signed for DNS names [voltorb localhost] and IPs [127.0.0.1 ::1]</span><br><span class="line">[certificates] Generated etcd/peer certificate and key.</span><br><span class="line">[certificates] etcd/peer serving cert is signed for DNS names [voltorb localhost] and IPs [10.230.56.118 127.0.0.1 ::1]</span><br><span class="line">[certificates] Generated etcd/healthcheck-client certificate and key.</span><br><span class="line">[certificates] Generated apiserver-etcd-client certificate and key.</span><br><span class="line">[certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot;</span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot;</span><br><span class="line">[controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span><br><span class="line">[controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br><span class="line">[controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br><span class="line">[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br><span class="line">[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[init] this might take a minute or longer if the control plane images have to be pulled</span><br><span class="line">[apiclient] All control plane components are healthy after 89.502565 seconds</span><br><span class="line">[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.11&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[markmaster] Marking the node voltorb as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class="line">[markmaster] Marking the node voltorb as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;voltorb&quot; as an annotation</span><br><span class="line">[bootstraptoken] using token: 1yz24f.1hv4qn59rjxgj7cb</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 10.230.56.118:6443 --token 1yz24f.1hv4qn59rjxgj7cb --discovery-token-ca-cert-hash sha256:12dde7a18134d6d2effd66b17ad4e9b6b008ddfaa2c2d82232164e296d98ff0f</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br></pre></td><td class="code"><pre><span class="line">root@vps:~# ps -ef |grep kube</span><br><span class="line">root       704     1 99 06:48 ?        02:10:08 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=cgroupfs --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d --network-plugin=cni --resolv-conf=/run/systemd/resolve/resolv.conf --fail-swap-on=false</span><br><span class="line">root      1169  1146  4 06:49 ?        00:01:51 etcd --advertise-client-urls=https://127.0.0.1:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --initial-advertise-peer-urls=https://127.0.0.1:2380 --initial-cluster=vps=https://127.0.0.1:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379 --listen-peer-urls=https://127.0.0.1:2380 --name=vps --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">root      1207  1190 10 06:49 ?        00:04:11 kube-apiserver --authorization-mode=Node,RBAC --advertise-address=10.230.56.118 --allow-privileged=true --client-ca-file=/etc/kubernetes/pki/ca.crt --disable-admission-plugins=PersistentVolumeLabel --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key</span><br><span class="line">root      1339  1320 14 06:49 ?        00:05:28 kube-controller-manager --address=127.0.0.1 --allocate-node-cidrs=true --cluster-cidr=10.244.0.0/16 --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --node-cidr-mask-size=24 --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --use-service-account-credentials=true</span><br><span class="line">root      1394  1365  2 06:49 ?        00:01:05 kube-scheduler --address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true</span><br><span class="line">root      1858  1840  0 06:49 ?        00:00:20 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf</span><br><span class="line">root     11432 11394  0 07:10 ?        00:00:01 /usr/bin/kube-controllers</span><br><span class="line">root     20581 19399  0 07:28 pts/1    00:00:00 grep --color=auto kube</span><br><span class="line"></span><br><span class="line">root@vps:~# cat /var/lib/kubelet/kubeadm-flags.env</span><br><span class="line">KUBELET_KUBEADM_ARGS=--cgroup-driver=cgroupfs --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d --network-plugin=cni --resolv-conf=/run/systemd/resolve/resolv.conf</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~# cat /var/lib/kubelet/config.yaml</span><br><span class="line">address: 0.0.0.0</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">authentication:</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: false</span><br><span class="line">  webhook:</span><br><span class="line">    cacheTTL: 2m0s</span><br><span class="line">    enabled: true</span><br><span class="line">  x509:</span><br><span class="line">    clientCAFile: /etc/kubernetes/pki/ca.crt</span><br><span class="line">authorization:</span><br><span class="line">  mode: Webhook</span><br><span class="line">  webhook:</span><br><span class="line">    cacheAuthorizedTTL: 5m0s</span><br><span class="line">    cacheUnauthorizedTTL: 30s</span><br><span class="line">cgroupDriver: cgroupfs</span><br><span class="line">cgroupsPerQOS: true</span><br><span class="line">clusterDNS:</span><br><span class="line">- 10.96.0.10</span><br><span class="line">clusterDomain: cluster.local</span><br><span class="line">containerLogMaxFiles: 5</span><br><span class="line">containerLogMaxSize: 10Mi</span><br><span class="line">contentType: application/vnd.kubernetes.protobuf</span><br><span class="line">cpuCFSQuota: true</span><br><span class="line">cpuManagerPolicy: none</span><br><span class="line">cpuManagerReconcilePeriod: 10s</span><br><span class="line">enableControllerAttachDetach: true</span><br><span class="line">enableDebuggingHandlers: true</span><br><span class="line">enforceNodeAllocatable:</span><br><span class="line">- pods</span><br><span class="line">eventBurst: 10</span><br><span class="line">eventRecordQPS: 5</span><br><span class="line">evictionHard:</span><br><span class="line">  imagefs.available: 15%</span><br><span class="line">  memory.available: 100Mi</span><br><span class="line">  nodefs.available: 10%</span><br><span class="line">  nodefs.inodesFree: 5%</span><br><span class="line">evictionPressureTransitionPeriod: 5m0s</span><br><span class="line">failSwapOn: true</span><br><span class="line">fileCheckFrequency: 20s</span><br><span class="line">hairpinMode: promiscuous-bridge</span><br><span class="line">healthzBindAddress: 127.0.0.1</span><br><span class="line">healthzPort: 10248</span><br><span class="line">httpCheckFrequency: 20s</span><br><span class="line">imageGCHighThresholdPercent: 85</span><br><span class="line">imageGCLowThresholdPercent: 80</span><br><span class="line">imageMinimumGCAge: 2m0s</span><br><span class="line">iptablesDropBit: 15</span><br><span class="line">iptablesMasqueradeBit: 14</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">kubeAPIBurst: 10</span><br><span class="line">kubeAPIQPS: 5</span><br><span class="line">makeIPTablesUtilChains: true</span><br><span class="line">maxOpenFiles: 1000000</span><br><span class="line">maxPods: 110</span><br><span class="line">nodeStatusUpdateFrequency: 10s</span><br><span class="line">oomScoreAdj: -999</span><br><span class="line">podPidsLimit: -1</span><br><span class="line">port: 10250</span><br><span class="line">registryBurst: 10</span><br><span class="line">registryPullQPS: 5</span><br><span class="line">resolvConf: /etc/resolv.conf</span><br><span class="line">rotateCertificates: true</span><br><span class="line">runtimeRequestTimeout: 2m0s</span><br><span class="line">serializeImagePulls: true</span><br><span class="line">staticPodPath: /etc/kubernetes/manifests</span><br><span class="line">streamingConnectionIdleTimeout: 4h0m0s</span><br><span class="line">syncFrequency: 1m0s</span><br><span class="line">volumeStatsAggPeriod: 1m0s</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~# cat /etc/kubernetes/admin.conf</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: xxx=</span><br><span class="line">    server: https://10.230.56.118:6443</span><br><span class="line">  name: kubernetes</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: kubernetes</span><br><span class="line">    user: kubernetes-admin</span><br><span class="line">  name: kubernetes-admin@kubernetes</span><br><span class="line">current-context: kubernetes-admin@kubernetes</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: kubernetes-admin</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: xxx=</span><br><span class="line">    client-key-data: xxx=</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~# cat /etc/kubernetes/kubelet.conf</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: xxx=</span><br><span class="line">    server: https://10.230.56.118:6443</span><br><span class="line">  name: kubernetes</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: kubernetes</span><br><span class="line">    user: system:node:vps</span><br><span class="line">  name: system:node:vps@kubernetes</span><br><span class="line">current-context: system:node:vps@kubernetes</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: system:node:vps</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: xxx=</span><br><span class="line">    client-key-data: xxx==</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~# cat /etc/kubernetes/controller-manager.conf</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: xxx=</span><br><span class="line">    server: https://10.230.56.118:6443</span><br><span class="line">  name: kubernetes</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: kubernetes</span><br><span class="line">    user: system:kube-controller-manager</span><br><span class="line">  name: system:kube-controller-manager@kubernetes</span><br><span class="line">current-context: system:kube-controller-manager@kubernetes</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: system:kube-controller-manager</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: xxx==</span><br><span class="line">    client-key-data: xxx=</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~# cat /etc/kubernetes/scheduler.conf</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: xxx=</span><br><span class="line">    server: https://10.230.56.118:6443</span><br><span class="line">  name: kubernetes</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: kubernetes</span><br><span class="line">    user: system:kube-scheduler</span><br><span class="line">  name: system:kube-scheduler@kubernetes</span><br><span class="line">current-context: system:kube-scheduler@kubernetes</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: system:kube-scheduler</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: xxx==</span><br><span class="line">    client-key-data: xxx==</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~# cat /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    component: kube-apiserver</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: kube-apiserver</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - command:</span><br><span class="line">    - kube-apiserver</span><br><span class="line">    - --authorization-mode=Node,RBAC</span><br><span class="line">    - --advertise-address=10.230.56.118</span><br><span class="line">    - --allow-privileged=true</span><br><span class="line">    - --client-ca-file=/etc/kubernetes/pki/ca.crt</span><br><span class="line">    - --disable-admission-plugins=PersistentVolumeLabel</span><br><span class="line">    - --enable-admission-plugins=NodeRestriction</span><br><span class="line">    - --enable-bootstrap-token-auth=true</span><br><span class="line">    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt</span><br><span class="line">    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key</span><br><span class="line">    - --etcd-servers=https://127.0.0.1:2379</span><br><span class="line">    - --insecure-port=0</span><br><span class="line">    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt</span><br><span class="line">    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key</span><br><span class="line">    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname</span><br><span class="line">    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt</span><br><span class="line">    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key</span><br><span class="line">    - --requestheader-allowed-names=front-proxy-client</span><br><span class="line">    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">    - --requestheader-extra-headers-prefix=X-Remote-Extra-</span><br><span class="line">    - --requestheader-group-headers=X-Remote-Group</span><br><span class="line">    - --requestheader-username-headers=X-Remote-User</span><br><span class="line">    - --secure-port=6443</span><br><span class="line">    - --service-account-key-file=/etc/kubernetes/pki/sa.pub</span><br><span class="line">    - --service-cluster-ip-range=10.96.0.0/12</span><br><span class="line">    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt</span><br><span class="line">    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key</span><br><span class="line">    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.0</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    livenessProbe:</span><br><span class="line">      failureThreshold: 8</span><br><span class="line">      httpGet:</span><br><span class="line">        host: 10.230.56.118</span><br><span class="line">        path: /healthz</span><br><span class="line">        port: 6443</span><br><span class="line">        scheme: HTTPS</span><br><span class="line">      initialDelaySeconds: 15</span><br><span class="line">      timeoutSeconds: 15</span><br><span class="line">    name: kube-apiserver</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: 250m</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /etc/kubernetes/pki</span><br><span class="line">      name: k8s-certs</span><br><span class="line">      readOnly: true</span><br><span class="line">    - mountPath: /etc/ssl/certs</span><br><span class="line">      name: ca-certs</span><br><span class="line">      readOnly: true</span><br><span class="line">    - mountPath: /usr/share/ca-certificates</span><br><span class="line">      name: usr-share-ca-certificates</span><br><span class="line">      readOnly: true</span><br><span class="line">    - mountPath: /usr/local/share/ca-certificates</span><br><span class="line">      name: usr-local-share-ca-certificates</span><br><span class="line">      readOnly: true</span><br><span class="line">    - mountPath: /etc/ca-certificates</span><br><span class="line">      name: etc-ca-certificates</span><br><span class="line">      readOnly: true</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  priorityClassName: system-cluster-critical</span><br><span class="line">  volumes:</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /etc/kubernetes/pki</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: k8s-certs</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /etc/ssl/certs</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: ca-certs</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /usr/share/ca-certificates</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: usr-share-ca-certificates</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /usr/local/share/ca-certificates</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: usr-local-share-ca-certificates</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /etc/ca-certificates</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: etc-ca-certificates</span><br><span class="line">status: &#123;&#125;</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~# cat /etc/kubernetes/manifests/kube-controller-manager.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    component: kube-controller-manager</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: kube-controller-manager</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - command:</span><br><span class="line">    - kube-controller-manager</span><br><span class="line">    - --address=127.0.0.1</span><br><span class="line">    - --allocate-node-cidrs=true</span><br><span class="line">    - --cluster-cidr=10.244.0.0/16</span><br><span class="line">    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt</span><br><span class="line">    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key</span><br><span class="line">    - --controllers=*,bootstrapsigner,tokencleaner</span><br><span class="line">    - --kubeconfig=/etc/kubernetes/controller-manager.conf</span><br><span class="line">    - --leader-elect=true</span><br><span class="line">    - --node-cidr-mask-size=24</span><br><span class="line">    - --root-ca-file=/etc/kubernetes/pki/ca.crt</span><br><span class="line">    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key</span><br><span class="line">    - --use-service-account-credentials=true</span><br><span class="line">    image: k8s.gcr.io/kube-controller-manager-amd64:v1.11.0</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    livenessProbe:</span><br><span class="line">      failureThreshold: 8</span><br><span class="line">      httpGet:</span><br><span class="line">        host: 127.0.0.1</span><br><span class="line">        path: /healthz</span><br><span class="line">        port: 10252</span><br><span class="line">        scheme: HTTP</span><br><span class="line">      initialDelaySeconds: 15</span><br><span class="line">      timeoutSeconds: 15</span><br><span class="line">    name: kube-controller-manager</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: 200m</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /usr/share/ca-certificates</span><br><span class="line">      name: usr-share-ca-certificates</span><br><span class="line">      readOnly: true</span><br><span class="line">    - mountPath: /usr/local/share/ca-certificates</span><br><span class="line">      name: usr-local-share-ca-certificates</span><br><span class="line">      readOnly: true</span><br><span class="line">    - mountPath: /etc/ca-certificates</span><br><span class="line">      name: etc-ca-certificates</span><br><span class="line">      readOnly: true</span><br><span class="line">    - mountPath: /etc/kubernetes/pki</span><br><span class="line">      name: k8s-certs</span><br><span class="line">      readOnly: true</span><br><span class="line">    - mountPath: /etc/ssl/certs</span><br><span class="line">      name: ca-certs</span><br><span class="line">      readOnly: true</span><br><span class="line">    - mountPath: /etc/kubernetes/controller-manager.conf</span><br><span class="line">      name: kubeconfig</span><br><span class="line">      readOnly: true</span><br><span class="line">    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec</span><br><span class="line">      name: flexvolume-dir</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  priorityClassName: system-cluster-critical</span><br><span class="line">  volumes:</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /etc/kubernetes/pki</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: k8s-certs</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /etc/ssl/certs</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: ca-certs</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /etc/kubernetes/controller-manager.conf</span><br><span class="line">      type: FileOrCreate</span><br><span class="line">    name: kubeconfig</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: flexvolume-dir</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /usr/share/ca-certificates</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: usr-share-ca-certificates</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /usr/local/share/ca-certificates</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: usr-local-share-ca-certificates</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /etc/ca-certificates</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: etc-ca-certificates</span><br><span class="line">status: &#123;&#125;</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~# cat /etc/kubernetes/manifests/kube-scheduler.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    component: kube-scheduler</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: kube-scheduler</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - command:</span><br><span class="line">    - kube-scheduler</span><br><span class="line">    - --address=127.0.0.1</span><br><span class="line">    - --kubeconfig=/etc/kubernetes/scheduler.conf</span><br><span class="line">    - --leader-elect=true</span><br><span class="line">    image: k8s.gcr.io/kube-scheduler-amd64:v1.11.0</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    livenessProbe:</span><br><span class="line">      failureThreshold: 8</span><br><span class="line">      httpGet:</span><br><span class="line">        host: 127.0.0.1</span><br><span class="line">        path: /healthz</span><br><span class="line">        port: 10251</span><br><span class="line">        scheme: HTTP</span><br><span class="line">      initialDelaySeconds: 15</span><br><span class="line">      timeoutSeconds: 15</span><br><span class="line">    name: kube-scheduler</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        cpu: 100m</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /etc/kubernetes/scheduler.conf</span><br><span class="line">      name: kubeconfig</span><br><span class="line">      readOnly: true</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  priorityClassName: system-cluster-critical</span><br><span class="line">  volumes:</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /etc/kubernetes/scheduler.conf</span><br><span class="line">      type: FileOrCreate</span><br><span class="line">    name: kubeconfig</span><br><span class="line">status: &#123;&#125;</span><br><span class="line">root@vps:~#</span><br><span class="line">root@vps:~# cat /etc/kubernetes/manifests/etcd.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    component: etcd</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: etcd</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - command:</span><br><span class="line">    - etcd</span><br><span class="line">    - --advertise-client-urls=https://127.0.0.1:2379</span><br><span class="line">    - --cert-file=/etc/kubernetes/pki/etcd/server.crt</span><br><span class="line">    - --client-cert-auth=true</span><br><span class="line">    - --data-dir=/var/lib/etcd</span><br><span class="line">    - --initial-advertise-peer-urls=https://127.0.0.1:2380</span><br><span class="line">    - --initial-cluster=vps=https://127.0.0.1:2380</span><br><span class="line">    - --key-file=/etc/kubernetes/pki/etcd/server.key</span><br><span class="line">    - --listen-client-urls=https://127.0.0.1:2379</span><br><span class="line">    - --listen-peer-urls=https://127.0.0.1:2380</span><br><span class="line">    - --name=vps</span><br><span class="line">    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt</span><br><span class="line">    - --peer-client-cert-auth=true</span><br><span class="line">    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key</span><br><span class="line">    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">    - --snapshot-count=10000</span><br><span class="line">    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">    image: k8s.gcr.io/etcd-amd64:3.2.18</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    livenessProbe:</span><br><span class="line">      exec:</span><br><span class="line">        command:</span><br><span class="line">        - /bin/sh</span><br><span class="line">        - -ec</span><br><span class="line">        - ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">          --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key</span><br><span class="line">          get foo</span><br><span class="line">      failureThreshold: 8</span><br><span class="line">      initialDelaySeconds: 15</span><br><span class="line">      timeoutSeconds: 15</span><br><span class="line">    name: etcd</span><br><span class="line">    resources: &#123;&#125;</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /var/lib/etcd</span><br><span class="line">      name: etcd-data</span><br><span class="line">    - mountPath: /etc/kubernetes/pki/etcd</span><br><span class="line">      name: etcd-certs</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  priorityClassName: system-cluster-critical</span><br><span class="line">  volumes:</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /etc/kubernetes/pki/etcd</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: etcd-certs</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /var/lib/etcd</span><br><span class="line">      type: DirectoryOrCreate</span><br><span class="line">    name: etcd-data</span><br><span class="line">status: &#123;&#125;</span><br><span class="line">root@vps:~#</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/07/13/Using-kubeadm-to-deploy-k8s/" data-id="cjqc2cuv6000gd4bpsyke8edm" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-用OpenSSL做自签名的证书" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/13/用OpenSSL做自签名的证书/" class="article-date">
  <time datetime="2018-07-13T05:10:13.000Z" itemprop="datePublished">2018-07-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/07/13/用OpenSSL做自签名的证书/">用OpenSSL做自签名的证书</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者：张华  发表于：2014-04-18<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</p>
<p>(<a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )</p>
<p>加密技术回顾<br>非对称加密算法如RSA的特点如下:<br>1, 公钥加密私钥解密, 大家都可以用我的公钥给我发加密的数据了, 因为只有我有私钥才能解密.<br>2, 私钥加密公钥解密叫数字签名(例如所谓的UEFI secure boot就是在主板硬件里集成一些操作系统的公钥，由主板硬件去校验操作系统是法合法，但关键是微软把持了公钥的申请，主板硬件厂商没有提供界面让用法自定义公钥，尤其在移动领域很多win8的硬件根本不提供关闭secure boot的选项这样就造成只能安装win8一种系统）, 大家收到我用私钥加密后的数据, 看用公钥能不能打得开, 能打开说明这数据确实是由我所发的, 因为别人没有我的私钥不可能伪造这些数据.<br>非对称加密去处很费时间, 我们一般采用对称密钥算法如DES来加密, 但对称密钥的保存是一个问题.<br>所以我们可以采用非对称加密算法来加密先协商交换对称密钥, 这就叫SSL. 假设客户端A的公私钥对是(P1,V1), 服务端B的公私钥对是(P2,V2), A需要确认和它通信的是B, 那么SSL的过程是:<br>首先, A和B都持有对方的公钥.<br>step1, A-&gt;B: hello 是step2, B-&gt;A: 用V2加密过的P1（即用户证书，A就用P2解密出P1, 这种数字签名方式让A确定了和它通信的是B）<br>step3, A-&gt;B: ok<br>step4, B-&gt;A: 用V1加密的一段信息<br>step5, A-&gt;B: 用P1加密一个自动生成的对称密钥K（用之前的P1解密成功这段信息则认为B是可信的了）<br>step6, B-&gt;A: 用K加密的数据（之后两对密钥功能结束，由K来加解密数据）<br>总结一下, 这里(P2,V2)就是certificate authority (CA)用来给客户签名用的公私钥。<br>(P1,V1)是客户自己的公私钥，提交给CA，CA所做的事情就是上述step2用(P2,V2)来给客户的(P1,V1)签名，简单吧？<br>V2是CA公司要保密的，而P2就是公用CA证书要安装到客户端</p>
<p>用V2加密过（签名过）的P1，称为用户证书，和用户私钥V1连起一个文件后, 一般被安装在服务器端。</p>
<p>X.509证书是一些标准字段的集合, 是包含有关用户或设备及其相应公钥信息的一种非常通用的证书格式, 目前版本是3. 必要字段包括:<br>1, 版本号<br>2, 由CA给每一个证书分配的序列号;<br>3, 证书使用的签名算法<br>4, 证书的认证机构<br>5, 证书的有效日期<br>6, 证书的所有人的唯一标识<br>7, 认证机构使用私钥的数字签名<br>8, 公钥信息<br>不同于PGP证书任何人都可以扮演认证者的角色, X.509证书的认证者只能是CA或由CA指定的人.要获得一份X.509证书，必须请求CA发给你证书。用户提供自己的公钥，证明自己拥有相应的私钥，并提供有关自己的某些特定信息。然后在这些信息上数字签名，并将整个数据包(称为证书请求)发给CA。CA做一些努力来验证用户提供的信息是正确的，然后就生成证书并返回给用户。<br>OpenSSL对X.509的支持如下:<br>(1) 证书请求管理<br>(2) 证书生成<br>(3) 证书吊销及CRL管理<br>(4) X509名字管理<br>(5) 属性管理<br>(6) 扩展管理<br>(7) 验证及信任管理</p>
<p>用OpenSSL做自签名的证书(pem格式)步骤:<br>1, 先生成CA的公私钥<br>   mkdir CA &amp; cd CA<br>   mkdir newcerts private<br>   echo ‘01’ &gt; serial #会生成以为个数字为名字的pem文件, 且每个数字自增1<br>   touch index.txt #生成记录数据库<br>   使用配置文件, 由于openssl命令行参数太多, 为避免写太多, 就使用一个配置文件代替, 如<a href="https://github.com/openstack/nova/blob/master/nova/CA/openssl.cnf.tmpl" target="_blank" rel="external">https://github.com/openstack/nova/blob/master/nova/CA/openssl.cnf.tmpl</a><br>   生成(P2,V2), 这时候P2=cacert.pem, V2=private/cakey.pem<br>   openssl req -new -x509 -extensions v3_ca -keyout private/cakey.pem -out cacert.pem -days 365 -config ./openssl.cnf -batch -nodes<br>   查看证书信息, openssl x509 -in cacert.pem -noout -text<br>2, 生成<p1,v1>,即Certificate signing Reqeust(CSR), P1=req.pem, V1=key.pem<br>   openssl req -new -nodes -out req.pem -config ./openssl.cnf<br>3, 用CA的私钥V2为P1签名, 即在newcerts目录生成用户证书cert.pem, 并更新数据库文件index.txt及serail文件<br>   openssl ca -out cert.pem -config ./openssl.cnf -infiles req.pem<br>   查看证书信息, openssl x509 -in cert.pem -noout -text<br>4, 安装证书<br>   用户私钥key.pem(V1)和用V2加密过的用户公钥(cert.pem)安装到服务端(有的服务器碉要把这两个文件连成一个,可以执行: cat key.pem cert.pem &gt; key-cert.pem), 如:<br>   /home/httpd/ssl/cert.pem Site certificate<br>   /home/httpd/ssl/key.pem Site private key<br>   最后将CA的公钥P2=cacert.pem安装到客户端</p1,v1></p>
<p>在OpenStack PKI认证中：<br>1, Keystone产生了CA公私钥: CA.pem, CA.key<br>2, Keystone产生了用户公私钥: keystone.pub, keystone.key<br>3, Keystone产生了用户证书: keystone.pem (即使用CA.key对keystone.pub进行了签名)<br>假如nova要使用PKI认证的话：<br>1, CA端，即keystone端，安装有: CA.pem, CA.key, keystone.key, keystone.pem<br>2, 用户端，即nova端，安装有：keystone.pem<br>过程：<br>1, 用户拿用户名和密码去keystone认证，keystone将用户信息通过keystone.key进行签名后作为token返回用户<br>2, 用户用这一token去访问nova, nova拿到token后，使用keystone.pem解密。（而原来的UUID方式nova还得再拿token去keystone那边验证一下是否有效，所以使用PKI方式能减轻keystone的压力。</p>
<p>再举个例子，如在安装openconnect时生成证书：</p>
<p>sudo apt-get -y install build-essential pkg-config libgnutls28-dev libreadline-dev libseccomp-dev libwrap0-dev libnl-nf-3-dev liblz4-dev gnutls-bin</p>
<p>#Create CA certificate<br>mkdir -p /tmp/cert &amp;&amp; cd /tmp/cert<br>cat &gt; /tmp/cert/ca.tmpl &lt;&lt; EOF<br>cn = “sts CA”<br>organization = “sts CA”<br>serial = 1<br>expiration_days = 3650<br>ca<br>signing_key<br>cert_signing_key<br>crl_signing_key<br>EOF</p>
<p>#Generate CA secret KEY: V2<br>certtool –generate-privkey –outfile CA.key</p>
<p>#Generate CA certifice: P2 signed by V2<br>certtool –generate-self-signed –load-privkey CA.key –template ca.tmpl –outfile CA.pem</p>
<p>#Create User certificate (here is for VPN server)<br>cat &gt; /tmp/cert/vpnserver.tmpl &lt;&lt; EOF<br>cn = “sts vpn server”<br>organization = “sts”<br>expiration_days = 3650<br>signing_key<br>encryption_key<br>tls_www_server<br>EOF</p>
<p>#Generate User secret KEY: V1<br>certtool –generate-privkey –outfile vpnserver.key</p>
<p>#Generate User certificate: <p1 signed="" by="" v2=""><br>certtool –generate-certificate –load-privkey vpnserver.key –load-ca-certificate CA.pem –load-ca-privkey CA.key –template vpnserver.tmpl –outfile vpnserver.pem</p1></p>
<p>#CA.pem,vpnserver,pem,vpnserver.key need to be installed in vpnserver<br>sudo cp CA.pem /etc/ssl/certs/CA.pem<br>sudo cp vpnserver.pem /etc/ssl/private/vpnserver.pem<br>sudo cp vpnserver.key /etc/ssl/private/vpnserver.key<br>OpenStack创建CA的方法：</p>
<p>openssl genrsa -out /etc/keystone/ssl/private/cakey.pem 1024<br>openssl req -new -x509 -extensions v3_ca -key /etc/keystone/ssl/private/cakey.pem -out /etc/keystone/ssl/certs/ca.pem -days 3650 -config /etc/keystone/ssl/certs/openssl.conf -subj /C=US/ST=Unset/L=Unset/O=Unset/CN=localhost<br>openssl genrsa -out /etc/keystone/ssl/private/keystonekey.pem 1024<br>openssl req -key /etc/keystone/ssl/private/keystonekey.pem -new -out /etc/keystone/ssl/certs/req.pem -config /etc/keystone/ssl/certs/openssl.conf -subj /C=US/ST=Unset/L=Unset/O=Unset/CN=localhost<br>openssl ca -batch -out /etc/keystone/ssl/certs/keystone.pem -config /etc/keystone/ssl/certs/openssl.conf -days 3650d -cert /etc/keystone/ssl/certs/ca.pem -keyfile /etc/keystone/ssl/private/cakey.pem -infiles /etc/keystone/ssl/certs/req.pem</p>
<p>再看一个使用easy-rsa为openvpn生成证书的实例：</p>
<p>sudo apt-get install easy-rsa openssl<br>sudo cp -r /usr/share/easy-rsa/ /etc/openvpn<br>cd /etc/openvpn/easy-rsa<br>sudo chown -R <code>whoami</code>:root /etc/openvpn<br>mkdir /etc/openvpn/easy-rsa/keys<br>source ./vars<br>export KEY_COUNTRY=CN<br>export KEY_PROVINCE=BJ<br>export KEY_CITY=BJ<br>export KEY_ORG=sts<br>export KEY_OU=sts<br>export KEY_NAME=sts<br>export KEY_EMAIL=root@sts<br>export KEY_NAME=”server”<br>./clean-all<br>./build-ca<br>$ ls keys/<br>ca.crt  ca.key  index.txt  serial<br>./build-key-server server<br>$ ls keys/<br>01.pem  ca.key     index.txt.attr  serial      server.crt  server.key<br>ca.crt  index.txt  index.txt.old   serial.old  server.csr<br>cp /etc/openvpn/easy-rsa/keys/{server.crt,server.key,ca.crt} /etc/openvpn</p>
<p>#It’s ideal for each client connecting to the VPN to have its own unique certificate and key.</p>
<p>#This is preferable to generating one general certificate and key to use among all client devices.<br>./build-key client1<br>scp ubuntu@<server>:/etc/openvpn/easy-rsa/keys/client1.crt /etc/openvpn/<br>scp ubuntu@<server>:/etc/openvpn/easy-rsa/keys/client1.key /etc/openvpn/<br>scp ubuntu@<server>:/etc/openvpn/easy-rsa/keys/client.ovpn /etc/openvpn/<br>scp ubuntu@<server>:/etc/openvpn/easy-rsa/keys/ca.crt /etc/openvpn/</server></server></server></server></p>
<p>常见证书格式及转换</p>
<p>PKCS(Public-Key Cryptography Standards), 是由RSA实验室与其他安全系统开发商共同制定的一个公钥密码标准<br>X.509是常用的通用的证书格式, 所有的证书都符合PKI(Public Key Infrastructure)制定的的ITU-T X509国际标准<br>.cer/.crt是用于存储证书, 以二进制形式存储, 不含私钥<br>.pem跟.cer/.crt的区别是它以ascii来表示<br>pfx/p12用于存放个人证书/私钥, 他通常包含保护密码, 二进制存储, 转换如:openssl pkcs12 -export -clcerts -in server-cert.cer -inkey server-key.key -out server.p12<br>JKS和JCEKS是Java密钥库(KeyStore)的两种比较常见类型, 可以使用java提供的证书工具keytool(openssl和keytool都是可以用来管理证书的工具而已)进行转换(如:keytool -import -v -trustcacerts -storepass 123456 -alias server -file cacert.pem -keystore server.jks)</p>
<p>例如： k8s中的dashboard若不在浏览器里导入p12证书在采用RBAC授权时就会什么也看不到：</p>
<h1 id="generate-client-certificate-data"><a href="#generate-client-certificate-data" class="headerlink" title="generate client-certificate-data"></a>generate client-certificate-data</h1><p>grep ‘client-certificate-data’ /var/run/kubernetes/admin.kubeconfig | head -n 1 | awk ‘{print $2}’ | base64 -d &gt;&gt; kubecfg.crt</p>
<h1 id="generate-client-key-data"><a href="#generate-client-key-data" class="headerlink" title="generate client-key-data"></a>generate client-key-data</h1><p>grep ‘client-key-data’ /var/run/kubernetes/admin.kubeconfig | head -n 1 | awk ‘{print $2}’ | base64 -d &gt;&gt; kubecfg.key</p>
<h1 id="generate-p12"><a href="#generate-p12" class="headerlink" title="generate p12"></a>generate p12</h1><p>openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name “kubernetes-client”</p>
<p>非对称算法可以使用开源的GPG工具，可参考文档： <a href="http://wenku.baidu.com/link?url=d5fWoN46-y7581212dDUEt7dkUfFkriW0bKy_gwUjps4zpKH64jytimWDm-yfuKIwAtu0jFoWW_ocVTJhSeRb2_QQLUz8oIBQulU6jSI133" target="_blank" rel="external">http://wenku.baidu.com/link?url=d5fWoN46-y7581212dDUEt7dkUfFkriW0bKy_gwUjps4zpKH64jytimWDm-yfuKIwAtu0jFoWW_ocVTJhSeRb2_QQLUz8oIBQulU6jSI133</a></p>
<p>及：<br><a href="https://help.ubuntu.com/community/GnuPrivacyGuardHowto" target="_blank" rel="external">https://help.ubuntu.com/community/GnuPrivacyGuardHowto</a></p>
<p>sudo apt-get install rng-tools<br>sudo rngd -r /dev/urandom</p>
<p>sudo apt-get install gnupg-agent<br>killall -q gpg-agent<br>eval $(gpg-agent –daemon)</p>
<p>创建密钥对：gpg –gen-key， 如创建了：”Zhang Hua (zhhuabj) <a href="&#109;&#x61;&#x69;&#108;&#116;&#x6f;&#58;&#118;&#101;&#x72;&#x79;&#104;&#117;&#x61;&#x32;&#48;&#x30;&#54;&#64;&#x67;&#x6d;&#97;&#105;&#x6c;&#x2e;&#99;&#x6f;&#x6d;">&#118;&#101;&#x72;&#x79;&#104;&#117;&#x61;&#x32;&#48;&#x30;&#54;&#64;&#x67;&#x6d;&#97;&#105;&#x6c;&#x2e;&#99;&#x6f;&#x6d;</a>“</p>
<pre><code>export GPGKEY=A24B36AE
</code></pre><p>查看公钥：gpg –list-public<br>查看私钥：gpg –list-secret-key<br>查看签名：gpg –list-sig<br>查看公钥指纹：gpg –fingerprint $GPGKEY<br>提取公钥：gpg –armor –output public.key –export $GPGKEY  或者： gpg –export -a $GPGKEY &gt; public.key</p>
<p>提取私钥：gpg -a –export-secret-keys $KEYID &gt; customer-mirror.key<br>生成公钥回收证书，当私钥出问题时可将它上传密钥服务器声明公钥作废:<br>  gpg –output revoke.asc –gen-revoke $GPGKEY<br>  声明作废：gpg –keyserver Server Address –send-keys $GPGKEY</p>
<p>迁移KEY</p>
<p>gpg –output mygpgkey_pub.gpg –armor –export  $GPGKEY<br>gpg –output mygpgkey_sec.gpg –armor –export-secret-key $GPGKEY</p>
<p>gpg –import mygpgkey_pub.gpg<br>gpg –allow-secret-key-import –import mygpgkey_sec.gpg</p>
<p>上传公钥到密钥服务器，如：gpg –send-keys –keyserver keyserver.ubuntu.com $GPGKEY 或把公钥导成文本之后直接在<a href="http://keyserver.ubuntu.com/这里提交公钥。" target="_blank" rel="external">http://keyserver.ubuntu.com/这里提交公钥。</a></p>
<p>交互命令窗口：gpg –cert-digest-algo=SHA256 –edit-key $GPGKEY</p>
<p>给自己加密文件，加密是用公钥，gpg –encrypt -r veryhua2006@gmail.com test.txt, 会生成名为test.txt.gpg的加密文件<br>给自己解决文件，gpg –decrypt test.txt.gpg &gt; test.txt</p>
<p>给别人加密文件当然要先导入别人的公钥：gpg –import otherpublic.key<br>核对对方的公钥指纹：gpg –fingerprint other@gmail.com<br>为别人加密文件: gpg –encrypt –recipient other@gmail.com test.txt<br>对别人的公钥进行签名，这样别人知道是你发的： gpg –sign-key other@gmail.com</p>
<p>对文件进行签名： gpg –clearsign file<br>验证签名是否完整： gpg –verity file.asc</p>
<p>OpenPGP能用于加密邮件，将GPG指纹注册到launchpad中(<a href="https://launchpad.net/~zhhuabj)，这样launchpad将给你发签名或加密过的邮件，然后再用openPGP" target="_blank" rel="external">https://launchpad.net/~zhhuabj)，这样launchpad将给你发签名或加密过的邮件，然后再用openPGP</a> enabled的邮件客户端如thunderbird来接收解密邮件和验证签名。<br>thunderbird通过enigmail插件来支持OpenPGP, Configure OpenPGP support in Thunderbird under Enigmail-&gt;Preferences and add under GnuPG executable path. The path for GnuPG is /usr/bin/gpg.<br>如果不想用邮件客户端，直接用firefox来访问如gmail等webmail的话，安装firegpg插件即可。chrome不需要装插件直接支持pgp解密。</p>
<p>将GPG指纹（gpg –fingerprint)注册到launchpad中(<a href="https://launchpad.net/~zhhuabj)后会收到一封邮件，内容类似如下，将其存成一个文件，再解密(gpg" target="_blank" rel="external">https://launchpad.net/~zhhuabj)后会收到一封邮件，内容类似如下，将其存成一个文件，再解密(gpg</a> –decrypt file.txt)后就生成了一个验证链接如<a href="https://launchpad.net/token/Hc7J9x7kF55CHTZJs，点击验证即可。" target="_blank" rel="external">https://launchpad.net/token/Hc7J9x7kF55CHTZJs，点击验证即可。</a><br>—–BEGIN PGP MESSAGE—–<br>Version: GnuPG v1.4.11 (GNU/Linux)<br>…….<br>52gY/bZADAl0xhScHvvuYquGS3oApfgtNM3UJWXa<br>=ZgnD<br>—–END PGP MESSAGE—–</p>
<p>Signed Ubuntu Code of Conduct in <a href="https://launchpad.net/~zhhuabj，" target="_blank" rel="external">https://launchpad.net/~zhhuabj，</a><br>1, 先下载UbuntuCodeofConduct-2.0.txt, <a href="https://launchpad.net/codeofconduct/2.0/+download" target="_blank" rel="external">https://launchpad.net/codeofconduct/2.0/+download</a><br>2, gpg –clearsign UbuntuCodeofConduct-2.0.txt<br>3, 将生成的UbuntuCodeofConduct-2.0.txt.asc文件再上传至 <a href="https://launchpad.net/codeofconduct/2.0/+sign即可。" target="_blank" rel="external">https://launchpad.net/codeofconduct/2.0/+sign即可。</a></p>
<p>2014-5-23日添加，配置使用Google Authenticator服务</p>
<p>Google帐户支持密码+临时验证码的两阶段验证方式。<br>临时验证码也支持直接短信发到手机上，也可以在Android手机上安装Google Authenticator服务来接收临时验证码。<br>具体先在<a href="https://accounts.google.com/b/0/SmsAuthConfig页面配置，并先生成google服务端为安装了Google" target="_blank" rel="external">https://accounts.google.com/b/0/SmsAuthConfig页面配置，并先生成google服务端为安装了Google</a> Authenticator服务的客户端生成的密钥。然后再在Google Authenticator里输入这个密钥就可以实现一次一密了。</p>
<p>参考:<br><a href="http://www.blogjava.net/alwayscy/archive/2006/12/01/84852.html" target="_blank" rel="external">http://www.blogjava.net/alwayscy/archive/2006/12/01/84852.html</a><br><a href="http://blog.sina.com.cn/s/blog_9e9d2211010199yj.html" target="_blank" rel="external">http://blog.sina.com.cn/s/blog_9e9d2211010199yj.html</a></p>
<p><a href="http://lingxiankong.github.io/blog/2014/02/06/openstack-ssl/" target="_blank" rel="external">http://lingxiankong.github.io/blog/2014/02/06/openstack-ssl/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/07/13/用OpenSSL做自签名的证书/" data-id="cjqc2cuwr000xd4bpm0e2thwf" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Set-up-k8s-development-env-by-quqi99" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/10/Set-up-k8s-development-env-by-quqi99/" class="article-date">
  <time datetime="2018-07-10T09:47:57.000Z" itemprop="datePublished">2018-07-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/07/10/Set-up-k8s-development-env-by-quqi99/">Set up k8s development env (by quqi99)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (作者：张华 发表于：2018-07-10)</strong></p>
<h2 id="Sign-the-CLA"><a href="#Sign-the-CLA" class="headerlink" title="Sign the CLA"></a>Sign the CLA</h2><p>Sign via Hellosign - <a href="https://github.com/kubernetes/community/blob/master/CLA.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/CLA.md</a><br>then set email for github - <a href="https://github.com/settings/emails" target="_blank" rel="external">https://github.com/settings/emails</a><br>git config –global user.email “xxx@gmail.com”</p>
<h2 id="Run-local-k8s-via-source-code"><a href="#Run-local-k8s-via-source-code" class="headerlink" title="Run local k8s  via source code"></a>Run local k8s  via source code</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># Install some packages</span><br><span class="line">sudo apt install -y gcc make socat git build-essential</span><br><span class="line"></span><br><span class="line"># Install docker</span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br><span class="line">sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt-cache policy docker-ce</span><br><span class="line">sudo apt install docker-ce</span><br><span class="line"></span><br><span class="line"># Change default location of docker image</span><br><span class="line">service docker stop</span><br><span class="line">rsync -aXS /var/lib/docker/* /bak/.docker/</span><br><span class="line">rm -rf /var/lib/docker/*</span><br><span class="line">echo /bak/.docker/ /var/lib/docker none bind 0 0 &gt;&gt; /etc/fstab</span><br><span class="line">mount –a</span><br><span class="line">service docker start</span><br><span class="line"></span><br><span class="line"># Install etcd &gt; 3.2.13</span><br><span class="line">ETCD_VER=v3.2.18</span><br><span class="line">DOWNLOAD_URL=&quot;https://github.com/coreos/etcd/releases/download&quot;</span><br><span class="line">curl -L $&#123;DOWNLOAD_URL&#125;/$&#123;ETCD_VER&#125;/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz -o /tmp/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz</span><br><span class="line">tar xzvf /tmp/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz</span><br><span class="line">sudo /bin/cp -f etcd-$&#123;ETCD_VER&#125;-linux-amd64/&#123;etcd,etcdctl&#125; /usr/bin</span><br><span class="line">rm -rf /tmp/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz etcd-$&#123;ETCD_VER&#125;-linux-amd64</span><br><span class="line"></span><br><span class="line"># Install golang &gt; 1.10.2</span><br><span class="line">wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz</span><br><span class="line">sudo rm -rf /usr/lib/go &amp;&amp; sudo tar -C /usr/lib -xzf go1.10.3.linux-amd64.tar.gz</span><br><span class="line">export GOROOT=/usr/lib/go</span><br><span class="line">export GOPATH=/bak/golang</span><br><span class="line">export PATH=$GOROOT/bin:$GOPATH/bin:$PATH</span><br><span class="line"></span><br><span class="line"># Install and run kubernetes in local env - https://www.cnblogs.com/edisonxiang/p/6951787.html</span><br><span class="line">mkdir -p $GOPATH/src/k8s.io</span><br><span class="line">#go get -d k8s.io/kubernetes</span><br><span class="line">git clone git@github.com:zhhuabj/kubernetes.git $GOPATH/src/k8s.io/kubernetes</span><br><span class="line">#make GOGCFLAGS=&quot;-N -l&quot;  #Debug it</span><br><span class="line">sudo usermod -a -G docker $&#123;USER&#125;</span><br><span class="line">sudo systemctl restart docker.service</span><br><span class="line">sudo systemctl disable kubelet.service</span><br><span class="line">sudo systemctl stop kubelet.service</span><br><span class="line"></span><br><span class="line">#注意：一直不成功的原因是需要用小写true，它是区分大小写的</span><br><span class="line">KUBE_ENABLE_CLUSTER_DASHBOARD=true ./hack/local-up-cluster.sh</span><br><span class="line">#注意：加GO_OUT可避免再次编译</span><br><span class="line">GO_OUT=/bak/golang/src/k8s.io/kubernetes/_output/bin</span><br><span class="line">KUBE_ENABLE_CLUSTER_DASHBOARD=true ./hack/local-up-cluster.sh</span><br><span class="line"></span><br><span class="line"># Test local env</span><br><span class="line">export KUBECONFIG=/var/run/kubernetes/admin.kubeconfig</span><br><span class="line">cluster/kubectl.sh get pods --all-namespaces</span><br></pre></td></tr></table></figure>
<h2 id="github-process"><a href="#github-process" class="headerlink" title="github process"></a>github process</h2><p><a href="https://github.com/kubernetes/community/blob/master/contributors/guide/github-workflow.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/contributors/guide/github-workflow.md</a><br>k8s与openstack不一样，openstack使用gerrit来review code, 但是k8s使用github的PR机制。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">kubernetes提交PR的流程可以采用pull模型（Shared Repository Model，https://gist.github.com/seshness/3943237），也可以采用fork模型（https://www.cnblogs.com/edisonxiang/p/6951787.html）。我们采用fork模型：</span><br><span class="line"></span><br><span class="line"># Click &apos;Fork&apos; button to fork your own branch - https://github.com/kubernetes/kubernetes, then we have https://github.com/zhhuabj/kubernetes</span><br><span class="line">mkdir -p $GOPATH/src/k8s.io</span><br><span class="line">git clone git@github.com:zhhuabj/kubernetes.git $GOPATH/src/k8s.io/kubernetes</span><br><span class="line">cd kubernetes</span><br><span class="line">hack/local-up-cluster.sh</span><br><span class="line"></span><br><span class="line"># set up upstream branch</span><br><span class="line">git remote add upstream https://github.com/kubernetes/kubernetes.git</span><br><span class="line">git remote set-url --push upstream no_push</span><br><span class="line">git remote -v</span><br><span class="line"></span><br><span class="line"># Update our branch</span><br><span class="line">git fetch upstream</span><br><span class="line">git checkout master</span><br><span class="line">git rebase upstream/master</span><br><span class="line">#git pull upstream master</span><br><span class="line"></span><br><span class="line"># Add new branch myfeature</span><br><span class="line">git checkout -b myfeature</span><br><span class="line">git config --global user.email &quot;veryhua2006@gmail.com&quot;</span><br><span class="line">git config --global user.name &quot;zhhuabj&quot;</span><br><span class="line"># Add or Modify files</span><br><span class="line">...</span><br><span class="line">git add .</span><br><span class="line">git commit -a -F ./msg</span><br><span class="line">git commit --amend -a -F ./message</span><br><span class="line">git commit -m &quot;update&quot;</span><br><span class="line">git push origin myfeature</span><br><span class="line">git push origin :myfeature  #delete remote branch</span><br><span class="line"></span><br><span class="line"># Rebase unmerged PR into our repo</span><br><span class="line">git fetch upstream pull/56136/head:BRANCHNAME</span><br><span class="line"></span><br><span class="line"># Merge multiple local commits into a full commit by using &apos;git squash&apos;</span><br><span class="line">git log</span><br><span class="line">git rebase -i HEAD~6 把顶部的六个版本聚到一起进入编辑页面</span><br><span class="line">　　把需要压缩的日志前面的pick都改为s（squash的缩写）</span><br><span class="line">　　注意必须保留一个pick，如果将所有的pick都改为了s那就没有合并的载体了就会报如下错误</span><br><span class="line">　　依次输入CTRL+X Y ENTER三个命令完成编辑。</span><br><span class="line">　　最后Git Push orgin branchname</span><br><span class="line"></span><br><span class="line"># Pull Request - https://github.com/zhhuabj/kubernetes, 在新上传的Branch上，点击Compare &amp; Pull Request按钮创建一个Pull Requst</span><br><span class="line"></span><br><span class="line"># 最后https://github.com/kubernetes/kubernetes/pulls就可以找到刚刚提交的Pull Request。</span><br></pre></td></tr></table></figure></p>
<h2 id="Review-process"><a href="#Review-process" class="headerlink" title="Review process"></a>Review process</h2><p><a href="https://github.com/kubernetes/community/blob/master/contributors/guide/pull-requests.md#the-testing-and-merge-workflow" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/contributors/guide/pull-requests.md#the-testing-and-merge-workflow</a><br> openstack社区更开放，使用gerrit机制，新人都能review代码，并能+1。<br>但k8s使用github的PR，相对封闭一些，新人是不能review代码的，新人的角色叫contributor，可以修改issue (在issue上回复/assign)并提交代码。<br>只有每个子模块下OWNERS文件定义的reviewer, approver角色的人员(<a href="https://github.com/kubernetes/community/blob/master/community-membership.md)才能review代码(reviewer可以LDTM(looks" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/community-membership.md)才能review代码(reviewer可以LDTM(looks</a> good to me, +1), approver可以+2，一个+1一个+2就可以进代码，但openstack中是只能两个+2才可以)</p>
<p>如果要为某个issue创建PR, 需要在PR的描述里填写fixes #issue_num 。这样PR在 merge后issue会“自动”关闭。PR创建后，k8s机器人会做以下几件事：<br>在相应OWNER列表里选取一个人做为reviewer<br>如果是kubernetes member，则启动CI来检查PR，例如UT, e2e test；如果不是kuberentes member ，则需要一个member帮忙启动相应ci<br>待CI没有问题后，可以ping相应的reviewers来检查代码了</p>
<p>在reviewer认为可以后，需要标lgtm (look go to me) 标签；同时需要该模块的approver标记approve标签。两个标签都有了以后，就可以等待合并了。代码的合并也是由k8s机器人完成的，可以在 <a href="http://submit-queue.k8s.io/#/queue" target="_blank" rel="external">http://submit-queue.k8s.io/#/queue</a> 看到等待合并的PR。在合并之前，k8s机器人也会自动重新跑ci以保证代码没有问题。<br>以上三步差不多就可以将typo提交到主干上。其中大部分工作都有k8s机器人自动完成，比如分配reviewer。<br>  Bot命令如下：</p>
<ul>
<li>Jenkins verification: @k8s-bot verify test this</li>
<li>GCE E2E: @k8s-bot cvm gce e2e test this</li>
<li>Test all: @k8s-bot test this please, issue #IGNORE</li>
<li>CRI test: @k8s-bot cri test this.</li>
<li>Verity test: @k8s-bot verify test this</li>
<li>LGTM (only applied if you are one of assignees):: /lgtm</li>
<li>LGTM cancel: /lgtm cancel<br>更多命令见 <a href="https://prow.k8s.io/command-help" target="_blank" rel="external">https://prow.k8s.io/command-help</a><h2 id="How-to-do-test"><a href="#How-to-do-test" class="headerlink" title="How to do test"></a>How to do test</h2><a href="https://github.com/kubernetes/community/blob/master/contributors/devel/testing.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/contributors/devel/testing.md</a><br>make verify<br>make test<br>make test-integration<h2 id="How-to-debug-k8s"><a href="#How-to-debug-k8s" class="headerlink" title="How to debug k8s"></a>How to debug k8s</h2>local-up-cluster.sh是通过_output/local/bin/linux/amd64/hyperkube在容器里启动k8s各服务的，那样是不方便使用(dlv exec ${OUTDIR}/bin/kubelet – $kubelet_flags)来调试基于B/S的k8s服务的，那首先将k8s各服务以本地进程的形式启动，这样调试k8s服务就变得像调试openstack服务一样。</li>
</ul>
<p>1， 第一步创建systemd启动配置文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line">$ cat /lib/systemd/system/kube-etcd.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kube-etcd Service</span><br><span class="line">After=network.target</span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">ExecStart=/usr/bin/etcd -name etcd -data-dir /var/lib/etcd \</span><br><span class="line">          -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 \</span><br><span class="line">          -advertise-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001</span><br><span class="line">Restart=always</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=default.target</span><br><span class="line"></span><br><span class="line">$ cat /lib/systemd/system/kube-apiserver.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kube-apiserver Service</span><br><span class="line">After=network.target</span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">ExecStart=/bak/golang/src/k8s.io/kubernetes/_output/bin/kube-apiserver \</span><br><span class="line">            --admission-control=NamespaceAutoProvision,LimitRanger,SecurityContextDeny \</span><br><span class="line">            --apiserver-count=1 \</span><br><span class="line">            --cors-allowed-origins=.* \</span><br><span class="line">            --enable-garbage-collector=false \</span><br><span class="line">            --etcd-servers=http://127.0.0.1:2379 \</span><br><span class="line">            --insecure-bind-address=0.0.0.0 \</span><br><span class="line">            --insecure-port=8080 \</span><br><span class="line">            --log-dir=~/.kube/log/kube-apiserver \</span><br><span class="line">            --logtostderr=false \</span><br><span class="line">            --service-cluster-ip-range=10.0.0.0/16 \</span><br><span class="line">            --v=5 \</span><br><span class="line">Restart=always</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=default.target</span><br><span class="line"></span><br><span class="line">$ cat /lib/systemd/system/kube-controller-manager.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kube-controller-manager Service</span><br><span class="line">After=network.target</span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">ExecStart=/bak/golang/src/k8s.io/kubernetes/_output/bin/kube-controller-manager \</span><br><span class="line">          --enable-garbage-collector=false \</span><br><span class="line">          --logtostderr=false \</span><br><span class="line">          --log-dir=~/.kube/log/kube-controller-manager \</span><br><span class="line">          --pod-eviction-timeout=5m0s \</span><br><span class="line">          --master=http://0.0.0.0:8080 \</span><br><span class="line">          --node-monitor-grace-period=40s \</span><br><span class="line">          --terminated-pod-gc-threshold=12500 \</span><br><span class="line">          --leader-elect=true \</span><br><span class="line">          --v=4 \</span><br><span class="line">Restart=always</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=default.target</span><br><span class="line"></span><br><span class="line">$ cat /lib/systemd/system/kube-scheduler.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kube-scheduler Service</span><br><span class="line">After=network.target</span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">ExecStart=/bak/golang/src/k8s.io/kubernetes/_output/bin/kube-scheduler \</span><br><span class="line">            --log-dir=~/.k8s/log/kube-scheduler \</span><br><span class="line">            --logtostderr=false \</span><br><span class="line">            --master=http://0.0.0.0:8080 \</span><br><span class="line">            --leader-elect=true \</span><br><span class="line">            --v=5 \</span><br><span class="line">Restart=always</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=default.target</span><br><span class="line"></span><br><span class="line"># prepare kubelet.kubeconfig and kube-proxy.kubeconfig</span><br><span class="line">export KUBE_APISERVER=&quot;http://127.0.0.1:8080&quot;</span><br><span class="line">./_output/bin/kubectl config set-cluster myk8s --server=$&#123;KUBE_APISERVER&#125; --kubeconfig=kubelet.kubeconfig</span><br><span class="line">./_output/bin/kubectl config set-credentials kubelet --kubeconfig=kubelet.kubeconfig</span><br><span class="line">./_output/bin/kubectl config set-context myk8s-context --cluster=myk8s --user=kubelet --kubeconfig=kubelet.kubeconfig</span><br><span class="line">./_output/bin/kubectl config use-context myk8s-context --kubeconfig=kubelet.kubeconfig</span><br><span class="line"></span><br><span class="line">./_output/bin/kubectl config set-cluster myk8s --server=$&#123;KUBE_APISERVER&#125; --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line">./_output/bin/kubectl config set-credentials kube-proxy --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line">./_output/bin/kubectl config set-context myk8s-context --cluster=myk8s --user=kube-proxy --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line">./_output/bin/kubectl config use-context myk8s-context --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line">cp *.kubeconfig /home/hua/.kube/</span><br><span class="line"></span><br><span class="line">$ cat /lib/systemd/system/kubelet.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=kubelet: The Kubernetes Node Agent</span><br><span class="line">Documentation=http://kubernetes.io/docs/</span><br><span class="line">[Service]</span><br><span class="line">ExecStart=/bak/golang/src/k8s.io/kubernetes/_output/bin/kubelet \</span><br><span class="line">          --address=127.0.0.1 --port=10250 --hostname-override=127.0.0.1 \</span><br><span class="line">          --pod-infra-container-image=docker.io/kubernetes/pause \</span><br><span class="line">          --fail-swap-on=false --cgroup-driver=cgroupfs \</span><br><span class="line">          --kubeconfig=/home/hua/.kube/kubelet.kubeconfig \</span><br><span class="line">          --runtime-cgroups=/systemd/system.slice \</span><br><span class="line">          --kubelet-cgroups=/systemd/system.slice \</span><br><span class="line">          --eviction-hard=&apos;nodefs.available&lt;1%&apos; \</span><br><span class="line">          --logtostderr=false --log-dir=~/.kube/log/kubelet --v=4</span><br><span class="line">Restart=always</span><br><span class="line">StartLimitInterval=0</span><br><span class="line">RestartSec=10</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line">$ cat /lib/systemd/system/kube-proxy.service[Unit]</span><br><span class="line">Description=Kube-proxy Service</span><br><span class="line">After=network.target</span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">ExecStart=/bak/golang/src/k8s.io/kubernetes/_output/bin/kube-proxy \</span><br><span class="line">            --log-dir=~/.k8s/log/kube-proxy \</span><br><span class="line">            --logtostderr=false \</span><br><span class="line">            --master=http://0.0.0.0:8080 \</span><br><span class="line">            --kubeconfig=/home/hua/.kube/kube-proxy.kubeconfig \</span><br><span class="line">            --proxy-mode=userspace \</span><br><span class="line">            --v=5</span><br><span class="line">Restart=always</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=default.target</span><br></pre></td></tr></table></figure></p>
<p>2， 第二步，启动各服务:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl --system daemon-reload</span><br><span class="line">sudo systemctl start kube-etcd.service</span><br><span class="line">etcdctl -C http://localhost:4001 cluster-health</span><br><span class="line">sudo systemctl start kube-apiserver.service</span><br><span class="line">sudo systemctl start kube-controller-manager.service</span><br><span class="line">sudo systemctl start kube-scheduler.service</span><br><span class="line">sudo systemctl start kubelet.service</span><br></pre></td></tr></table></figure></p>
<p>3, 第二步，验证安装是否正确：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$ /bak/golang/src/k8s.io/kubernetes/_output/bin/kubectl -s http://127.0.0.1:8080 get componentstatus</span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">controller-manager   Healthy   ok</span><br><span class="line">scheduler            Healthy   ok</span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class="line"></span><br><span class="line">/bak/golang/src/k8s.io/kubernetes/_output/bin/kubectl config set-cluster myk8s --server=http://127.0.0.1:8080</span><br><span class="line">/bak/golang/src/k8s.io/kubernetes/_output/bin/kubectl config set-context myk8s-context --cluster=myk8s --namespace=default --user=client</span><br><span class="line">/bak/golang/src/k8s.io/kubernetes/_output/bin/kubectl config use-context myk8s-context</span><br><span class="line">/bak/golang/src/k8s.io/kubernetes/_output/bin/kubectl config set preferences.colors true</span><br><span class="line">$ cat ~/.kube/config</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    server: http://127.0.0.1:8080</span><br><span class="line">  name: myk8s</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: myk8s</span><br><span class="line">    namespace: default</span><br><span class="line">    user: client</span><br><span class="line">  name: myk8s-context</span><br><span class="line">current-context: myk8s-context</span><br><span class="line">kind: Config</span><br><span class="line">preferences:</span><br><span class="line">  colors: true</span><br><span class="line">users: []</span><br><span class="line"></span><br><span class="line">$ /bak/golang/src/k8s.io/kubernetes/_output/bin/kubectl get componentstatus</span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">controller-manager   Healthy   ok</span><br><span class="line">scheduler            Healthy   ok</span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class="line">$ ./_output/bin/kubectl get nodes</span><br><span class="line">NAME        STATUS    ROLES     AGE       VERSION</span><br><span class="line">127.0.0.1   Ready     &lt;none&gt;    11m       v1.12.0-alpha.0.1999+32dc6cc08aa034-dirty</span><br><span class="line">$ ./_output/bin/kubectl get events</span><br></pre></td></tr></table></figure></p>
<p>4，第四步，例如要调试kubelet服务的话，先停止该服务(sudo systemctl stop kubelet)，然后使用(dlv exec ${OUTDIR}/bin/kubelet – $kubelet_flags)命令启动，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo /bak/golang/bin/dlv --headless -l 127.0.0.1:1234 exec /bak/golang/src/k8s.io/kubernetes/_output/bin/kubelet -- --fail-swap-on=False --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice  --v=4</span><br><span class="line">API server listening at: 127.0.0.1:1234</span><br><span class="line"></span><br><span class="line">$ sudo /bak/golang/bin/dlv connect 127.0.0.1:1234</span><br><span class="line">Type &apos;help&apos; for list of commands.</span><br><span class="line">(dlv) b main.main</span><br><span class="line">Breakpoint 1 set at 0x2d08348 for main.main() ./_output/local/go/src/k8s.io/kubernetes/cmd/kubelet/kubelet.go:36</span><br><span class="line">(dlv) c</span><br></pre></td></tr></table></figure></p>
<h2 id="安装dashboard"><a href="#安装dashboard" class="headerlink" title="安装dashboard"></a>安装dashboard</h2><p>该命令(KUBE_ENABLE_CLUSTER_DASHBOARD=true ./hack/local-up-cluster.sh）会自动安装dashboard。<br>注意：如果不成功原因是需要用小写true，它是区分大小写的。</p>
<p>安装成功后使用命令（cluster/kubectl.sh cluster-info）查看它的访问地址如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://localhost:6443//api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</span><br></pre></td></tr></table></figure></p>
<p>这个链接有点问题，在api处有两个斜线会造成看不到UI，改成如下的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://localhost:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</span><br></pre></td></tr></table></figure></p>
<p>也可运行命令（kubectl proxy –port=8001 –kubeconfig=/var/run/kubernetes/admin.kubeconfig –accept-hosts=’^*$’）访问：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</span><br></pre></td></tr></table></figure></p>
<p>上面的’–accept-hosts’用于在非本机外部访问，但Dashboard只允许localhost和127.0.0.1使用HTTP连接进行访问，而其它地址只允许使用HTTPS。因此，如果需要在非本机访问Dashboard的话，只能采用NodePort:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n kube-system edit service kubernetes-dashboard</span><br><span class="line">$ kubectl -n kube-system get service kubernetes-dashboard</span><br><span class="line">NAME                   TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kubernetes-dashboard   NodePort   10.0.0.232   &lt;none&gt;        443:31050/TCP   1h</span><br><span class="line">visit: https://192.168.99.216:31050/</span><br></pre></td></tr></table></figure></p>
<p>这时访问dashboard仍然有下列问题:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;message&quot;: &quot;services \&quot;https:kubernetes-dashboard:\&quot; is forbidden: User \&quot;system:anonymous\&quot; cannot get services/proxy in the namespace \&quot;kube-system\&quot;: no RBAC policy matched&quot;,</span><br></pre></td></tr></table></figure></p>
<p>这是因为最新版的k8s默认启用了RBAC(–authorization-mode=Node,RBAC)，并为未认证用户赋予了一个默认的身份：anonymous<br>对于API Server来说，它是使用证书进行认证的，我们需要先创建一个证书：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># generate client-certificate-data</span><br><span class="line">grep &apos;client-certificate-data&apos; /var/run/kubernetes/admin.kubeconfig | head -n 1 | awk &apos;&#123;print $2&#125;&apos; | base64 -d &gt;&gt; kubecfg.crt</span><br><span class="line"># generate client-key-data</span><br><span class="line">grep &apos;client-key-data&apos; /var/run/kubernetes/admin.kubeconfig | head -n 1 | awk &apos;&#123;print $2&#125;&apos; | base64 -d &gt;&gt; kubecfg.key</span><br><span class="line"># generate p12</span><br><span class="line">openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name &quot;kubernetes-client&quot;</span><br></pre></td></tr></table></figure></p>
<p>然后将该p12证书导入到浏览器即可。此时默认的anonymous身份的token可以这样获取:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cluster/kubectl.sh get secret -n kube-system | grep dashboard</span><br><span class="line">cluster/kubectl.sh -n kube-system  get secret kubernetes-dashboard-token-kglhd -o jsonpath=&#123;.data.token&#125;| base64 -d</span><br></pre></td></tr></table></figure></p>
<p>anonymous身份可能看不到很多东西，所以我们再在kube-system名空间下再创建一个admin用户并和cluster-admin角色关联：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /tmp/admin-user.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line">cat &gt; /tmp/admin-user-role-binding.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line">cluster/kubectl.sh create -f /tmp/admin-user.yaml</span><br><span class="line">cluster/kubectl.sh create -f /tmp/admin-user-role-binding.yaml</span><br><span class="line">cluster/kubectl.sh -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin | awk &apos;&#123;print $1&#125;&apos;)</span><br></pre></td></tr></table></figure>
<p>##直接修改local-up-cluster.sh代替hyperkube用本地进程启动 ##<br>或者直接修改脚本去掉hyperkube, 然后运行ENABLE_CLUSTER_DASHBOARD=True ./hack/local-up-cluster.sh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">diff --git a/hack/local-up-cluster.sh b/hack/local-up-cluster.sh</span><br><span class="line">index 3b688d3..95de0df 100755</span><br><span class="line">--- a/hack/local-up-cluster.sh</span><br><span class="line">+++ b/hack/local-up-cluster.sh</span><br><span class="line">@@ -202,7 +202,8 @@ do</span><br><span class="line"> done</span><br><span class="line"></span><br><span class="line"> if [ &quot;x$GO_OUT&quot; == &quot;x&quot; ]; then</span><br><span class="line">-    make -C &quot;$&#123;KUBE_ROOT&#125;&quot; WHAT=&quot;cmd/kubectl cmd/hyperkube&quot;</span><br><span class="line">+    #make -C &quot;$&#123;KUBE_ROOT&#125;&quot; WHAT=&quot;cmd/kubectl cmd/hyperkube&quot;</span><br><span class="line">+    make -C &quot;$&#123;KUBE_ROOT&#125;&quot; GOGCFLAGS=&quot;-N -l&quot; WHAT=&quot;cmd/kubelet cmd/kube-proxy cmd/kube-apiserver cmd/kube-controller-manager cmd/cloud-controller-manager cmd/kube-scheduler cmd/kubectl&quot;</span><br><span class="line"> else</span><br><span class="line">     echo &quot;skipped the build.&quot;</span><br><span class="line"> fi</span><br><span class="line">@@ -578,7 +579,7 @@ function start_apiserver &#123;</span><br><span class="line">     fi</span><br><span class="line"></span><br><span class="line">     APISERVER_LOG=$&#123;LOG_DIR&#125;/kube-apiserver.log</span><br><span class="line">-    $&#123;CONTROLPLANE_SUDO&#125; &quot;$&#123;GO_OUT&#125;/hyperkube&quot; apiserver $&#123;swagger_arg&#125; $&#123;audit_arg&#125; $&#123;authorizer_arg&#125; $&#123;priv_arg&#125; $&#123;runtime_config&#125; \</span><br><span class="line">+    $&#123;CONTROLPLANE_SUDO&#125; &quot;$&#123;GO_OUT&#125;/kube-apiserver&quot; $&#123;swagger_arg&#125; $&#123;audit_arg&#125; $&#123;authorizer_arg&#125; $&#123;priv_arg&#125; $&#123;runtime_config&#125; \</span><br><span class="line">       $&#123;cloud_config_arg&#125; \</span><br><span class="line">       $&#123;advertise_address&#125; \</span><br><span class="line">       $&#123;node_port_range&#125; \</span><br><span class="line">@@ -650,7 +651,7 @@ function start_controller_manager &#123;</span><br><span class="line">     fi</span><br><span class="line"></span><br><span class="line">     CTLRMGR_LOG=$&#123;LOG_DIR&#125;/kube-controller-manager.log</span><br><span class="line">-    $&#123;CONTROLPLANE_SUDO&#125; &quot;$&#123;GO_OUT&#125;/hyperkube&quot; controller-manager \</span><br><span class="line">+    $&#123;CONTROLPLANE_SUDO&#125; &quot;$&#123;GO_OUT&#125;/kube-controller-manager&quot; \</span><br><span class="line">       --v=$&#123;LOG_LEVEL&#125; \</span><br><span class="line">       --vmodule=&quot;$&#123;LOG_SPEC&#125;&quot; \</span><br><span class="line">       --service-account-private-key-file=&quot;$&#123;SERVICE_ACCOUNT_KEY&#125;&quot; \</span><br><span class="line">@@ -685,7 +686,7 @@ function start_cloud_controller_manager &#123;</span><br><span class="line">     fi</span><br><span class="line"></span><br><span class="line">     CLOUD_CTLRMGR_LOG=$&#123;LOG_DIR&#125;/cloud-controller-manager.log</span><br><span class="line">-    $&#123;CONTROLPLANE_SUDO&#125; $&#123;EXTERNAL_CLOUD_PROVIDER_BINARY:-&quot;$&#123;GO_OUT&#125;/hyperkube&quot; cloud-controller-manager&#125; \</span><br><span class="line">+    $&#123;CONTROLPLANE_SUDO&#125; $&#123;EXTERNAL_CLOUD_PROVIDER_BINARY:-&quot;$&#123;GO_OUT&#125;/cloud-controller-manager&quot;&#125; \</span><br><span class="line">       --v=$&#123;LOG_LEVEL&#125; \</span><br><span class="line">       --vmodule=&quot;$&#123;LOG_SPEC&#125;&quot; \</span><br><span class="line">       $&#123;node_cidr_args&#125; \</span><br><span class="line">@@ -791,7 +792,7 @@ function start_kubelet &#123;</span><br><span class="line">     )</span><br><span class="line"></span><br><span class="line">     if [[ -z &quot;$&#123;DOCKERIZE_KUBELET&#125;&quot; ]]; then</span><br><span class="line">-      sudo -E &quot;$&#123;GO_OUT&#125;/hyperkube&quot; kubelet &quot;$&#123;all_kubelet_flags[@]&#125;&quot; &gt;&quot;$&#123;KUBELET_LOG&#125;&quot; 2&gt;&amp;1 &amp;</span><br><span class="line">+      sudo -E &quot;$&#123;GO_OUT&#125;/kubelet&quot; &quot;$&#123;all_kubelet_flags[@]&#125;&quot; &gt;&quot;$&#123;KUBELET_LOG&#125;&quot; 2&gt;&amp;1 &amp;</span><br><span class="line">       KUBELET_PID=$!</span><br><span class="line">     else</span><br><span class="line"></span><br><span class="line">@@ -889,14 +890,14 @@ EOF</span><br><span class="line">       done</span><br><span class="line">     fi &gt;&gt;/tmp/kube-proxy.yaml</span><br><span class="line"></span><br><span class="line">-    sudo &quot;$&#123;GO_OUT&#125;/hyperkube&quot; proxy \</span><br><span class="line">+    sudo &quot;$&#123;GO_OUT&#125;/kube-proxy&quot; \</span><br><span class="line">       --v=$&#123;LOG_LEVEL&#125; \</span><br><span class="line">       --config=/tmp/kube-proxy.yaml \</span><br><span class="line">       --master=&quot;https://$&#123;API_HOST&#125;:$&#123;API_SECURE_PORT&#125;&quot; &gt;&quot;$&#123;PROXY_LOG&#125;&quot; 2&gt;&amp;1 &amp;</span><br><span class="line">     PROXY_PID=$!</span><br><span class="line"></span><br><span class="line">     SCHEDULER_LOG=$&#123;LOG_DIR&#125;/kube-scheduler.log</span><br><span class="line">-    $&#123;CONTROLPLANE_SUDO&#125; &quot;$&#123;GO_OUT&#125;/hyperkube&quot; scheduler \</span><br><span class="line">+    $&#123;CONTROLPLANE_SUDO&#125; &quot;$&#123;GO_OUT&#125;/kube-scheduler&quot; \</span><br><span class="line">       --v=$&#123;LOG_LEVEL&#125; \</span><br><span class="line">       --kubeconfig &quot;$CERT_DIR&quot;/scheduler.kubeconfig \</span><br><span class="line">       --feature-gates=&quot;$&#123;FEATURE_GATES&#125;&quot; \</span><br></pre></td></tr></table></figure></p>
<h2 id="How-to-read-source-code"><a href="#How-to-read-source-code" class="headerlink" title="How to read source code"></a>How to read source code</h2><p><a href="http://dockone.io/article/895" target="_blank" rel="external">http://dockone.io/article/895</a></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://kubernetes.io/docs/imported/community/devel/" target="_blank" rel="external">https://kubernetes.io/docs/imported/community/devel/</a><br>[2] <a href="https://github.com/kubernetes/community/tree/master/contributors/devel" target="_blank" rel="external">https://github.com/kubernetes/community/tree/master/contributors/devel</a><br>[3] Bug - <a href="https://github.com/kubernetes/community/issues" target="_blank" rel="external">https://github.com/kubernetes/community/issues</a><br>[4] Submit code review - <a href="https://github.com/kubernetes/community/blob/master/contributors/guide/pull-requests.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/contributors/guide/pull-requests.md</a><br>[5] Membership - <a href="https://github.com/kubernetes/community/blob/master/community-membership.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/community-membership.md</a><br>[6] CONTRIBUTING - <a href="https://github.com/kubernetes/community/blob/master/CONTRIBUTING.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/CONTRIBUTING.md</a><br>[7] Code format - <a href="https://github.com/golang/go/wiki/CodeReviewComments" target="_blank" rel="external">https://github.com/golang/go/wiki/CodeReviewComments</a><br>[8] Slack - <a href="https://kubernetes.slack.com/messages" target="_blank" rel="external">https://kubernetes.slack.com/messages</a><br>[9] Mail-list - <a href="https://groups.google.com/forum/#!forum/kubernetes-dev" target="_blank" rel="external">https://groups.google.com/forum/#!forum/kubernetes-dev</a><br>[10] SIG-list (Special Interest Groups) - <a href="https://github.com/kubernetes/community/blob/master/sig-list.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/sig-list.md</a><br>[11] open-bug - <a href="https://github.com/kubernetes/community/blob/master/contributors/guide/issue-triage.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/contributors/guide/issue-triage.md</a><br>[12] BP - <a href="https://github.com/kubernetes/community/tree/master/contributors/design-proposals" target="_blank" rel="external">https://github.com/kubernetes/community/tree/master/contributors/design-proposals</a><br>[13] <a href="https://github.com/kubernetes/community/blob/master/community-membership.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/community-membership.md</a><br>[14] test - <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/testing.md" target="_blank" rel="external">https://github.com/kubernetes/community/blob/master/contributors/devel/testing.md</a><br>[15] <a href="https://ress.infoq.com/minibooks/Kubernetes-handbook/zh/pdf/kubernetes.pdf" target="_blank" rel="external">https://ress.infoq.com/minibooks/Kubernetes-handbook/zh/pdf/kubernetes.pdf</a><br>[16] <a href="https://kubernetes.io/docs/home/" target="_blank" rel="external">https://kubernetes.io/docs/home/</a><br>[17] <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way" target="_blank" rel="external">https://github.com/kelseyhightower/kubernetes-the-hard-way</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/07/10/Set-up-k8s-development-env-by-quqi99/" data-id="cjqc2cuv4000dd4bp8c0dmkgi" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-温故OpenStack中的测试-by-Joshua" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/10/温故OpenStack中的测试-by-Joshua/" class="article-date">
  <time datetime="2018-07-10T08:28:15.000Z" itemprop="datePublished">2018-07-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/07/10/温故OpenStack中的测试-by-Joshua/">温故OpenStack中的测试(by Joshua)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (作者：张华 发表于：2018-03-15)</strong></p>
<ol>
<li><p>沿用tox调用virtualenv自动创建的虚拟环境(virtualenv -p python3.5 .tox/py35)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source .tox/py35/bin/activate</span><br><span class="line">sudo pip install --upgrade -r requirements.txt</span><br><span class="line">sudo pip install --upgrade -r test-requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用unittest和nose运行测试。nose是对unittest的扩展，使得python的测试更加简单，nose自动发现测试代码并执行，nose提供了大量的插件，比如覆盖报表等。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m unittest -v unit_tests.test_neutron_utils.TestNeutronUtils.test_get_packages_ovs_newton</span><br><span class="line">.tox/py35/bin/python nosetests -v unit_tests/test_neutron_utils.py:TestNeutronUtils.test_get_packages_ovs_newton</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>注意：上面采用nosetests运行时会报错，因为我们的测试采用了python3, 所以需要在安装了python3-nose之后（sudo apt-get install python3-nose python3-mock）再采用下列三种方式之一运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nosetests3 -v unit_tests/test_neutron_utils.py:TestNeutronUtils.test_restart_map_ovs_odl</span><br><span class="line">/bak/work/charms/neutron-gateway/.tox/py35/bin/python /usr/local/bin/nosetests -v unit_tests/test_neutron_utils.py:TestNeutronUtils.test_get_packages_ovs_newton</span><br><span class="line">python -m nose unit_tests/test_neutron_utils.py:TestNeutronUtils.test_restart_map_ovs_odl</span><br></pre></td></tr></table></figure></p>
<p>但实际上仍然找不找nose模块，那是因为nose与virtualenv结合地不大好，在这个网页找着了答案(<a href="https://stackoverflow.com/questions/864956/problems-using-nose-in-a-virtualenv" target="_blank" rel="external">https://stackoverflow.com/questions/864956/problems-using-nose-in-a-virtualenv</a>) - You need to have a copy of nose installed in the virtual environment. In order to force installation of nose into the virtualenv, even though it is already installed in the global site-packages, run pip install with the -I flag: pip install nose -I</p>
<ol>
<li><p>上面使用unittest与nose运行测试的方式只是将结果输出到stdout，不便于分析。所以可以使用python-subunit模块来运行测试，并将测试结果通过subunit协议输出到文件中便于日后分析。因为subunit是基于二进制的不便于人眼看，所以可使用subunit2pyunit工具将其人类可读化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python -m subunit.run discover |subunit2pyunit</span><br><span class="line">python -m subunit.run discover -t ./ ./unit_tests |subunit2pyunit</span><br><span class="line">python -m subunit.run unit_tests.test_neutron_utils.TestNeutronUtils. |subunit2pyunit</span><br></pre></td></tr></table></figure>
</li>
<li><p>在大型应用中分析测试结果很重要，testrepository可以调用subunit来用python-subunit模块来运行测试，并将测试结果通过subunit协议输出到文件中，然后testrepository在些基础上有更多的分析，如分析哪些用例运行的时间最长，如显示失败的用例，如仅运行上次运行失败的用例。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">testr init</span><br><span class="line">testr run</span><br><span class="line">testr run --parallel</span><br><span class="line">$ cat .testr.conf</span><br><span class="line">[DEFAULT]</span><br><span class="line">test_command=OS_STDOUT_CAPTURE=$&#123;OS_STDOUT_CAPTURE:-1&#125; \</span><br><span class="line">             OS_STDERR_CAPTURE=$&#123;OS_STDERR_CAPTURE:-1&#125; \</span><br><span class="line">             OS_TEST_TIMEOUT=$&#123;OS_TEST_TIMEOUT:-60&#125; \</span><br><span class="line">             $&#123;PYTHON:-python&#125; -m subunit.run discover -t ./ ./unit_tests $LISTOPT $IDOPTION</span><br><span class="line">test_id_option=--load-list $IDFILE</span><br><span class="line">test_list_option=--list</span><br></pre></td></tr></table></figure>
</li>
<li><p>tox用于创建虚拟python环境，也可以集成上面的testrepository(commands = ostestr {posargs})</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$ cat tox.ini</span><br><span class="line">[tox]</span><br><span class="line">envlist = pep8,py27,py35</span><br><span class="line">skipsdist = True</span><br><span class="line"></span><br><span class="line">[testenv]</span><br><span class="line">setenv = VIRTUAL_ENV=&#123;envdir&#125;</span><br><span class="line">         PYTHONHASHSEED=0</span><br><span class="line">         CHARM_DIR=&#123;envdir&#125;</span><br><span class="line">         AMULET_SETUP_TIMEOUT=5400</span><br><span class="line">install_command =</span><br><span class="line">  pip install --allow-unverified python-apt &#123;opts&#125; &#123;packages&#125;</span><br><span class="line">commands = ostestr &#123;posargs&#125;</span><br><span class="line">whitelist_externals = juju</span><br><span class="line">passenv = HOME TERM AMULET_* CS_API_*</span><br><span class="line"></span><br><span class="line">[testenv:py27]</span><br><span class="line">basepython = python2.7</span><br><span class="line">deps = -r&#123;toxinidir&#125;/requirements.txt</span><br><span class="line">       -r&#123;toxinidir&#125;/test-requirements.txt</span><br><span class="line">commands = /bin/true</span><br><span class="line"></span><br><span class="line">[testenv:py35]</span><br><span class="line">basepython = python3.5</span><br><span class="line">deps = -r&#123;toxinidir&#125;/requirements.txt</span><br><span class="line">       -r&#123;toxinidir&#125;/test-requirements.txt</span><br><span class="line"></span><br><span class="line">[testenv:pep8]</span><br><span class="line">basepython = python2.7</span><br><span class="line">deps = -r&#123;toxinidir&#125;/requirements.txt</span><br><span class="line">       -r&#123;toxinidir&#125;/test-requirements.txt</span><br><span class="line">commands = flake8 &#123;posargs&#125; hooks unit_tests tests actions lib</span><br><span class="line">           charm-proof</span><br><span class="line"></span><br><span class="line">[flake8]</span><br><span class="line">ignore = E402,E226</span><br><span class="line">exclude = */helpers</span><br></pre></td></tr></table></figure>
</li>
<li><p>pydev使用virtualenv中的py35<br>在eclipse的”Preferences -&gt; Pydev -&gt; Interpreters -&gt; Python Interpreters”菜单中定义python35=/bak/work/charms/neutron-gateway/.tox/py35/bin/python,然后在工程上点右键从”Properties -&gt; Pydev - Interpreter/Grammar”定义使用python35。注意，需要将/bak/work/charms/neutron-gateway/.tox/py35/lib/python3.5/site-packages也选到环境变量中，否则后面会报ImportError: No module named ‘mock。<br>为一个测试类定义”Python unitest”类型的”Debug Configurations”, 也在其Interpreter选项卡中定义使用python35 (结果：eclipse似乎有bug，此处选择了python35后无法保存)<br>所以无法成功，似乎是pydev与python3协作不大好。最后还是pudb好使(sudo pip install pudb, import pudb; pdb.set_trace())</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/07/10/温故OpenStack中的测试-by-Joshua/" data-id="cjqc2cuvg000sd4bp8gzb78tk" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/12/31/Use-Octavia-to-Implement-HTTPS-Health-Monitors/">Use Octavia to Implement HTTPS Health Monitors</a>
          </li>
        
          <li>
            <a href="/2018/10/29/为租户下的虚机提供IPv6-DNS服务/">为租户下的虚机提供IPv6 DNS服务</a>
          </li>
        
          <li>
            <a href="/2018/09/10/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2018/09/09/Win-10-UEFI-Ubuntu-18-04-UEFI-双系统/">Win 10 UEFI + Ubuntu 18.04 UEFI 双系统</a>
          </li>
        
          <li>
            <a href="/2018/09/03/也谈wifi断流问题/">也谈wifi断流问题</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 张华<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
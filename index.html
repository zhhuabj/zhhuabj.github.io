<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="blog.csdn.net/quqi99">
<meta property="og:type" content="website">
<meta property="og:title" content="技术并艺术着">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="技术并艺术着">
<meta property="og:description" content="blog.csdn.net/quqi99">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="技术并艺术着">
<meta name="twitter:description" content="blog.csdn.net/quqi99">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>技术并艺术着</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">技术并艺术着</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">张华的技术博客 - blog.csdn.net/quqi99</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/01/15/OpenStack对NUMA的支持情况/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/15/OpenStack对NUMA的支持情况/" itemprop="url">OpenStack对NUMA的支持情况</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-15T19:11:40+08:00">
                2021-01-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华  发表于：2016-03-24<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )</p>
<p>理论<br>vCPU topology, libvirt将第一个vCPU在虚机中视为: 1 socket with 1 core and no hyper-threads. 但有的操作系统的license会限制socket的数目，另外一般地在不同的core上的2个threads的性能会比2个在同相core上的threads的性能要好。<br>NUMA topology, NUMA节点有自己独立的RAM、Bus、PCI和pCPU sockets, 为一个VM分配cpu及PCI设备时应该尽量分配同一个NUMA节点上的pCPU。<br>Guest NUMA, 如果一个VM要求的vCPU/RAM/PCIe大于一个NUMA节点所具有的物理pCPU/RAM/PCIe呢？<br>可划分为多个NUMA节点（hw:numa_nodes=N).<br>Large pages, CPU支持4k, 2M/4M, 1G的Hugepage模式(page_sizes=(any|small|large)，这时cpu的页表数目会减少提升TLB页表的命中率，但操作系统默认为4k，运行时间长了很难找到连续的大页空间分配。当前Kernel不允许为NUMA节点预留大页，NUMA节点有特定的RAM，也就是说，NUMA节点也有特定的大页。在的NUMA节点有足够的大页空间，有的可能刚好没有了。<br>Dedicated resource, overcommit_ram=0,overcommit_vcpus=0<br>KSM(kernel shared memory), 将相同的内存分页进行合并，合并之后若再遇到写就再用CoW打开一份，要能阻止宿主机将特定的内存分页合并。<br>例如：我的t440p是一个cpu(socket), 双核，开了超线程，4个core id, 所以siblings也是4(一个物理封装中的逻辑cpu个数，只有一个cpu所以是一个物理，然后双核，超线程).</p>
<p>1, cpu个数(socket)： cat /proc/cpuinfo | grep “physical id” | sort | uniq | wc -l<br>2, cpu核数： cat /proc/cpuinfo | grep “cpu cores” | uniq<br>3, Logic CPU ID: cat /proc/cpuinfo | grep “core id”<br>4, 每颗cpu的逻辑核数： cat /proc/cpuinfo | grep “siblings”<br>5, 所有cpu的逻辑核数： cat /proc/cpuinfo | grep “processor” | wc -l<br>6, 是否启用超线程(相同则没启用：cat /proc/cpuinfo | grep -e “cpu cores”  -e “siblings” | sort | uniq)</p>
<p>实际操作<br>Image方式, glance image-update –property hw_numa_nodes=2 hw_numa_cpus.0=0 hw_numa_mem.0=512 hw_numa_cpus.1=0 hw_numa_mem.1=512 image_name<br>Flavor方式, nova flavor-key flv_name set hw:numa_nodes=2 hw:numa_cpus.0=0 hw:numa_mem.0=512 hw:numa_cpus.1=0 hw:numa_mem.1=512<br>hw:numa_nodes=NN                     #guest NUMA node数量<br>hw:numa_mempolicy=preferred|strict   #RAM占用方式<br>hw:numa_cpus.N=<cpu-list>            #guest NUMA node N中的vcpu列表<br>hw:numa_mem.1=<ram-size>             #guest NUMA node N中的RAM大小<br>hw:cpu_thread_policy=prefer|isolate|require<br>hw:cpu_policy=shared|dedicated<br>libvirt.xml</ram-size></cpu-list></p>
<p><cputune><br>  /<strong> pin vCPU to pCPU set in cputune </strong>/<br>  <vcpupin vcpu="0" cpuset="4-7,12-15"><br></vcpupin></cputune><br>/<strong> expoert guest numa in cpu/numa </strong>/ </p>
<cpu><br>   <topology sockets="2" cores="2" threads="1"><br>   <numa><br>      <cell id="0" cpus="0-1" memory="1048576"><br>   </cell></numa><br></topology></cpu> 


<p>nova-scheduler<br>NUMATopologyFilter<br>一些命令<br>hua@node1:~$ sudo virsh nodeinfo<br>CPU model:           x86_64<br>CPU(s):              4<br>CPU frequency:       3348 MHz<br>CPU socket(s):       1<br>Core(s) per socket:  4<br>Thread(s) per core:  1<br>NUMA cell(s):        1<br>Memory size:         32753068 KiB</p>
<p>hua@node1:~$ sudo virsh freecell 0<br>0: 9974312 KiB</p>
<p>附件 - Power SMT</p>
<p>Power 8有一个SMT特性，即多线程的并发改为一个核内多个指令的并发从而提升性能， 但是KVM Power版本却不支持SMT， 所以需要将SMT关闭，这样造成性能下降，这样pacemaker将会报这种错”Sep 22 06:52:35 juju-b209b8-4-lxd-2 crmd[122100]: notice: High CPU load detected“，从而造成如glance service的vip不work了也就无法ping了。其实，即使SMT=off (echo off &gt; /sys/devices/system/cpu/smt/control)也是可以利offline的核的， 即采用这个网页（<a href="https://www.ibm.com/developerworks/community/blogs/fe313521-2e95-46f2-817d-44a4f27eba32/entry/enabling_smt_on_powerkvm_guests?lang=en）的方法：" target="_blank" rel="external">https://www.ibm.com/developerworks/community/blogs/fe313521-2e95-46f2-817d-44a4f27eba32/entry/enabling_smt_on_powerkvm_guests?lang=en）的方法：</a></p>
<p>nova针对一个虚机使用8个vcpu有下列三种配置方法：</p>
<p>1) By default in nova</p>
<p><vcpu placement="static">8</vcpu></p>
<cpu><br><topology sockets="8" cores="1" threads="1"><br>…<br></topology></cpu>

<p>2) 4 threads</p>
<p><vcpu>8</vcpu></p>
<cpu><br><topology sockets="1" cores="2" threads="4"><br>…<br></topology></cpu> 

<p>3) 8 threads</p>
<p><vcpu>8</vcpu></p>
<cpu><br><topology sockets="1" cores="1" threads="8"><br>…<br></topology></cpu>

<p>如想要配置成上面的2）的话，可以：</p>
<p>openstack flavor set <flavor_uuid> \<br>–vcpus 8 \<br>–property hw:cpu_sockets=1 \<br>–property hw:cpu_cores=2 \<br>–property hw:cpu_threads=4<br>配置成上面的3）的话，可以：</flavor_uuid></p>
<p>openstack flavor set <flavor_uuid> \<br>–vcpus 8 \<br>–property hw:cpu_sockets=1 \<br>–property hw:cpu_cores=1 \<br>–property hw:cpu_threads=8</flavor_uuid></p>
<p>Power8在设计上是有缺陷的, 它需要先关闭SMT, 然后在KVM里又可以使用offline CPU, Power9则修复了这个问题, 见: <a href="https://www.linux-kvm.org/images/7/77/01x09a-KVMPower.pdf" target="_blank" rel="external">https://www.linux-kvm.org/images/7/77/01x09a-KVMPower.pdf</a></p>
<p>另外, 因为SMT关闭了, 当Power8上的KVM未使用上面拓扑时一个使用8个vcpu的虚机占了8个核而不是一个核上的8线程, 这样无形中CPU数少了8倍. 这个机器如果不光是KVM还有mysql的话mysql也存在这种情况, 这样使用perf查看时会看到mysql与kvm并没有哪个特别宽, 都差不多分布比较均匀, 这样反而证明了整体性能需要提升. mysql因为是openstack的底层服务无法使用上面的拓扑使用多线程, 所以最好不要将mysql和kvm安装在一块.</p>
<p>再说说sbiblings<br>siblings指一个物理cpu上的cpu, 下面的设置是两个cell两个物理cpu，一个物理cpu是32个core超线程是2故共32个thread</p>
<p>NUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62<br>NUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63<br>CPU(s):              64<br>On-line CPU(s) list: 0-63<br>Thread(s) per core:  2<br>Core(s) per socket:  16<br>Socket(s):           2<br>NUMA node(s):        2</p>
<p>nova中看到的pcpuset与siblings如下：</p>
<p>pcpuset=set([8,10,12,14,16,18,20,22,24,26,28,30,40,42,44,46,48,50,52,54,56,58,60,62])<br>siblings=[set([16,48]),set([10,42]),set([54,22]),set([46,14]),set([12,44]),set([20,52]),set([26,58]),set([28,60]),set([62,30]),set([18,50]),set([8,40]),set([24,56])]</p>
<p>当设置cpu_thread_policy=’isolate’时，意味着不用thread只用真正的core.<br>因为siblings对象的引入的目的就是为了无论服务器是否开启了超线程，Nova 同样能够支持物理CPU绑定的功能。<br>这样，当一个虚机使用了[8, 10, 12, 16, 18, 20, 24, 26, 28, 46, 54, 62]这些cpu的话，pinned_cpus会如下：</p>
<p>pinned_cpus=set([8,10,12,14,16,18,20,22,24,26,28,30,40,42,44,46,48,50,52,54,56,58,60,62])</p>
<p>所以用thread的话就配置hw:cpu_thread_policy=require, 用core的话就用cpu_thread_policy=’isolate’.<br>而且如果emulator也用了isolate的话（hw:emulator_threads_policy=’isolate’）, not only will it not allow a sibling but it will also require all pcpu to be<br>available on the same numa node.</p>
<p>参考<br><a href="https://wiki.openstack.org/wiki/VirtDriverGuestCPUMemoryPlacement" target="_blank" rel="external">https://wiki.openstack.org/wiki/VirtDriverGuestCPUMemoryPlacement</a><br><a href="https://specs.openstack.org/openstack/nova-specs/specs/liberty/approved/virt-driver-cpu-pinning.html" target="_blank" rel="external">https://specs.openstack.org/openstack/nova-specs/specs/liberty/approved/virt-driver-cpu-pinning.html</a><br><a href="http://blog.csdn.net/canxinghen/article/details/41810241" target="_blank" rel="external">http://blog.csdn.net/canxinghen/article/details/41810241</a><br><a href="https://specs.openstack.org/openstack/nova-specs/specs/mitaka/approved/virt-driver-cpu-thread-pinning.html" target="_blank" rel="external">https://specs.openstack.org/openstack/nova-specs/specs/mitaka/approved/virt-driver-cpu-thread-pinning.html</a><br><a href="http://docs.openstack.org/developer/nova/testing/libvirt-numa.html" target="_blank" rel="external">http://docs.openstack.org/developer/nova/testing/libvirt-numa.html</a><br><a href="https://www.berrange.com/posts/2010/02/12/controlling-guest-cpu-numa-affinity-in-libvirt-with-qemu-kvm-xen/" target="_blank" rel="external">https://www.berrange.com/posts/2010/02/12/controlling-guest-cpu-numa-affinity-in-libvirt-with-qemu-kvm-xen/</a><br><a href="https://review.openstack.org/#/c/140290/" target="_blank" rel="external">https://review.openstack.org/#/c/140290/</a><br><a href="http://slideplayer.com/slide/4868412/" target="_blank" rel="external">http://slideplayer.com/slide/4868412/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/01/14/Using-BPF-USDT-to-trace-OpenStack/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/14/Using-BPF-USDT-to-trace-OpenStack/" itemprop="url">Using BPF USDT to trace OpenStack</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-14T20:22:20+08:00">
                2021-01-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>作者：张华 发表于：2021-01-14<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</strong></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>./nova/compute/resource_tracker.py#_update_available_resource中的_update_usage_from_instances函数没有DEBUG LEVEL日志，有办法通过probe则不是通过改代码写日志的方法来调试该函数吗？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def _update_available_resource(self, context, resources, startup=False):</span><br><span class="line">...</span><br><span class="line">cn = self.compute_nodes[nodename]</span><br></pre></td></tr></table></figure></p>
<h2 id="确认python版本是否支持USDT探针"><a href="#确认python版本是否支持USDT探针" class="headerlink" title="确认python版本是否支持USDT探针"></a>确认python版本是否支持USDT探针</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install python3-bpfcc libbpfcc bpfcc-tools -y</span><br><span class="line"># if the output is empty, it means we need to compile python with dtrace</span><br><span class="line"># for &gt;&gt;python3.7 supports &apos;--with-dtrace&apos;</span><br><span class="line"># https://bugs.launchpad.net/ubuntu/+source/python3.8/+bug/1818778</span><br><span class="line">tplist-bpfcc -l $(which python3)</span><br></pre></td></tr></table></figure>
<h2 id="探针种类"><a href="#探针种类" class="headerlink" title="探针种类"></a>探针种类</h2><p><a href="https://www.collabora.com/news-and-blog/blog/2019/05/14/an-ebpf-overview-part-5-tracing-user-processes/" target="_blank" rel="external">https://www.collabora.com/news-and-blog/blog/2019/05/14/an-ebpf-overview-part-5-tracing-user-processes/</a><br>有三种探针:<br>1, USDT静态探针, 主要是针对所依赖的二进制模块(eg: libc.so, libpthread.so, libvirt.so etc)的预定义探针。python是解释型语言，看来函数./nova/compute/resource_tracker.py#_update_available_resource只能通过下列function<strong>entry与function</strong>return两种探针来做。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># https://github.com/iovisor/bcc/blob/master/tools/tplist.py</span><br><span class="line"># tplist-bpfcc -p $(ps -ef |grep nova-compute |grep -v grep |awk &apos;&#123;print $2&#125;&apos;) |grep python |grep function</span><br><span class="line">b&apos;/proc/1551047/root/usr/bin/python3.8&apos; b&apos;python&apos;:b&apos;function__entry&apos;</span><br><span class="line">b&apos;/proc/1551047/root/usr/bin/python3.8&apos; b&apos;python&apos;:b&apos;function__return&apos;</span><br></pre></td></tr></table></figure></p>
<p>2, 自定义探针(tracepints), 需要在你的python代码中通过provider.add_probe添加探针，这种和打日志没啥区别啊。略。<br>3, uprobes动态探针，不需要改运行代码，可以通过下列类似b.attach_uprobe来添加探针，但这种探针显示也是针对模块的，对解释型的python不适用。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b.attach_uprobe(name=&quot;c&quot;, sym=&quot;getaddrinfo&quot;, fn_name=&quot;do_entry&quot;, pid=args.pid)</span><br><span class="line">b.attach_uretprobe(name=&quot;c&quot;, sym=&quot;getaddrinfo&quot;, fn_name=&quot;do_return&quot;, pid=args.pid)</span><br></pre></td></tr></table></figure></p>
<h2 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">root@demo:~# ./test.py $(ps -ef |grep nova-compute |grep -v grep |awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">207625.396728000   b&apos;_update_available_resource here here!&apos;</span><br><span class="line"></span><br><span class="line">root@demo:~# cat test.py</span><br><span class="line">#!/usr/bin/env python3</span><br><span class="line">from bcc import BPF, USDT</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">bpf = &quot;&quot;&quot;</span><br><span class="line">#include &lt;uapi/linux/ptrace.h&gt;</span><br><span class="line"></span><br><span class="line">static int strncmp(char *s1, char *s2, int size) &#123;</span><br><span class="line">    for (int i = 0; i &lt; size; ++i)</span><br><span class="line">        if (s1[i] != s2[i])</span><br><span class="line">            return 1;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int trace_file_transfers(struct pt_regs *ctx) &#123;</span><br><span class="line">    uint64_t fnameptr;</span><br><span class="line">    char fname[128]=&#123;0&#125;, searchname[30]=&quot;_update_available_resource&quot;;</span><br><span class="line"></span><br><span class="line">    bpf_usdt_readarg(2, ctx, &amp;fnameptr);</span><br><span class="line">    bpf_probe_read(&amp;fname, sizeof(fname), (void *)fnameptr);</span><br><span class="line"></span><br><span class="line">    if (!strncmp(fname, searchname, sizeof(searchname)))</span><br><span class="line">        bpf_trace_printk(&quot;_update_available_resource here here!\\n&quot;);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">u = USDT(pid=int(sys.argv[1]))</span><br><span class="line">u.enable_probe(probe=&quot;function__entry&quot;, fn_name=&quot;trace_file_transfers&quot;)</span><br><span class="line">b = BPF(text=bpf, usdt_contexts=[u])</span><br><span class="line">while 1:</span><br><span class="line">    try:</span><br><span class="line">        (_, _, _, _, ts, msg) = b.trace_fields()</span><br><span class="line">    except ValueError:</span><br><span class="line">        continue</span><br><span class="line">    print(&quot;%-18.9f %s&quot; % (ts, msg))</span><br></pre></td></tr></table></figure>
<h2 id="打印变量"><a href="#打印变量" class="headerlink" title="打印变量"></a>打印变量</h2><p><a href="https://zhuanlan.zhihu.com/p/138887361" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/138887361</a><br>bcc自带的脚本已经能够满足一般的需求, 但是也不能满足所有需求. 这里以uprobe例, 看一下在bcc里面怎么访问变量, 很大程度上取决于probe的位置:</p>
<ul>
<li>如果在函数的入口, 那么可以通过PT_REGS_PARM很方便读取到入参</li>
<li>如果在函数中间, 上面的方式就不在工作了, PT_REGS_PARM这些宏其实就是一些寄存器, 在函数中间入参所对应的寄存器可能已经被修改. 如果想要访问函数的入参或者局部变量,需要反汇编并找到对应的寄存器或者地址</li>
<li>如果是return probe, 这个时候的sp/bp已经是caller的栈了, 需要小心计算在栈上的偏移 bcc目前还不支持读取dwarf信息<br><img src="https://img-blog.csdnimg.cn/20210114201720154.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F1cWk5OQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">$sudo ./write_local.py</span><br><span class="line">a: 1, b: 2, uninit_c: 0, c: 80</span><br><span class="line">$./foo</span><br><span class="line">83</span><br><span class="line">#!/usr/bin/python</span><br><span class="line"></span><br><span class="line">from __future__ import print_function</span><br><span class="line">import bcc</span><br><span class="line">import ctypes as ct</span><br><span class="line"></span><br><span class="line">text = &quot;&quot;&quot;</span><br><span class="line">#include &lt;uapi/linux/ptrace.h&gt;</span><br><span class="line"></span><br><span class="line">struct data_t &#123;</span><br><span class="line">    int a;</span><br><span class="line">    int b;</span><br><span class="line">    int uninit_c;</span><br><span class="line">    int c;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">BPF_PERF_OUTPUT(events);</span><br><span class="line"></span><br><span class="line">int foo(struct pt_regs *ctx) &#123;</span><br><span class="line">    struct data_t data = &#123;&#125;;</span><br><span class="line">    int c = 80;</span><br><span class="line">    void *bp = (void *)ctx-&gt;bp;</span><br><span class="line"></span><br><span class="line">    data.a = PT_REGS_PARM1(ctx);</span><br><span class="line">    data.b = PT_REGS_PARM2(ctx);</span><br><span class="line">    bpf_probe_read(&amp;data.uninit_c, sizeof(data.uninit_c), bp - 4);</span><br><span class="line">    bpf_probe_write_user(bp - 4, &amp;c, 4);</span><br><span class="line">    bpf_probe_read(&amp;data.c, sizeof(data.c), bp - 4);</span><br><span class="line"></span><br><span class="line">    events.perf_submit(ctx, &amp;data, sizeof(struct data_t));</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">b = bcc.BPF(text=text)</span><br><span class="line">b.attach_uprobe(name=&quot;/home/wufei/work/test/foo&quot;, addr=0x40053a, fn_name=&quot;foo&quot;)</span><br><span class="line"></span><br><span class="line">class Data(ct.Structure):</span><br><span class="line">    _fields_ = [</span><br><span class="line">        (&quot;a&quot;, ct.c_int),</span><br><span class="line">        (&quot;b&quot;, ct.c_int),</span><br><span class="line">        (&quot;uninit_c&quot;, ct.c_int),</span><br><span class="line">        (&quot;c&quot;, ct.c_int),</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">def print_event(cpu, data, size):</span><br><span class="line">    event = ct.cast(data, ct.POINTER(Data)).contents</span><br><span class="line">    print(&quot;a: %d, b: %d, uninit_c: %d, c: %d&quot; % (event.a, event.b, event.uninit_c, event.c))</span><br><span class="line"></span><br><span class="line"># loop with callback to print_event</span><br><span class="line">b[&quot;events&quot;].open_perf_buffer(print_event)</span><br><span class="line">while 1:</span><br><span class="line">    b.kprobe_poll()</span><br></pre></td></tr></table></figure>
<p>似乎trace变量并不容易：</p>
<ul>
<li>一是python代码怎么反编译找到cn变量的位置呢？这种方法（python3 -m dis ./nova/compute/resource_tracker.py |grep ‘Disassembly of &lt;code object _update_available_resource’ -A 10）似乎是伪码。</li>
<li>cn变量不是基本变量，而是一个结构体，这样类似于应用态的systemtap一样结构体所依赖的结构体层层定义在bpf中，这样非常麻烦的。</li>
<li>传入参数似乎很容易打印，但也只是涉及基本变量，若是结构体也蛮麻烦的。</li>
<li>本例中的cn变量是一个全局变量，而且依赖于位置，更麻烦。</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://blog.csdn.net/hehuyi_in/article/details/108910781" target="_blank" rel="external">https://blog.csdn.net/hehuyi_in/article/details/108910781</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/30/Testing-OpenStack-NUMA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/30/Testing-OpenStack-NUMA/" itemprop="url">Testing OpenStack NUMA</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-12-30T12:13:20+08:00">
                2020-12-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华  发表于：2016-07-22<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )</p>
<p>虚机模拟NUMA硬件<br>我们可能没有NUMA硬件，但可以采用VM模拟。</p>
<p>1,  参考链接[1]打开KVM的嵌套虚拟化功能，Ubuntu KVM默认应该是开启的。</p>
<p>2, [可选] host机的网络如下，将仅有一块网卡eth0插到了br-phy中。</p>
<p>auto br-phy<br>allow-ovs br-phy<br>iface br-phy inet static<br>pre-up /usr/bin/ovs-vsctl – –may-exist add-br br-phy<br>pre-up /usr/bin/ovs-vsctl – –may-exist add-port br-phy eth0<br>  address 192.168.99.125<br>  gateway 192.168.99.1<br>  network 192.168.99.0<br>  netmask 255.255.255.0<br>  broadcast 192.168.99.255<br>ovs_type OVSBridge<br>ovs_ports eth0</p>
<p>#sudo ip -6 addr add 2001:2:3:4500:fa32:e4ff:febe:87cd/64 dev br-phy<br>iface br-phy inet6 static<br>pre-up modprobe ipv6<br>address 2001:2:3:4500:fa32:e4ff:febe:87cd<br>netmask 64<br>gateway 2001:2:3:4500::1</p>
<p>auto eth0<br>allow-br-phy eth0<br>iface eth0 inet manual<br>ovs_bridge br-phy<br>ovs_type OVSPort</p>
<p>然后让KVM支持Openvswitch网桥：</p>
<p>sudo ovs-vsctl add-br br-phy<br>sudo virsh net-destroy default<br>sudo virsh net-edit default</p>
<p><network><br>  <name>default</name><br>  <forward mode="bridge"><br>  <bridge name="br-phy"><br>  <virtualport type="openvswitch"><br></virtualport></bridge></forward></network><br>sudo virsh net-undefine default<br>sudo virsh net-autostart br-phy<br>3, 结合virt-manger安装一VM.</p>
<p>sudo apt-get install virt-viewer openvswitch-switch qemu-kvm libvirt-bin virt-manager virtinst virt-top  python-libvirt<br>sudo virt-install \<br>   –name openstack_demo \<br>   –ram 8096 \<br>   –vcpus 8 \<br>   –file /images/kvm/openstack_demo.img \<br>   –file-size 20 \<br>   –cdrom /images/iso/ubuntu-20.04-legacy-server-amd64.iso<br>4, 一定要在VM关闭的情况下修改VM的拓扑为NUMA拓扑。</p>
<p>sudo virsh destroy openstack_demo<br>sudo virsh edit openstack_demo</p>
<cpu mode="host-passthrough"><br>  <numa><br>    <cell id="0" cpus="0-3" memory="4096000"><br>    <cell id="1" cpus="4-5" memory="2048000"><br>    <cell id="2" cpus="6-7" memory="2048000"><br>  </cell></cell></cell></numa><br></cpu>

<p>sudo virsh start openstack_demo<br>5, VM内打开大页支持，因为openstack numa需要大页支持。</p>
<p>hua@demo:~$ cat  /etc/default/grub |grep GRUB_CMDLINE_LINUX<br>GRUB_CMDLINE_LINUX_DEFAULT=””<br>GRUB_CMDLINE_LINUX=”transparent_hugepage=never hugepagesz=2M hugepages=512 default_hugepagesz=2M”</p>
<h1 id="add-also-add-isolcpus-0-1-2-3"><a href="#add-also-add-isolcpus-0-1-2-3" class="headerlink" title="add also add isolcpus=0,1,2,3"></a>add also add isolcpus=0,1,2,3</h1><p>cat &lt;&lt; EOF | sudo tee -a /etc/fstab<br>nodev /mnt/huge hugetlbfs pagesize=2MB 0 0<br>EOF</p>
<p>sudo update-grub<br>sudo mkdir -p /mnt/huge<br>sudo reboot</p>
<p>#hua@demo:~$ sudo sysctl -w vm.nr_hugepages=512</p>
<p>#vm.nr_hugepages = 512<br>6, 这时我们看到：</p>
<p>ubuntu@demo:~$ numactl –hardware<br>available: 3 nodes (0-2)<br>node 0 cpus: 0 1 2 3<br>node 0 size: 3846 MB<br>node 0 free: 3115 MB<br>node 1 cpus: 4 5<br>node 1 size: 1969 MB<br>node 1 free: 1542 MB<br>node 2 cpus: 6 7<br>node 2 size: 1966 MB<br>node 2 free: 1466 MB<br>node distances:<br>node   0   1   2<br>  0:  10  20  20<br>  1:  20  10  20<br>  2:  20  20  10<br>ubuntu@demo:~$ grep Hugepagesize /proc/meminfo<br>Hugepagesize:       2048 kB<br>ubuntu@demo:~$ cat /proc/meminfo | grep Huge<br>AnonHugePages:         0 kB<br>ShmemHugePages:        0 kB<br>FileHugePages:         0 kB<br>HugePages_Total:     512<br>HugePages_Free:      512<br>HugePages_Rsvd:        0<br>HugePages_Surp:        0<br>Hugepagesize:       2048 kB<br>Hugetlb:         1048576 kB<br>ubuntu@demo:~$ sudo cat /sys/devices/system/node/node*/meminfo | grep -i huge<br>Node 0 AnonHugePages:         0 kB<br>Node 0 ShmemHugePages:        0 kB<br>Node 0 FileHugePages:        0 kB<br>Node 0 HugePages_Total:   171<br>Node 0 HugePages_Free:    171<br>Node 0 HugePages_Surp:      0<br>Node 1 AnonHugePages:         0 kB<br>Node 1 ShmemHugePages:        0 kB<br>Node 1 FileHugePages:        0 kB<br>Node 1 HugePages_Total:   171<br>Node 1 HugePages_Free:    171<br>Node 1 HugePages_Surp:      0<br>Node 2 AnonHugePages:         0 kB<br>Node 2 ShmemHugePages:        0 kB<br>Node 2 FileHugePages:        0 kB<br>Node 2 HugePages_Total:   170<br>Node 2 HugePages_Free:    170<br>Node 2 HugePages_Surp:      0<br>安装OpenStack<br>还需要配置pypi，详见：<a href="https://blog.csdn.net/quqi99/article/details/97622336" target="_blank" rel="external">https://blog.csdn.net/quqi99/article/details/97622336</a></p>
<p>git clone git://github.com/openstack-dev/devstack.git<br>cd devstack<br>cat &lt;&lt; EOF | sudo tee local.conf<br>[[local|localrc]]</p>
<p>#OFFLINE=True<br>GIT_BASE=<a href="http://git.trystack.cn" target="_blank" rel="external">http://git.trystack.cn</a><br>NOVNC_REPO=<a href="http://git.trystack.cn/kanaka/noVNC.git" target="_blank" rel="external">http://git.trystack.cn/kanaka/noVNC.git</a><br>SPICE_REPO=<a href="http://git.trystack.cn/git/spice/sice-html5.git" target="_blank" rel="external">http://git.trystack.cn/git/spice/sice-html5.git</a><br>DEST=/home/ubuntu/openstack<br>DATA_DIR=\$DEST/data<br>SERVICE_DIR=\$DEST/status</p>
<p>DOWNLOAD_DEFAULT_IMAGES=False<br>IMAGE_URLS=”<a href="http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img" target="_blank" rel="external">http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img</a>“</p>
<p>LOGFILE=\$DATA_DIR/logs/stack.log<br>VERBOSE=True</p>
<p>disable_service n-net<br>enable_service neutron q-svc q-dhcp q-l3 q-meta q-agt</p>
<p>MYSQL_PASSWORD=password<br>DATABASE_PASSWORD=password<br>SERVICE_TOKEN=password<br>SERVICE_PASSWORD=password<br>ADMIN_PASSWORD=password<br>RABBIT_PASSWORD=password</p>
<p>[[post-config|$NOVA_CONF]]<br>[DEFAULT]<br>firewall_driver=nova.virt.firewall.NoopFirewallDriver</p>
<p>[filter_scheduler]<br>enabled_filters=RamFilter,ComputeFilter,AvailabilityZoneFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,PciPassthroughFilter,NUMATopologyFilter<br>EOF</p>
<p>FORCE=yes ./stack.sh</p>
<p>sudo systemctl enable mysql.service<br>sudo systemctl enable rabbitmq-server.service<br>sudo systemctl enable apache2.service</p>
<p>. openrc admin</p>
<p>VM内安装配置OpenStack<br>1, VM内使用devstack安装OpenStack，略。但记得添加AggregateInstanceExtraSpecsFilter, NUMATopologyFilter到scheduler_default_filters （/etc/nova/nova.conf)中并重启nova-schedule进程。</p>
<p>2, 配置aggregate</p>
<p>nova aggregate-create hpgs-aggr<br>nova aggregate-set-metadata hpgs-aggr hpgs=true<br>nova aggregate-create normal-aggr<br>nova aggregate-set-metadata normal-aggr hpgs=false</p>
<p>#Add one or more hosts to them:<br>nova aggregate-add-host hpgs-aggr demo<br>hua@demo:~$ nova aggregate-show hpgs-aggr<br>+—-+———–+——————-+——–+————-+<br>| Id | Name      | Availability Zone | Hosts  | Metadata    |<br>+—-+———–+——————-+——–+————-+<br>| 1  | hpgs-aggr | -                 | ‘demo’ | ‘hpgs=true’ |<br>+—-+———–+——————-+——–+————-+<br>3, 配置flavor</p>
<p>nova flavor-create –ephemeral 0 –swap 0 –rxtx-factor 1.0 –is-public True m1.numa2nodes b8c065ce-90c2-41f9-8d50-1d47a040b494 256 1 2<br>nova flavor-key m1.numa2nodes set aggregate_instance_extra_specs:hpgs=true      #使用aggregate<br>nova flavor-key m1.numa2nodes set hw:mem_page_size=2048                         #配置大页支持</p>
<p>#nova flavor-key m1.numa2nodes set hw:cpu_policy=’dedicated’ hw:cpu_thread_policy=’isolate’</p>
<p>#nova flavor-key m1.numa2nodes set hw:emulator_threads_policy=’isolate’ hypervisor_type=’QEMU’</p>
<p>nova flavor-key m1.numa2nodes set hw:numa_nodes=2                               #配置numa_nodes，若不配置下列两行就是自动模型，配置下面两行为手动模式<br>nova flavor-key m1.numa2nodes set hw:numa_mem.0=128 hw:numa_mem.1=128 hw:numa_mempolicy=strict<br>nova flavor-key m1.numa2nodes set hw:numa_cpus.0=0  hw:numa_cpus.1=1 hw:cpu_policy=dedicated</p>
<p>hua@demo:/bak/openstack/nova$ nova flavor-show m1.numa2nodes |grep extra_specs<br>| extra_specs                | {“hw:cpu_policy”: “dedicated”, “hw:mem_page_size”: “2048”, “hw:numa_mempolicy”: “strict”, “hw:numa_mem.1”: “128”, “hw:numa_mem.0”: “128”, “hw:numa_nodes”: “2”, “aggregate_instance_extra_specs:hpgs”: “true”, “hw:numa_cpus.0”: “0”, “hw:numa_cpus.1”: “1”} |<br>4, 创建虚机</p>
<p>NET_ID=$(neutron net-list |grep ‘private’ |awk ‘{print $2}’)<br>nova boot –image cirros-0.5.1-x86_64-disk –flavor m1.numa2nodes –nic net-id=$NET_ID i1<br>5, 创建虚机后的大页情况，可以看到在node0与node1上已经各用了171-107=64块大页</p>
<p>hua@demo:/bak/openstack/nova$ sudo cat /sys/devices/system/node/node*/meminfo | grep -i huge<br>Node 0 AnonHugePages:         0 kB<br>Node 0 HugePages_Total:   171<br>Node 0 HugePages_Free:    107<br>Node 0 HugePages_Surp:      0<br>Node 1 AnonHugePages:         0 kB<br>Node 1 HugePages_Total:   171<br>Node 1 HugePages_Free:    107<br>Node 1 HugePages_Surp:      0<br>Node 2 AnonHugePages:         0 kB<br>Node 2 HugePages_Total:   170<br>Node 2 HugePages_Free:    170<br>Node 2 HugePages_Surp:      0<br>6, DB情况</p>
<p>mysql&gt; select numa_topology from instance_extra;<br>{<br>  “nova_object.version”: “1.2”,<br>  “nova_object.changes”: [<br>    “cells”<br>  ],<br>  “nova_object.name”: “InstanceNUMATopology”,<br>  “nova_object.data”: {<br>    “cells”: [<br>      {<br>        “nova_object.version”: “1.3”,<br>        “nova_object.changes”: [<br>          “cpu_topology”,<br>          “pagesize”,<br>          “cpuset”,<br>          “cpu_policy”,<br>          “memory”,<br>          “cpu_pinning_raw”,<br>          “id”,<br>          “cpu_thread_policy”<br>        ],<br>        “nova_object.name”: “InstanceNUMACell”,<br>        “nova_object.data”: {<br>          “pagesize”: 2048,<br>          “cpu_topology”: {<br>            “nova_object.version”: “1.0”,<br>            “nova_object.changes”: [<br>              “cores”,<br>              “threads”,<br>              “sockets”<br>            ],<br>            “nova_object.name”: “VirtCPUTopology”,<br>            “nova_object.data”: {<br>              “cores”: 1,<br>              “threads”: 1,<br>              “sockets”: 1<br>            },<br>            “nova_object.namespace”: “nova”<br>          },<br>          “cpuset”: [<br>            0<br>          ],<br>          “cpu_policy”: “dedicated”,<br>          “memory”: 128,<br>          “cpu_pinning_raw”: {<br>            “0”: 0<br>          },<br>          “id”: 0,<br>          “cpu_thread_policy”: null<br>        },<br>        “nova_object.namespace”: “nova”<br>      },<br>      {<br>        “nova_object.version”: “1.3”,<br>        “nova_object.changes”: [<br>          “cpu_topology”,<br>          “pagesize”,<br>          “cpuset”,<br>          “cpu_policy”,<br>          “memory”,<br>          “cpu_pinning_raw”,<br>          “id”,<br>          “cpu_thread_policy”<br>        ],<br>        “nova_object.name”: “InstanceNUMACell”,<br>        “nova_object.data”: {<br>          “pagesize”: 2048,<br>          “cpu_topology”: {<br>            “nova_object.version”: “1.0”,<br>            “nova_object.changes”: [<br>              “cores”,<br>              “threads”,<br>              “sockets”<br>            ],<br>            “nova_object.name”: “VirtCPUTopology”,<br>            “nova_object.data”: {<br>              “cores”: 1,<br>              “threads”: 1,<br>              “sockets”: 1<br>            },<br>            “nova_object.namespace”: “nova”<br>          },<br>          “cpuset”: [<br>            1<br>          ],<br>          “cpu_policy”: “dedicated”,<br>          “memory”: 128,<br>          “cpu_pinning_raw”: {<br>            “1”: 4<br>          },<br>          “id”: 1,<br>          “cpu_thread_policy”: null<br>        },<br>        “nova_object.namespace”: “nova”<br>      }<br>    ]<br>  },<br>  “nova_object.namespace”: “nova”<br>}<br>其他有用信息：</p>
<p>export MYSQL_PASSWORD=ChangeMe123<br>juju run –application mysql “mysql -u root -p$MYSQL_PASSWORD -e \”select <em> from nova.instances where host=’juju-38b529-ovn-6.cloud.sts’\G\””<br>juju run –application mysql “mysql -u root -p$MYSQL_PASSWORD -e \”select </em> from nova.instance_extra where instance_uuid=’0761d1de-7acd-4781-ae8b-f5ba864ab6ec’\G\””<br>juju run –application mysql “mysql -u root -p$MYSQL_PASSWORD -e \”select <em> from nova.compute_nodes where hypervisor_hostname=’p2-bits-cloud-xxx.maas’ or hypervisor_hostname=’p2-bits-cloud-xxx.maas’\G\””<br>juju run –application mysql “mysql -u root -p$MYSQL_PASSWORD -e \”select </em> from nova_api.request_specs where instance_uuid=’0761d1de-7acd-4781-ae8b-f5ba864ab6ec’\G\””<br>生成的虚机配置文件</p>
<p><domain type="kvm" id="1"><br>  <name>instance-00000001</name><br>  <uuid>3bddd8f6-ee18-4b44-a9e3-86ba2c6c25cc</uuid><br>  <metadata><br>    <nova:instance xmlns:nova="http://openstack.org/xmlns/libvirt/nova/1.0"><br>      <nova:package version="14.0.0"><br>      <nova:name>i1</nova:name><br>      <nova:creationtime>2016-07-22 03:46:03</nova:creationtime><br>      <nova:flavor name="m1.numa2nodes"><br>        <nova:memory>256</nova:memory><br>        <nova:disk>1</nova:disk><br>        <nova:swap>0</nova:swap><br>        <nova:ephemeral>0</nova:ephemeral><br>        <nova:vcpus>2</nova:vcpus><br>      </nova:flavor><br>      <nova:owner><br>        <nova:user uuid="e6001bf33a174ffb9d3ad3e9ff47d059">admin</nova:user><br>        <nova:project uuid="f5a578104510494da0ecae0fb514a6f1">demo</nova:project><br>      </nova:owner><br>      <nova:root type="image" uuid="d8184047-f7b3-4622-83e1-fb9d7ede807f"><br>    </nova:root></nova:package></nova:instance><br>  </metadata><br>  <memory unit="KiB">262144</memory><br>  <currentmemory unit="KiB">262144</currentmemory><br>  <memorybacking><br>    <hugepages><br>      <page size="2048" unit="KiB" nodeset="0"><br>      <page size="2048" unit="KiB" nodeset="1"><br>    </page></page></hugepages><br>  </memorybacking><br>  <vcpu placement="static">2</vcpu><br>  <cputune><br>    <shares>2048</shares><br>    <vcpupin vcpu="0" cpuset="0"><br>    <vcpupin vcpu="1" cpuset="4"><br>    <emulatorpin cpuset="0,4"><br>  </emulatorpin></vcpupin></vcpupin></cputune><br>  <numatune><br>    <memory mode="strict" nodeset="0-1"><br>    <memnode cellid="0" mode="strict" nodeset="0"><br>    <memnode cellid="1" mode="strict" nodeset="1"><br>  </memnode></memnode></memory></numatune><br>  <resource><br>    <partition>/machine</partition><br>  </resource><br>  <sysinfo type="smbios"><br>    <system><br>      <entry name="manufacturer">OpenStack Foundation</entry><br>      <entry name="product">OpenStack Nova</entry><br>      <entry name="version">14.0.0</entry><br>      <entry name="serial">4d63962c-1942-0164-e7a7-fa97578f4e3a</entry><br>      <entry name="uuid">3bddd8f6-ee18-4b44-a9e3-86ba2c6c25cc</entry><br>      <entry name="family">Virtual Machine</entry><br>    </system><br>  </sysinfo><br>  <os><br>    <type arch="x86_64" machine="pc-i440fx-wily">hvm</type><br>    <kernel>/opt/stack/data/nova/instances/3bddd8f6-ee18-4b44-a9e3-86ba2c6c25cc/kernel</kernel><br>    <initrd>/opt/stack/data/nova/instances/3bddd8f6-ee18-4b44-a9e3-86ba2c6c25cc/ramdisk</initrd><br>    <cmdline>root=/dev/vda console=tty0 console=ttyS0</cmdline><br>    <boot dev="hd"><br>    <smbios mode="sysinfo"><br>  </smbios></boot></os><br>  <features><br>    <acpi><br>    <apic><br>  </apic></acpi></features><br>  <cpu><br>    <topology sockets="2" cores="1" threads="1"><br>    <numa><br>      <cell id="0" cpus="0" memory="131072" unit="KiB" memaccess="shared"><br>      <cell id="1" cpus="1" memory="131072" unit="KiB" memaccess="shared"><br>    </cell></cell></numa><br>  </topology></cpu><br>  <clock offset="utc"><br>    <timer name="pit" tickpolicy="delay"><br>    <timer name="rtc" tickpolicy="catchup"><br>    <timer name="hpet" present="no"><br>  </timer></timer></timer></clock><br>  <on_poweroff>destroy</on_poweroff><br>  <on_reboot>restart</on_reboot><br>  <on_crash>destroy</on_crash><br>  <devices><br>    <emulator>/usr/bin/kvm-spice</emulator><br>    <disk type="file" device="disk"><br>      <driver name="qemu" type="qcow2" cache="none"><br>      <source file="/opt/stack/data/nova/instances/3bddd8f6-ee18-4b44-a9e3-86ba2c6c25cc/disk"><br>      <backingstore type="file" index="1"><br>        <format type="raw"><br>        <source file="/opt/stack/data/nova/instances/_base/5b06ec6b6abd700935b24a454e8ce3461d050a9f"><br>        <backingstore><br>      </backingstore><br>      <target dev="vda" bus="virtio"><br>      <alias name="virtio-disk0"><br>      <address type="pci" domain="0x0000" bus="0x00" slot="0x03" function="0x0"><br>    </address></alias></target></format></backingstore></driver></disk><br>    <disk type="file" device="cdrom"><br>      <driver name="qemu" type="raw" cache="none"><br>      <source file="/opt/stack/data/nova/instances/3bddd8f6-ee18-4b44-a9e3-86ba2c6c25cc/disk.config"><br>      <backingstore><br>      <target dev="hdd" bus="ide"><br>      <readonly><br>      <alias name="ide0-1-1"><br>      <address type="drive" controller="0" bus="1" target="0" unit="1"><br>    </address></alias></readonly></target></backingstore></driver></disk><br>    <controller type="usb" index="0"><br>      <alias name="usb"><br>      <address type="pci" domain="0x0000" bus="0x00" slot="0x01" function="0x2"><br>    </address></alias></controller><br>    <controller type="pci" index="0" model="pci-root"><br>      <alias name="pci.0"><br>    </alias></controller><br>    <controller type="ide" index="0"><br>      <alias name="ide"><br>      <address type="pci" domain="0x0000" bus="0x00" slot="0x01" function="0x1"><br>    </address></alias></controller><br>    <interface type="bridge"><br>      <mac address="fa:16:3e:0b:52:0b"><br>      <source bridge="qbref7fef26-3e"><br>      <target dev="tapef7fef26-3e"><br>      <model type="virtio"><br>      <alias name="net0"><br>      <address type="pci" domain="0x0000" bus="0x00" slot="0x02" function="0x0"><br>    </address></alias></model></target></mac></interface><br>    <serial type="file"><br>      <source path="/opt/stack/data/nova/instances/3bddd8f6-ee18-4b44-a9e3-86ba2c6c25cc/console.log"><br>      <target port="0"><br>      <alias name="serial0"><br>    </alias></target></serial><br>    <serial type="pty"><br>      <source path="/dev/pts/20"><br>      <target port="1"><br>      <alias name="serial1"><br>    </alias></target></serial><br>    <console type="file"><br>      <source path="/opt/stack/data/nova/instances/3bddd8f6-ee18-4b44-a9e3-86ba2c6c25cc/console.log"><br>      <target type="serial" port="0"><br>      <alias name="serial0"><br>    </alias></target></console><br>    <memballoon model="virtio"><br>      <stats period="10"><br>      <alias name="balloon0"><br>      <address type="pci" domain="0x0000" bus="0x00" slot="0x04" function="0x0"><br>    </address></alias></stats></memballoon><br>  </devices><br>  <seclabel type="dynamic" model="apparmor" relabel="yes"><br>    <label>libvirt-3bddd8f6-ee18-4b44-a9e3-86ba2c6c25cc</label><br>    <imagelabel>libvirt-3bddd8f6-ee18-4b44-a9e3-86ba2c6c25cc</imagelabel><br>  </seclabel><br></domain><br>20201208更新<br>1, create a VM with 3 numa nodes according to the doc [1]</p>
<cpu mode="host-passthrough"><br>  <numa><br>    <cell id="0" cpus="0-3" memory="4096000"><br>    <cell id="1" cpus="4-5" memory="2048000"><br>    <cell id="2" cpus="6-7" memory="2048000"><br>  </cell></cell></cell></numa><br></cpu>

<p>2, enable huge page, and use ‘isolcpus=0,1,2,3’ to just use cpu in numa0<br>注意：grub里定义isolcpus并不会让nova不使用这些cpu, nova里专门有vcpu_pin_set来做这件事。</p>
<p>$ cat /proc/cmdline |grep isolcpu<br>BOOT_IMAGE=/boot/vmlinuz-5.4.0-26-generic root=/dev/mapper/vgdemo-root ro transparent_hugepage=never hugepagesz=2M hugepages=512 default_hugepagesz=2M isolcpus=0,1,2,3</p>
<p>3, set up openstack test env.</p>
<p>4, create test aggreate and flavor and test vm</p>
<p>nova aggregate-create hpgs-aggr<br>nova aggregate-set-metadata hpgs-aggr hpgs=true<br>nova aggregate-create normal-aggr<br>nova aggregate-set-metadata normal-aggr hpgs=false<br>nova aggregate-add-host hpgs-aggr demo<br>nova aggregate-show hpgs-aggr</p>
<p>nova flavor-create –ephemeral 0 –swap 0 –rxtx-factor 1.0 –is-public True m1.numa2nodes b8c065ce-90c2-41f9-8d50-1d47a040b494 256 1 2<br>nova flavor-key m1.numa2nodes set aggregate_instance_extra_specs:hpgs=true<br>nova flavor-key m1.numa2nodes set hw:mem_page_size=2048<br>nova flavor-key m1.numa2nodes set hw:cpu_policy=’dedicated’ hw:cpu_thread_policy=’isolate’<br>nova flavor-key m1.numa2nodes set hw:emulator_threads_policy=’isolate’ hypervisor_type=’QEMU’</p>
<p>nova boot –image cirros-0.5.1-x86_64-disk –flavor m1.numa2nodes –nic net-id=$NET_ID i1</p>
<p>3, This instance i1 uses cpu 0 and 1, and use cpu 2 as emulatorpin</p>
<p>$ ps -eLo psr,args |grep qemu- |grep -v ‘grep’ |awk ‘{print $1}’ |sort -n |uniq<br>0<br>1<br>2</p>
<p>$ sudo virsh emulatorpin instance-00000001</p>
<h2 id="emulator-CPU-Affinity"><a href="#emulator-CPU-Affinity" class="headerlink" title="emulator: CPU Affinity"></a>emulator: CPU Affinity</h2><pre><code>*: 2
</code></pre><p>$ sudo virsh vcpuinfo instance-00000001<br>VCPU:           0<br>CPU:            0<br>State:          running<br>CPU time:       21.2s<br>CPU Affinity:   y——-</p>
<p>VCPU:           1<br>CPU:            1<br>State:          running<br>CPU time:       14.9s<br>CPU Affinity:   -y——</p>
<p>$ taskset -apc <code>pidof qemu-system-x86_64</code><br>pid 15014’s current affinity list: 2<br>pid 15016’s current affinity list: 2<br>pid 15022’s current affinity list: 2<br>pid 15023’s current affinity list: 0<br>pid 15025’s current affinity list: 1<br>pid 15039’s current affinity list: 2</p>
<p>sudo virsh dumpxml instance-00000001<br>  …<br>  <vcpu placement="static">2</vcpu><br>  <cputune><br>    <shares>2048</shares><br>    <vcpupin vcpu="0" cpuset="0"><br>    <vcpupin vcpu="1" cpuset="1"><br>    <emulatorpin cpuset="2"><br>  </emulatorpin></vcpupin></vcpupin></cputune><br>  <cpu mode="custom" match="exact" check="full"><br>    <model fallback="forbid">qemu64</model><br>    <topology sockets="2" cores="1" threads="1"><br>    <feature policy="require" name="x2apic"><br>    <feature policy="require" name="hypervisor"><br>    <feature policy="require" name="lahf_lm"><br>    <feature policy="disable" name="svm"><br>    <numa><br>      <cell id="0" cpus="0-1" memory="262144" unit="KiB" memaccess="shared"><br>    </cell></numa><br>  </feature></feature></feature></feature></topology></cpu></p>
<p>4, host’s numa_toplogy</p>
<p>select numa_topology from nova_cell1.compute_nodes where host=’demo’ \G;</p>
<p>see <a href="https://paste.ubuntu.com/p/MjmMHZxS6s/" target="_blank" rel="external">https://paste.ubuntu.com/p/MjmMHZxS6s/</a></p>
<p>5, instance’s numa_toplogy</p>
<p>select numa_topology from instance_extra where instance_uuid in (select uuid from instances where host=’demo’) \G;</p>
<p>see <a href="https://paste.ubuntu.com/p/CP2t2ghCfH/" target="_blank" rel="external">https://paste.ubuntu.com/p/CP2t2ghCfH/</a></p>
<p>参考<br>[1] <a href="http://docs.openstack.org/developer/devstack/guides/devstack-with-nested-kvm.html" target="_blank" rel="external">http://docs.openstack.org/developer/devstack/guides/devstack-with-nested-kvm.html</a></p>
<p>[2] <a href="https://docs.openstack.org/nova/rocky/contributor/testing/libvirt-numa.html" target="_blank" rel="external">https://docs.openstack.org/nova/rocky/contributor/testing/libvirt-numa.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/28/回忆pdb基础/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/28/回忆pdb基础/" itemprop="url">回忆pdb基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-12-28T10:06:02+08:00">
                2020-12-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华  发表于：2015-05-05<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )<br>人啊，总要不断地与记忆作斗争。</p>
<p>1, 加-g参数编译可执行性文件, 如果没有-g，你将看不见程序的函数名、变量名，所代替的全是运行时的内存地址。<br>   cc -g test.c -o test<br>   静态检查程序错误，splint test.c<br>   单元测试可使用CUnit, gcc -o test.o -I /usr/local/include/CUnit -L /usr/local/lib/ -g testcase.c -l cunit ./test.o</p>
<p>   若是要调试master可以下载源码用./configure前加上CFLAGS=”-Wall -O2 -g”即可编译。若是ubuntu repo里已经有的package，则直接安装dbg即可编译(没有dbg包时可次优选择dbgsym包）。<br>2, 启动gdb, gdb test<br>   启动GDB的方法有以下几种：<br>    a、gdb <program><br>       program也就是你的执行文件，一般在当然目录下。<br>    b、gdb <program> core<br>       用gdb同时调试一个运行程序和core文件，core是程序非法执行后core dump后产生的文件。<br>       如果产生coredump呢？ 需要执ulimit -c unlimited才可以(缺省是coredump文件大小是0字节所以它不生成）。<br>       并不是所有的异常都会生成coredump文件，附录1是一个运行异常时会产生coredump文件的例子。<br>       运行后coredump文件产生在当前文件夹的core文件中。分析它： gdb ./test ./core, 如下显示了是哪一句出了错误：<br>       (gdb) bt</program></program></p>
<pre><code>   #0  0x000000000040054d in core_dump () at test.c:5
   #1  0x0000000000400585 in main () at test.c:10
c、gdb &lt;program&gt; &lt;PID&gt;
   通过sudo gdb启动gdb后直接attach &lt;PID&gt;可以attach到正在运行的程序，当然程序得加-g参数编译的。
</code></pre><p>3, 一些常用gdb命令<br>   l, 列出源代码</p>
<p>   break 16, 第16行设置断点<br>   b fn1 if a&gt;b, 条件设置断点<br>   break func, 在函数的入口处设置断点<br>   delete/disable/enable/clear 16, 删除第16个断点<br>   delete breakpoints，清除所有断点<br>   info break, 查看断点<br>   r, 运行程序，到断点处处时会停止运行等待用户输入（run), 在gdb中正常结束的程序也可以用此命令重新运行<br>   n, 单步执行(next)<br>   s, 进入函数(step)<br>   c, 继续运行(continue), 到下一个断点处<br>   p <variable>, 打印变量的值<br>   set var i=1, 修改变量的值<br>   info registers, 查看寄存器<br>   bt, 查看函数堆栈<br>   call 函数(参数),调用程序中可见的函数，并传递“参数”，如：call gdb_test(55)<br>   display 表达式：在单步运行时将非常有用，它将在每次单步进行指令后，紧接着输出被设置的表达式及值。如： display a<br>   watch 表达式：设置一个监视点，一旦被监视的“表达式”的值改变，gdb将强行终止正在被调试的程序。如：watch a<br>   q, 退出<br>4, 使用GDB调试多进程程序, <a href="https://www.ibm.com/developerworks/cn/linux/l-cn-gdbmp/" target="_blank" rel="external">https://www.ibm.com/developerworks/cn/linux/l-cn-gdbmp/</a></variable></p>
<p>5, 使用cgdb可以很方便的查看代码。<br>6, 性能分析, 比如哪个函数调用了多少次，被谁调用了， 平均每次调用花费多少时间等。这时候要用gprof,gprof是分析profile输出的。<br>   要想执行时输出profile文件编译时要加-pg选项, gcc -o helloworld.o -pg -g helloworld.c<br>   执行上面语句后会在当前目录下生成gmon.out文件, 然后用gprof去读取并显示出来，gprof -b -A -p -q helloworld.o gmon.out &gt;prof_info.txt </p>
<p>7, gdb也可以调试go语言的程序，采用”go build -gcflags “-N -l” test.go“编译，再’gdb test”即可。可能相比于C会多出一个‘info goroutines’命令。</p>
<p>附录1, 产生coredump文件的例子</p>
<p>#include <stdio.h><br>int core_dump() {<br>    int i;<br>    for (i = 5; i &gt;= 0; i–) {<br>        printf(“(%d, %d)\n”, i, 100 / i);<br>    }<br>    return 0;<br>}<br>int main() {<br>    core_dump();<br>    return 0;<br>}</stdio.h></p>
<p>附录2, 如何分析CoreDump文件</p>
<p>1, 安装dbgsym，注意安装的是librados2-dbg librbd1-dbg， 而不是librados2-dbgsym librbd1-dbgsym. （注意：有dbg内建包时优先安装dbg包，没有时才安装dbgsym非内建包，这点异常重要，否则会造成gdb调试时缺失符号表，这个网页有解释： <a href="https://wiki.ubuntu.com/DebuggingProgramCrash#Debug_Symbol_Packages）" target="_blank" rel="external">https://wiki.ubuntu.com/DebuggingProgramCrash#Debug_Symbol_Packages）</a><br>echo “deb <a href="http://ddebs.ubuntu.com" target="_blank" rel="external">http://ddebs.ubuntu.com</a> $(lsb_release -cs) main restricted universe multiverse” | sudo tee -a /etc/apt/sources.list.d/debuginfo_debs.list<br>echo “deb <a href="http://ddebs.ubuntu.com" target="_blank" rel="external">http://ddebs.ubuntu.com</a> $(lsb_release -cs)-updates main restricted universe multiverse” | sudo tee -a /etc/apt/sources.list.d/debuginfo_debs.list<br>sudo apt-key adv –keyserver keyserver.ubuntu.com –recv-keys C8CAB6595FDFF622<br>sudo apt update<br>sudo apt-get install qemu-system-x86-dbgsym librados2-dbg librbd1-dbg</p>
<p>For example: /usr/lib/x86_64-linux-gnu/libstdc++.so.6 and the debugging version is at /usr/lib/x86_64-linux-gnu/debug/libstdc++.so.6</p>
<p>(gdb) set debug-file-directory /usr/lib/x86_64-linux-gnu/debug/</p>
<p>(gdb) directory /xxx/qemu/qemu-2.5+dfsg     #sometimes directory doesn’t take effect, so use the following line</p>
<p>(gdb) set substitute-path /build/qemu-e2kucK/qemu-2.5+dfsg /xxx/qemu/qemu-2.5+dfsg</p>
<p>(gdb) l</p>
<p>(gdb) info sharedlibrary</p>
<p>2, 解压 - apport-unpack _usr_bin_qemu-system-x86_64.64055.crash ./tmp/</p>
<p>3, 分析CoreDump (确保/usr/bin/qemu-system-x86_64与版本与之一致）<br>   gdb /usr/bin/qemu-system-x86_64 ./tmp/CoreDump</p>
<p>4, 查看堆栈<br>(gdb) bt 1</p>
<p>#0  malloc_consolidate (av=av@entry=0x7fa810000020) at malloc.c:4175</p>
<p>5, 查看汇编，上面的malloc.c:4175可能由于编译器优化和源代码对不上，所以仍然需要通过编译代码一一比对<br>(gdb) p $rip<br>$5 = (void (*)()) 0x7fa84c275470 <malloc_consolidate+336><br>(gdb) x/2gi $rip<br>   0x7fa84c275320 <malloc_consolidate>: cmpq   $0x0,0x3484d0(%rip)        # 0x7fa84c5bd7f8 <global_max_fast><br>   0x7fa84c275328 <malloc_consolidate+8>: je     0x7fa84c275a8b <malloc_consolidate+1899></malloc_consolidate+1899></malloc_consolidate+8></global_max_fast></malloc_consolidate></malloc_consolidate+336></p>
<p>6, 采用汇编理解出错代码，结合源代码辅助分析（因为编译器会优化代码，所以不能完全相信CoreDump中的行号）</p>
<p>注：UCA缺乏debug symbol，例如为trusty上的mitaka uca的qemu包构建debug symbol，因为都是ppa编译的其编译器版本及设置都差不多这样编译出的debug symbol还可以近似地去用。<br>1, PPA中有”Build debug symbols”的选项（<a href="https://launchpad.net/~zhhuabj/+archive/ubuntu/trusty-mitaka-sru/+edit），再在" target="_blank" rel="external">https://launchpad.net/~zhhuabj/+archive/ubuntu/trusty-mitaka-sru/+edit），再在</a> “Add PPA dependency”里添加”~ubuntu-cloud-archive/+archive/ubuntu/mitaka-staging”</p>
<p>2, 进这个页面(<a href="https://launchpad.net/~ubuntu-cloud-archive/+archive/ubuntu/mitaka-staging），点击&quot;View" target="_blank" rel="external">https://launchpad.net/~ubuntu-cloud-archive/+archive/ubuntu/mitaka-staging），点击&quot;View</a> package details” -&gt; “copy packages”，在搜索框输入qemu并同时将其后的下拉列表从Published改为Superseded这样就可以选择客户所用的qemu版本。</p>
<h1 id="trusty-doesn’t-support-mitaka-uca-sudo-add-apt-repository-cloud-archive-mitaka-but-it-supports-mitaka-staging"><a href="#trusty-doesn’t-support-mitaka-uca-sudo-add-apt-repository-cloud-archive-mitaka-but-it-supports-mitaka-staging" class="headerlink" title="trusty doesn’t support mitaka uca (sudo add-apt-repository cloud-archive:mitaka), but it supports mitaka-staging"></a>trusty doesn’t support mitaka uca (sudo add-apt-repository cloud-archive:mitaka), but it supports mitaka-staging</h1><p>sudo add-apt-repository ppa:ubuntu-cloud-archive/mitaka-staging</p>
<p>sudo add-apt-repository ppa:xxxx</p>
<p>sudo apt update</p>
<p>sudo apt install qemu-system-x86=1:2.5+dfsg-5ubuntu10.16~cloud0</p>
<h1 id="seems-can’t-use-‘apt-cache’-to-qeury-dbgsym-in-PPA-so-change-to-use-wget"><a href="#seems-can’t-use-‘apt-cache’-to-qeury-dbgsym-in-PPA-so-change-to-use-wget" class="headerlink" title="seems can’t use ‘apt-cache’ to qeury dbgsym in PPA, so change to use wget"></a>seems can’t use ‘apt-cache’ to qeury dbgsym in PPA, so change to use wget</h1><p>wget <a href="http://ppa.launchpad.net/xxx/sf177500/ubuntu/pool/main/q/qemu/qemu-system-x86-dbgsym_2.5+dfsg-5ubuntu10.16~cloud0_amd64.ddeb" target="_blank" rel="external">http://ppa.launchpad.net/xxx/sf177500/ubuntu/pool/main/q/qemu/qemu-system-x86-dbgsym_2.5+dfsg-5ubuntu10.16~cloud0_amd64.ddeb</a><br>sudo dpkg -i qemu-system-x86-dbgsym_2.5+dfsg-5ubuntu10.16~cloud0_amd64.ddeb<br>sudo apt install qemu-system-x86-dbgsym=1:2.5+dfsg-5ubuntu10.16~cloud0<br>gdb /usr/bin/qemu-system-x86_64 10Apr18-instance-000a2f60-qemu-system-x86_64.core.2003745</p>
<p>#GDB调试<br>sysctl kernel.core_pattern<br>sudo service apport start force_start=1 enabled=1<br>grep enabled /etc/default/apport<br>sudo killall -SIGSEGV ovs-vswitchd<br>sudo cat /var/log/apport.log<br>sudo apt install openvswitch-dbg cgdb</p>
<p>#cgdb $(which ovs-vswitchd) $(pidof ovs-vswitchd)<br>cgdb ovs-vswitchd /var/crash/_usr_lib_openvswitch-switch_ovs-vswitchd.0.crash</p>
<p>但是上面killall生成coredump的方法会让ovs停掉，下面的不会：</p>
<p>sudo apt install gdb openvswitch-dbg -y<br>sudo gcore <code>pidof ovs-vswitchd</code><br>sudo gdb –core ./core.16524 /usr/sbin/ovs-vswitchd</p>
<p>sudo tail -f /var/log/openvswitch/ovs-vswitchd.log | awk ‘/blocked 1000 ms waiting for main to quiesce/ {system(“sudo /usr/bin/gcore <code>pidof ovs-vswitchd</code>“)}’<br>thread apply all bt<br>info proc mapping   #query all sysbol tables</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/12/RabbitMQ-Deep-Dive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/12/RabbitMQ-Deep-Dive/" itemprop="url">RabbitMQ Deep Dive</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-12-12T12:01:46+08:00">
                2020-12-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华  发表于：2015-06-03<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )<br>AMQP概念<br>通过消息机制，可以实现数据传输，非阻塞型操作，推送通知，发布/订阅，异步处理，work队列。<br>AMQP当中有四个概念非常重要：虚拟主机（virtual host），交换机（exchange），队列（queue）和绑定（binding）。</p>
<p>virutal host相当于namespace，用于不同tenant之间的exchange, queue, binding的隔离。<br> Queue队列, 每个消息都会被投入到一个或多个队列。消息就一直在里面，直到有客户端（也就是消费者，Consumer）连接到这个队列并且将其取走为止。队列是由消费者（Consumer）通过程序建立的，不是通过配置文件或者命令行工具。<br>Binding绑定, 它的作用就是把exchange和queue按照路由规则绑定起来。<br>Routing_Key路由关键字：exchange根据这个关键字进行消息投递。<br>Channele消息通道：在客户端的每个连接里，可建立多个channel，每个channel代表一个会话任务。<br>Exchange交换机，对消息进行路由，当收到Publisher传递给它的消息后，Excahnge会根据路由键routing_key决定将消息加入到哪些消息队列中。<br>消息的类型：</p>
<p>Direct Exchange – 处理路由键。需要将一个队列绑定到交换机上，要求该消息与一个特定的路由键完全匹配。一对一交换类型。<br>Topic Exchange – 将路由键和某模式进行匹配。此时队列需要绑定要一个模式上。符号“#”匹配一个或多个词，符号“*”匹配不多不少一个词。一对多主题多播交换类型。<br>Fanout Exchange – 不处理路由键。你只需要简单的将队列绑定到交换机上。一个发送到交换机的消息都会被转发到与该交换机绑定的所有队列上。一对多广播交换类型。</p>
<p>RabbitMQ简介与特点<br>RabbitMQ是一个开源的AMQP协议的实现，它具有如下特点：可靠性（Reliability）, RabbitMQ使用一些机制来保证程序的可靠性，如持久化、传输确认机制、发布确认、高可用性。灵活的路由机制（Flexible Routing）, 在消息进入队列之前，通过Exchange来路由消息的。对于典型的路由功能，RabbitMQ已经提供了一些内置的Exchange来实现。针对更复杂的路由功能，可以将多个Exchange绑定在一起，也通过插件机制实现自己的Exchange。消息集群（Clustering）多个RabbitMQ服务器可以组成一个集群，形成单个逻辑Broker。Federation, For servers that need to be more loosely and unreliably connected than clustering allows, RabbitMQ offers a federation model.队列高可用（Highly Available Queues）, 队列可以在集群中的机器上进行镜像，以确保在硬件问题下还保证消息安全。多种协议的支持（Multi-protocol）, RabbitMQ支持多种消息队列协议。</p>
<p>一个rabbitmq python例子</p>
<p>#coding:utf-8<br>import sys<br>from amqplib import client_0_8 as amqp<br>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    if (len(sys.argv) &lt;= 1):<br>        ispublisher = ‘0’<br>        print “Then pls run ‘rabbittest 1’ to sent message.”<br>    else:<br>        ispublisher = sys.argv[1]<br>    conn = amqp.Connection(host=”localhost:5672 “, userid=”guest”, password=”password”, virtual_host=”/“, insist=False)</p>
<pre><code># 每个channel都被分配了一个整数标识
chan = conn.channel()
# 创建一个队列，它是durable的（重启后会重新建立）a
# 消费者断开时不会自动删除（auto_delte=False)
chan.queue_declare(queue=&quot;queue1&quot;, durable=True, exclusive=False, auto_delete=False)
# 创建交换机，参数意思和上面的队列是一样的，还有一个type类型：fanout, direct, topic
chan.exchange_declare(exchange=&quot;switch1&quot;, type=&quot;direct&quot;,
                      durable=True, auto_delete=False,)
# 绑定交换机和队列
chan.queue_bind(queue=&quot;queue1&quot;, exchange=&quot;switch1&quot;, routing_key=&quot;key1&quot;)
if (ispublisher == &apos;1&apos;):
    # 生产者
    msg = amqp.Message(&quot;Test message!&quot;)
    msg.properties[&quot;delivery_mode&quot;] = 2
    chan.basic_publish(msg, exchange=&quot;switch1&quot;, routing_key=&quot;key1&quot;)
else:
    # 主动从队列拉消息
    msg = chan.basic_get(&quot;queue1&quot;)
    print msg.body
    chan.basic_ack(msg.delivery_tag)
    # 消息来了通知回调
    # 如果no_ack=True可以使用chan.basic_ack()人工确认，使用delivery_tag参数
    def recv_callback(msg):
        print &apos;Received: &apos; + msg.body
    chan.basic_consume(queue=&apos;queue1&apos;, no_ack=False,
                       callback=recv_callback, consumer_tag=&quot;testtag&quot;)
    # chan.basic_cancel(&quot;testtag&quot;) # 取消回调函数
    while True:
        chan.wait()  # 等待在队列上，直到下一个消息到达队列。
chan.close()
conn.close()
</code></pre><p>RabbitMQ CLI<br>安装，sudo apt-get install rabbitmq-server<br>重启，sudo service rabbitmq-server restart<br>sudo rabbitmqctl list_vhostssudo rabbitmqctl add_vhost demo<br>sudo rabbitmqctl list_users<br>sudo rabbitmqctl add_user test password<br>sudo rabbitmqctl change_password test password<br>sudo rabbitmqctl clear_password test<br>sudo rabbitmqctl list_user_permissions test<br>sudo rabbitmqctl set_permissions -p demo test “.<em>“ “.</em>“ “.*”<br>sudo rabbitmqctl clear_permissions -p demo test<br>sudo rabbitmqctl list_queues -p demo name durable auto_delete slave_pids synchronised_slave_pids<br>sudo rabbitmqadmin delete queue name=’queuename’<br>sudo rabbitmqctl list_exchanges -p demosudo rabbitmqctl list_bindings -p demo<br>sudo rabbitmqctl list_consumers -p demosudo rabbitmqctl statussudo rabbitmqctl report</p>
<h1 id="fileds-can-be-name-durable-auto-delete-arguments-policy-pid-owner-pid-exclusive-exclusive-consumer-pid-exclusive-consumer-tag-messages-ready-messages-unacknowledged-messages-messages-ready-ram-messages-unacknowledged-ram-messages-ram-messages-persistent-message-bytes-message-bytes-ready-message-bytes-unacknowledged-message-bytes-ram-message-bytes-persistent-head-message-timestamp-disk-reads-disk-writes-consumers-consumer-utilisation-memory-slave-pids-synchronised-slave-pids-state"><a href="#fileds-can-be-name-durable-auto-delete-arguments-policy-pid-owner-pid-exclusive-exclusive-consumer-pid-exclusive-consumer-tag-messages-ready-messages-unacknowledged-messages-messages-ready-ram-messages-unacknowledged-ram-messages-ram-messages-persistent-message-bytes-message-bytes-ready-message-bytes-unacknowledged-message-bytes-ram-message-bytes-persistent-head-message-timestamp-disk-reads-disk-writes-consumers-consumer-utilisation-memory-slave-pids-synchronised-slave-pids-state" class="headerlink" title="fileds can be:  [name, durable, auto_delete, arguments, policy, pid, owner_pid, exclusive, exclusive_consumer_pid, exclusive_consumer_tag, messages_ready, messages_unacknowledged, messages, messages_ready_ram, messages_unacknowledged_ram, messages_ram, messages_persistent, message_bytes, message_bytes_ready, message_bytes_unacknowledged, message_bytes_ram, message_bytes_persistent, head_message_timestamp, disk_reads, disk_writes, consumers, consumer_utilisation, memory, slave_pids, synchronised_slave_pids, state]"></a>fileds can be:  [name, durable, auto_delete, arguments, policy, pid, owner_pid, exclusive, exclusive_consumer_pid, exclusive_consumer_tag, messages_ready, messages_unacknowledged, messages, messages_ready_ram, messages_unacknowledged_ram, messages_ram, messages_persistent, message_bytes, message_bytes_ready, message_bytes_unacknowledged, message_bytes_ram, message_bytes_persistent, head_message_timestamp, disk_reads, disk_writes, consumers, consumer_utilisation, memory, slave_pids, synchronised_slave_pids, state]</h1><p>sudo rabbitmqctl list_queues name slave_pids synchronised_slave_pids durable -p openstack<br>RabbitMQ GUI</p>
<p>Enable it, sudo rabbitmq-plugins enable rabbitmq_management<br><a href="http://localhost:15672" target="_blank" rel="external">http://localhost:15672</a>  (guest/guest)<br>RabbitMQ配置文件<br><a href="http://www.rabbitmq.com/configure.html#configuration-file" target="_blank" rel="external">http://www.rabbitmq.com/configure.html#configuration-file</a><br>sudo find / -name rabbitmq.config*<br>sudo mv /usr/share/doc/rabbitmq-server/rabbitmq.config.example.gz /etc/rabbitmq/cd /etc/rabbitmq/ &amp;&amp; sudo gunzip rabbitmq.config.example.gz<br>sudo mv rabbitmq.config.example rabbitmq.config<br>RabbitMQ调优</p>
<p>1, 流控(Flow Control)机制，默认RabbitMQ在使用机器的40%以上的内存时流控机制会起作用block掉所有连接。故确保使用64位操作系统与64位Erlang VM。当RabbitMQ是集群情况下，当其中有一台机器硬盘不足的时候，所有节点的producer链接也会被阻止。</p>
<p>rabbitmqctl  set_vm_memory_high_watermark 0.4<br>rabbitmqctl set_vm_memory_high_watermark_paging_ratio 0.75<br>rabbitmqctl status<br><a href="http://www.rabbitmq.com/memory.html" target="_blank" rel="external">http://www.rabbitmq.com/memory.html</a><br>Max open files，/etc/default/rabbitmq-server<br>ulimit -n 65535<br>cat /proc/$RABBITMQ_BEAM_PROCESS_PID/limits<br>2, Erlang的Hipe优化, 可以设置hipe_compiles设置。可以看到有20-50%的性能优化。而你只需要付出1分钟左右的延迟启动。 HiPE需要你检查是否编译进入你的Erlang安装环境。Ubuntu，需要安装erlang-base-hipe.默认有些平台不支持。如果Erlang VM segfaults,请关闭这个选项。</p>
<p>[{rabbit, [{hipe_compile, true}]}].<br>RabbitMQ集群</p>
<p>跨三个节点部署 RabbitMQ 集群和镜像消息队列。可以使用 HAProxy 提供负载均衡，或者将 RabbitMQ host list 配置给 OpenStack 组件（使用 rabbit_hosts 和 rabbit_ha_queues 配置项）。</p>
<p>先看第一种方式（采用HAproxy）：</p>
<h1 id="每个节点上执行下列命令配置RabbitMQ集群"><a href="#每个节点上执行下列命令配置RabbitMQ集群" class="headerlink" title="每个节点上执行下列命令配置RabbitMQ集群"></a>每个节点上执行下列命令配置RabbitMQ集群</h1><h1 id="根据需要设置当前节点的工作模式-ram-disk-，ROOT-NODE-HOSTNAME为集群根节点的主机名，注意在此必须使用主机名"><a href="#根据需要设置当前节点的工作模式-ram-disk-，ROOT-NODE-HOSTNAME为集群根节点的主机名，注意在此必须使用主机名" class="headerlink" title="根据需要设置当前节点的工作模式(ram/disk)，ROOT_NODE_HOSTNAME为集群根节点的主机名，注意在此必须使用主机名"></a>根据需要设置当前节点的工作模式(ram/disk)，ROOT_NODE_HOSTNAME为集群根节点的主机名，注意在此必须使用主机名</h1><p>apt-get install rabbitmq-server<br>rabbitmq-server -detached                   #detached为后台运行别占据终端<br>echo ‘MYRABBITMQCLUSTERABC’ &gt; /var/lib/rabbitmq/.erlang.cookie<br>chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie<br>chmod 400 /var/lib/rabbitmq/.erlang.cookie<br>/usr/lib/rabbitmq/bin/rabbitmq-plugins enable rabbitmq_management<br>/usr/sbin/rabbitmqctl stop_app<br>/usr/sbin/rabbitmqctl reset<br>/usr/sbin/rabbitmqctl join_cluster –ram rabbit@${ROOT_NODE_HOSTNAME}<br>/usr/sbin/rabbitmqctl start_app<br>service rabbitmq-server restart</p>
<h1 id="在主节点上添加用户"><a href="#在主节点上添加用户" class="headerlink" title="在主节点上添加用户"></a>在主节点上添加用户</h1><p>/usr/sbin/rabbitmqctl add_user web_admin password<br>/usr/sbin/rabbitmqctl add_user mgmt_admin password<br>/usr/sbin/rabbitmqctl set_user_tags username monitoring<br>/usr/sbin/rabbitmqctl set_user_tags mgmt_admin administrator<br>/usr/sbin/rabbitmqctl rabbitmqctl list_users<br>/usr/sbin/rabbitmqctl set_permissions -p / mgmt_admin “.<em>“ “.</em>“ “.*”</p>
<h1 id="设置HAProxy-需要设置成镜像队列，可以访问http-192-168-64-87-8888，用户名web-admin-password进行访问"><a href="#设置HAProxy-需要设置成镜像队列，可以访问http-192-168-64-87-8888，用户名web-admin-password进行访问" class="headerlink" title="设置HAProxy, 需要设置成镜像队列，可以访问http://192.168.64.87:8888，用户名web_admin/password进行访问"></a>设置HAProxy, 需要设置成镜像队列，可以访问<a href="http://192.168.64.87:8888，用户名web_admin/password进行访问" target="_blank" rel="external">http://192.168.64.87:8888，用户名web_admin/password进行访问</a></h1><p>/usr/sbin/rabbitmqctl set_policy ha-all “^” ‘{“ha-mode”:”all”}’<br>修改文件：/etc/haproxy/haproxy.cfg<br>listen rabbitmq_cluster 0.0.0.0:5672<br>mode tcp<br>balance roundrobin<br>server   node1 192.168.1.1:5672 check inter 2000 rise 2 fall 3<br>/usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -D<br>第二种使用 rabbit_hosts 和 rabbit_ha_queues 配置项：</p>
<p>rabbit_hosts = rabbit1:5672,rabbit2:5672<br>rabbit_host = rabbit1<br>rabbit_ha_queues = true<br>如果配置了rabbit_hosts，那么nova将会按照顺序连接一个RabbitMQ服务，如果正在使用的MQ服务断开则依次尝试连接下一个，由于所有MQ的消息都是同步的，所以消息不会丢失。<br>如果配置了rabbit_host，那么需要在集群前架设haproxy，保证集群VIP服务正常。</p>
<p>confirm that actual queue is connected and can consume that queue.  </p>
<p>sudo rabbitmq-plugins enable rabbitmq_management<br>wget <a href="http://127.0.0.1:15672/cli/rabbitmqadmin" target="_blank" rel="external">http://127.0.0.1:15672/cli/rabbitmqadmin</a> &amp;&amp; chmod 777 rabbitmqadmi<br>sudo rabbitmqctl add_user test password<br>sudo rabbitmqctl set_user_tags test administrator<br>sudo rabbitmqctl set_permissions -p openstack test “.<em>“ “.</em>“ “.*”<br><a href="http://10.5.0.6:15672/#/queues/openstack/compute.juju-a09725-xenial-mitaka-7" target="_blank" rel="external">http://10.5.0.6:15672/#/queues/openstack/compute.juju-a09725-xenial-mitaka-7</a><br>./rabbitmqadmin publish -V openstack -u test -p password exchange=nova routing_key=compute.juju-a09725-xenial-mitaka-7 payload=”test”<br>tail -f /var/log/nova/nova-compute.log</p>
<p>具体地见：<a href="http://m.blog.csdn.net/blog/gtt116/21083533" target="_blank" rel="external">http://m.blog.csdn.net/blog/gtt116/21083533</a><br>Debug Hacks<br>$ tshark -r xxx.pcap |awk ‘{arr[$5]++}END{for (a in arr) print a, arr[a]}’ |sort -n -k 2 -r | head -n 3<br>10.55.74.103 62756<br>10.55.74.142 12976<br>10.55.74.139 12228<br>juju run -u rabbitmq-server/0 ‘sudo rabbitmqctl list_queues -p openstack|grep -wv 0’</p>
<p>watch -c “sudo rabbitmqctl list_queues -p openstack | grep -E ‘log|neutron|agent’”<br>Reset rabbitmq slave<br>1) On juju-3182a3-69-lxd-2, back mnesia, stop the service<br>$ sudo mv /var/lib/rabbitmq/mnesia /var/lib/rabbitmq/mnesia-back<br>$ sudo service rabbitmq-server stop<br>2) Forget the cluster nodes from the rabbit master node<br>$ sudo rabbitmqctl stop_app<br>$ sudo rabbitmqctl forget_cluster_node rabbit@juju-3182a3-69-lxd-2<br>$ sudo rabbitmqctl start_app<br>如何恢复systemd管理的native mirror rabbitmq cluster</p>
<p>如何恢复systemd管理的native mirror rabbitmq cluster<br>1, 确保在3个节点上，rabbitmq-server先由systemd启动(随后会由pacemaker接管)，这样能可能运行rabbitmqctl cluster_status命令．假设此时3个节点各自为政．<br>juju run –application=rabbitmq-server ‘sudo rabbitmqctl cluster_status’</p>
<p>2, juju status看有没有error状态，例如现在看到rabbitmq-server/1因为下列日志为error状态, rabbitmq-server/1上运行：<br><a href="https://www.jianshu.com/p/498c63e4ace1" target="_blank" rel="external">https://www.jianshu.com/p/498c63e4ace1</a><br><a href="https://ywnz.com/linuxyffq/3899.html" target="_blank" rel="external">https://ywnz.com/linuxyffq/3899.html</a><br>2020-02-21 09:24:09 DEBUG config-changed subprocess.CalledProcessError: Command ‘[‘timeout’, ‘180’, ‘/usr/sbin/rabbitmqctl’, ‘wait’, ‘/var/lib/rabbitmq/mnesia/rabbit@juju-cbd760-octavia-10.pid’]’ returned non-zero exit status 70.<br>systemctl restart rabbitmq-server<br>rabbitmqctl status |grep pid   #write pid to /var/lib/rabbitmq/mnesia/rabbit@juju-cbd760-octavia-10.pid<br>rabbitmqctl wait /var/lib/rabbitmq/mnesia/rabbit@juju-cbd760-octavia-10.pid<br>juju resolved rabbitmq-server/1 –no-retry<br>[可选]确保juju status没有rabbitmq untis相关错误之后，触发hooks<br>juju run –application ha hooks/config-changed<br>juju run –application rabbitmq-server hooks/ha-relation-joined<br>juju show-status-log neutron-openvswitch/2</p>
<p>3, 没有ceph的情况下，代码显示必须有ha-vip-only=true<br>juju deploy -n 3 rabbitmq-server<br>juju deploy hacluster ha<br>juju add-relation rabbitmq-server ha<br>juju config rabbitmq-server vip=10.5.100.20<br>juju config rabbitmq-server vip_iface=ens3<br>juju config ha corosync_bindiface=ens3<br>juju config rabbitmq-server ha-vip-only=true<br>juju config ha cluster_count=3<br>juju config rabbitmq-server min-cluster-size=3</p>
<p>4, 找到leader，假如leader是rabbitmq-server/1，并在其上找到cluster_name<br>juju run –application rabbitmq-server “is-leader”   #assue rabbitmq-server/1 is the leader<br>root@juju-cbd760-octavia-10:~# rabbitmqctl cluster_status |grep cluster<br> {cluster_name,<a href="&#109;&#x61;&#x69;&#108;&#116;&#111;&#58;&#x3c;&#34;&#x72;&#97;&#x62;&#98;&#105;&#x74;&#109;&#x71;&#45;&#x73;&#101;&#114;&#118;&#x65;&#114;&#x40;&#x6a;&#x75;&#x6a;&#117;&#x2d;&#99;&#x62;&#x64;&#x37;&#54;&#x30;&#45;&#111;&#x63;&#116;&#x61;&#118;&#x69;&#97;&#x2d;&#x31;&#x30;&#34;">&#x3c;&#34;&#x72;&#97;&#x62;&#98;&#105;&#x74;&#109;&#x71;&#45;&#x73;&#101;&#114;&#118;&#x65;&#114;&#x40;&#x6a;&#x75;&#x6a;&#117;&#x2d;&#99;&#x62;&#x64;&#x37;&#54;&#x30;&#45;&#111;&#x63;&#116;&#x61;&#118;&#x69;&#97;&#x2d;&#x31;&#x30;&#34;</a>&gt;},</p>
<p>5, rabbitmq-server/1, 把RABBITMQ_NODENAME由localhost改成juju-cbd760-octavia-10<br>root@juju-cbd760-octavia-10:~# grep -r ‘RABBITMQ_NODENAME’ /etc/rabbitmq/rabbitmq-env.conf<br>RABBITMQ_NODENAME=rabbitmq-server@juju-cbd760-octavia-10<br>root@juju-cbd760-octavia-10:~# cat /var/lib/rabbitmq/.erlang.cookie<br>CZIFVOYCELFFGUFWJBZY<br>systemctl restart rabbitmq-server<br>rabbitmqctl cluster_status</p>
<p>但代码中有这一句会在ha-vip-only=false时将RABBITMQ_NODENAME设为localhost, 所以ha-vip-only应为ha-vip-only，<br>同时，这里也说对于rabbitmq的hacluster方式的ha已经废弃了<br><a href="https://github.com/openstack/charm-rabbitmq-server/blob/master/hooks/rabbitmq_context.py#L250" target="_blank" rel="external">https://github.com/openstack/charm-rabbitmq-server/blob/master/hooks/rabbitmq_context.py#L250</a></p>
<pre><code># TODO: this is legacy HA and should be removed since it is now
# deprecated.
if relation_ids(&apos;ha&apos;):
    if not config(&apos;ha-vip-only&apos;):
        # TODO: do we need to remove this setting if it already exists
        # and the above is false?
        context[&apos;settings&apos;][&apos;RABBITMQ_NODENAME&apos;] = \
            &apos;{}@localhost&apos;.format(service_name())
</code></pre><p>6, 登录到rabbitmq-server/0,　修改hostname并加入集群<br>root@juju-cbd760-octavia-9:~# grep -r ‘RABBITMQ_NODENAME’ /etc/rabbitmq/rabbitmq-env.conf<br>RABBITMQ_NODENAME=rabbitmq-server@juju-cbd760-octavia-9<br>systemctl restart rabbitmq-server<br>rabbitmqctl cluster_status<br>rabbitmqctl stop_app<br>rabbitmqctl reset<br>rabbitmqctl join_cluster rabbitmq-server@juju-cbd760-octavia-10<br>rabbitmqctl cluster_status</p>
<p>7, 登录到rabbitmq-server/2重复上一步，只是hostname不同, 这里为juju-cbd760-octavia-11</p>
<p>8, 此时．<br>root@juju-cbd760-octavia-10:~# rabbitmqctl cluster_status<br>Cluster status of node ‘rabbitmq-server@juju-cbd760-octavia-10’<br>[{nodes,[{disc,[‘rabbitmq-server@juju-cbd760-octavia-10’,<br>                ‘rabbitmq-server@juju-cbd760-octavia-11’,<br>                ‘rabbitmq-server@juju-cbd760-octavia-9’]}]},<br> {running_nodes,[‘rabbitmq-server@juju-cbd760-octavia-10’]},<br> {cluster_name,<a href="&#109;&#97;&#x69;&#x6c;&#116;&#x6f;&#58;&#60;&#34;&#x72;&#97;&#98;&#98;&#x69;&#116;&#109;&#113;&#45;&#x73;&#x65;&#x72;&#x76;&#101;&#x72;&#64;&#x6a;&#117;&#106;&#117;&#x2d;&#x63;&#98;&#x64;&#x37;&#x36;&#x30;&#45;&#x6f;&#x63;&#x74;&#97;&#x76;&#x69;&#x61;&#45;&#x31;&#x30;&#x22;">&#60;&#34;&#x72;&#97;&#98;&#98;&#x69;&#116;&#109;&#113;&#45;&#x73;&#x65;&#x72;&#x76;&#101;&#x72;&#64;&#x6a;&#117;&#106;&#117;&#x2d;&#x63;&#98;&#x64;&#x37;&#x36;&#x30;&#45;&#x6f;&#x63;&#x74;&#97;&#x76;&#x69;&#x61;&#45;&#x31;&#x30;&#x22;</a>&gt;},<br> {partitions,[]},<br> {alarms,[{‘rabbitmq-server@juju-cbd760-octavia-10’,[]}]}]</p>
<p>9, 但上面修复只是systemd管理的native集群.<br>   如何被pacemaker管理呢？ 在rabbmitmq-server/1上(juju-cbd760-octavia-10)停掉systemd,启动corosync与packemaker，但前提是hacluster charm已经成功为corosync配置了res_rabbitmq_vip resources (当ha-vip-only=true时只有这一个）<br>   此时看到crm status没有res_rabbitmq_vip这个resource，</p>
<p>#juju run –application rabbitmq-server hooks/ha-relation-joined<br>juju run –application ha hooks/config-changed<br>juju run –unit ha/0 “sudo corosync-quorumtool -s”<br>juju run –unit ha/0 “sudo crm status”<br>juju run –unit ha/0 “sudo crm resource restart res_rabbitmq_vip”<br>juju run –unit ha/0 “sudo crm resource clean res_rabbitmq_vip”</p>
<p>#<a href="https://github.com/ClusterLabs/resource-agents/blob/master/heartbeat/rabbitmq-cluster" target="_blank" rel="external">https://github.com/ClusterLabs/resource-agents/blob/master/heartbeat/rabbitmq-cluster</a><br>ls /usr/lib/ocf/resource.d/rabbitmq/rabbitmq-server-ha<br>　　　若运行上面hook无法加入res_rabbitmq_vip的话，检查代码是应该设置ha-vip-only=true，然后使用下面方法添加:<br>juju remove-relation rabbitmq-server ha<br>juju add-relation rabbitmq-server ha<br>      然后可以看到：<br>root@juju-cbd760-octavia-11:~# crm status |grep vip<br> res_rabbitmq_vip       (ocf::heartbeat:IPaddr2):       Started juju-cbd760-octavia-10<br>     但是仍然看到这种错误：<br>ubuntu@zhhuabj-bastion:~$ juju status |grep waiting<br>rabbitmq-server             3.6.10   waiting      3  rabbitmq-server             jujucharms  358  ubuntu<br>rabbitmq-server/0                waiting   idle   9        10.5.0.35       5672/tcp                    Unit has peers, but RabbitMQ not clustered<br>rabbitmq-server/1*               waiting   idle   10       10.5.0.32       5672/tcp                    Unit has peers, but RabbitMQ not clustered<br>rabbitmq-server/2                waiting   idle   11       10.5.0.28       5672/tcp                    Unit has peers, but RabbitMQ not clustered</p>
<p>10, 继续调试, ha relation的数目不对，所以停在’ Unit has peers, but RabbitMQ not clustered’<br>ubuntu@zhhuabj-bastion:~$ juju run –unit rabbitmq-server/0 “relation-ids ha”<br>ha:41<br>ubuntu@zhhuabj-bastion:~$ juju run –unit rabbitmq-server/0 “relation-list -r ha:41”<br>ha/3   #because hacluster was named to ha so name is ha/3 now<br>ubuntu@zhhuabj-bastion:~$ juju run –unit rabbitmq-server/0 “relation-get -r ha:41 - ha/3”<br>clustered: “yes”<br>egress-subnets: 10.5.0.35/32<br>ingress-address: 10.5.0.35<br>private-address: 10.5.0.35</p>
<p>它在找rabbitmqctl cluster_status中为running_nodes的个数<br> 828 @cached<br> 829 def clustered():<br> 830     ‘’’ Determine whether local rabbitmq-server is clustered ‘’’<br> 831     # NOTE: A rabbitmq node can only join a cluster once.<br> 832     # Simply checking for more than one running node tells us<br> 833     # if this unit is in a cluster.<br> 834     if len(running_nodes()) &gt; 1:<br> 835         return True<br> 836     else:<br> 837         return False</p>
<p> 787 @cached<br> 788 def running_nodes():<br> 789     ‘’’ Determine the current set of running nodes in the RabbitMQ cluster ‘’’<br> 790     return nodes(get_running=True)</p>
<p> 770 @cached<br> 771 def nodes(get_running=False):<br> 772     ‘’’ Get list of nodes registered in the RabbitMQ cluster ‘’’<br> 773     out = rabbitmqctl_normalized_output(‘cluster_status’)<br> 774     cluster_status = {}<br> 775     for m in re.finditer(“{([^,]+),(?![{)[([^]]*)”, out):<br> 776         state = m.group(1)<br> 777         items = m.group(2).split(‘,’)<br> 778         items = [x.replace(“‘“, ‘’).strip() for x in items]<br> 779         cluster_status.update({state: items})<br> 780<br> 781     if get_running:<br> 782         return cluster_status.get(‘running_nodes’, [])<br> 783<br> 784     return cluster_status.get(‘disc’, []) + cluster_status.get(‘ram’, [])<br>Fix a rabbitmq issue<br><a href="https://zhhuabj.blog.csdn.net/article/details/105847301" target="_blank" rel="external">https://zhhuabj.blog.csdn.net/article/details/105847301</a></p>
<p>Reference<br>[1] <a href="https://gist.github.com/niedbalski/69a72103adad4f0f9609a0857c9810a4" target="_blank" rel="external">https://gist.github.com/niedbalski/69a72103adad4f0f9609a0857c9810a4</a></p>
<p>[2] <a href="https://pastebin.ubuntu.com/p/sJ94RmmS5x/" target="_blank" rel="external">https://pastebin.ubuntu.com/p/sJ94RmmS5x/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/12/Fix-a-rabbitmq-issue/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/12/Fix-a-rabbitmq-issue/" itemprop="url">Fix a rabbitmq issue</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-12-12T12:00:58+08:00">
                2020-12-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>问题</strong><br>rabbitmq cluster的3个节点挂了. 3个节点是lxd容器:<br>11-lxd-8<br>69-lxd-2<br>9-lxd-9<br><strong>初步sosreport分析</strong><br>11-lxd-8上看到system_limit错误<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">=ERROR REPORT==== 17-Apr-2020::13:19:44 ===</span><br><span class="line">Too many processes</span><br><span class="line">=ERROR REPORT==== 17-Apr-2020::13:19:44 ===</span><br><span class="line">Ranch listener &#123;acceptor,&#123;0,0,0,0,0,0,0,0&#125;,5672&#125; connection process start failure; rabbit_connection_sup:start_link/4 crashed with reason: error:system_limit</span><br></pre></td></tr></table></figure></p>
<p>但是已经设置了LimitNOFILE<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ grep -r &apos;Limit&apos; lib/systemd/system/rabbitmq-server.service</span><br><span class="line">LimitNOFILE=65536</span><br></pre></td></tr></table></figure></p>
<p>不过LimitNOFILE似乎是管file_descriptors的, 而不是erlang processes的.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;file_descriptors,     [&#123;total_limit,65436&#125;,      &#123;total_used,6415&#125;,      &#123;sockets_limit,58890&#125;,      &#123;sockets_used,6413&#125;]&#125;,</span><br><span class="line">16:28:22  &#123;processes,[&#123;limit,1048576&#125;,&#123;used,118304&#125;]&#125;,</span><br></pre></td></tr></table></figure></p>
<p>上面的输出显示似乎用了118304个erlang processes还未超出limit. 不过, 这可能是重启机器之后抓的sosreport. 并且limit已经很大, 单纯增加limit似乎也并不是正道. 所以继续寻找线索.</p>
<p>在69-lxd-2里也看到了’Error while waiting for Mnesia tables’这种错误, 似乎是Mnesia数据库未同步.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">var/log/rabbitmq/rabbit@juju-3182a3-69-lxd-2.log.1</span><br><span class="line">=INFO REPORT==== 17-Apr-2020::13:27:26 ===</span><br><span class="line">Waiting for Mnesia tables for 30000 ms, 9 retries left</span><br><span class="line">=WARNING REPORT==== 17-Apr-2020::13:27:56 ===</span><br><span class="line">Error while waiting for Mnesia tables: &#123;timeout_waiting_for_tables,</span><br><span class="line">[rabbit_user,rabbit_user_permission,</span><br><span class="line">rabbit_vhost,rabbit_durable_route,</span><br><span class="line">rabbit_durable_exchange,</span><br><span class="line">rabbit_runtime_parameters,</span><br><span class="line">rabbit_durable_queue]&#125;</span><br></pre></td></tr></table></figure></p>
<p>3个units上用’netstat -s’看到大量的reset tcp<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sos_commands/networking/netstat_-s |head -n2</span><br><span class="line">7154216 connection resets received</span><br><span class="line">34025359 resets sent</span><br></pre></td></tr></table></figure></p>
<h2 id="深入分析"><a href="#深入分析" class="headerlink" title="深入分析"></a>深入分析</h2><p>9-lxd-9上 通过下列命令看到各个openstack组件上都有到9-lxd-9的rabbitmq tcp连接, 一个组件就有约四五百个连接, 是不是多了一点? 其他两个units没这么多.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat sos_commands/networking/netstat_-W_-neopa| awk &apos;/:5672/ &#123; print $5 &#125;&apos; | awk -F: &apos;&#123; a[$1]++ &#125;; END &#123; for (i in a) print i, a[i] &#125;&apos; |sort -n -k 2 -r |more |head -n 80</span><br></pre></td></tr></table></figure></p>
<p>但一个组件就有四五百个连接, 每个组件都这么多, 不可能每个组件都有问题吧. 除了大量tcp连接可以造成容器cpu升高, host机器的cpu升高也可以造成容器cpu升高的吧. 所以接着检查host机器的cpu占用率:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ cat sos_commands/process/ps_auxwww |head -n 1</span><br><span class="line">USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</span><br><span class="line">$ cat sos_commands/process/ps_auxwww |head -n1 &amp;&amp; cat sos_commands/process/ps_auxwww |sort -n -k3 -r | head -n 3</span><br><span class="line">USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND</span><br><span class="line">nova 1936435 130 0.0 347704 126140 ? Rs 11:40 0:01 /usr/bin/python2 /usr/bin/nova-api-metadata --config-file=/etc/nova/nova.conf --log-file=/var/log/nova/nova-api-metadata.log</span><br><span class="line">libvirt+ 1809025 100 0.0 51373088 236744 ? Sl Mar09 56456:32 qemu-system-x86_64 -enable-kvm -name guest=instance-000099e0,debug-threads=on -S -object secret,id=masterKey0,format=raw,file=/var/lib/libvirt/qemu/domain-26-instance-000099e0/master-key.aes -machine pc-i440fx-</span><br></pre></td></tr></table></figure></p>
<p>ok, 虚机占用cpu就是100%. 看样子是控制服务(rabbitmq)与计算服务(nova-compute)安装到同一台物理机了, 虚机占用的cpu或者内存大页什么的直接导致rabbitmq cluster挂掉.<br>注:<br>ps命令中第三列相加的cpu大于100%也不一定就意味着cpu一定高, 因为可以这些进程全跑在一个cpu核的, 它可能计算的是一个核的, 要想准确还得使用mpstat来查看</p>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>调整架构是不可能了, 可以暂时使用isolcpu隔离一些cpu单独通过taskset给rabbitmq使用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mpstat -P ALL 能看到所有cpu核的负载情况</span><br><span class="line">cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniq</span><br><span class="line">grub中添加isolcpus=1,3来隔离1和3的cpu待用, 然后运行update-grub后重启, 验证:</span><br><span class="line">1, cat /proc/cmdline |grep isolcpu</span><br><span class="line">2, ps -To &apos;pid,lwp,psr,cmd&apos; -p 310597</span><br><span class="line">3, ps -eo pid,cmd,psr |awk &apos;&#123;if($3=3) print $0&#125;&apos;</span><br><span class="line">taskset -p0x8 &lt;pid&gt;绑定&lt;pid&gt;到cpu3</span><br><span class="line">taskset -c 1 /etc/init.d/mysql start</span><br><span class="line">systemd manages the affinity for you. See &quot;man systemd.exec&quot; and CPUAffinity= option.</span><br></pre></td></tr></table></figure></p>
<h2 id="20201212更新-queue-master-locator-min-master"><a href="#20201212更新-queue-master-locator-min-master" class="headerlink" title="20201212更新 - queue_master_locator=min-master"></a>20201212更新 - queue_master_locator=min-master</h2><p>我们可能需要配置queue_master_locator=min-master - <a href="https://bugs.launchpad.net/charm-rabbitmq-server/+bug/1890759" target="_blank" rel="external">https://bugs.launchpad.net/charm-rabbitmq-server/+bug/1890759</a>, 进而可能造成这两个bug (控制平面居然影响了数据面） - <a href="https://bugs.launchpad.net/neutron/+bug/1871850" target="_blank" rel="external">https://bugs.launchpad.net/neutron/+bug/1871850</a><br><a href="https://bugs.launchpad.net/neutron/+bug/1869808" target="_blank" rel="external">https://bugs.launchpad.net/neutron/+bug/1869808</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># Queue masters are mostly on 11-lxd-8</span><br><span class="line">&gt; cat rabbitmqctl_report | awk -F&apos;\t&apos; &apos;/^Queues on openstack/ &#123;a=1&#125;; a &amp;&amp; /rabbit/ &#123;split($1,s,&quot;.&quot;); b[s[1]]++&#125;; NF==0 &#123;a=0&#125;; END &#123;for(i in b) &#123;print i, b[i]&#125;&#125;&apos;</span><br><span class="line">&lt;rabbit@juju-0aa49a-9-lxd-9 15</span><br><span class="line">&lt;rabbit@juju-0aa49a-11-lxd-8 15426</span><br><span class="line">&lt;rabbit@juju-0aa49a-10-lxd-8 309</span><br><span class="line"></span><br><span class="line"># Most connections are made to 11-lxd-8</span><br><span class="line">&gt; cat rabbitmqctl_report| awk -F&apos;\t&apos; &apos;/^Connections/ &#123;a=1&#125;; a &amp;&amp; /rabbit/ &#123;split($1,s,&quot;.&quot;); b[s[1]]++&#125;; NF==0 &#123;a=0&#125;; END &#123;for(i in b) &#123;print i, b[i]&#125;&#125;&apos;</span><br><span class="line">&lt;rabbit@juju-0aa49a-9-lxd-9 8763</span><br><span class="line">&lt;rabbit@juju-0aa49a-11-lxd-8 14843</span><br><span class="line">&lt;rabbit@juju-0aa49a-10-lxd-8 6413</span><br><span class="line"></span><br><span class="line"># 11-lxd-8 uses the most memory</span><br><span class="line">&gt; cat rabbitmqctl_report| awk &apos;/^Status/ &#123;a=1; print&#125;; a &amp;&amp; /total,/; !NF &#123;a=0&#125;&apos;</span><br><span class="line">Status of node &apos;rabbit@juju-0aa49a-10-lxd-8&apos;</span><br><span class="line">     [&#123;total,3536412448&#125;,</span><br><span class="line">Status of node &apos;rabbit@juju-0aa49a-11-lxd-8&apos;</span><br><span class="line">     [&#123;total,7030459176&#125;,</span><br><span class="line">Status of node &apos;rabbit@juju-0aa49a-9-lxd-9&apos;</span><br><span class="line">     [&#123;total,4256377400&#125;,</span><br></pre></td></tr></table></figure></p>
<p>crontab检查rabbitmq memory使用率:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"># process_memory_checker.sh</span><br><span class="line">MAILTO=root</span><br><span class="line">process=&quot;bin/beam.smp&quot;</span><br><span class="line">mem_percentage=`ps -o %mem,command ax | grep &quot;$&#123;process&#125;&quot; | grep -v grep | awk &apos;&#123;print $1&#125;&apos;`</span><br><span class="line">threshold_percentage=25</span><br><span class="line">if (( $(echo &quot;$&#123;mem_percentage&#125; &gt; $&#123;threshold_percentage&#125;&quot;|bc -l) ));</span><br><span class="line">then</span><br><span class="line">	echo &quot;Process $&#123;process&#125; on `hostname -f` is using $&#123;mem_percentage&#125;% of total memory (threshold is $&#123;threshold_percentage&#125;%).&quot; | \</span><br><span class="line">	mail -s &quot;Memory usage warning for process $&#123;process&#125; on $(hostname -s)&quot; $&#123;MAILTO&#125; 2&gt; /dev/null;</span><br><span class="line">	exit 1;</span><br><span class="line">fi</span><br><span class="line">echo &quot;No memory issues found for process $&#123;process&#125;&quot;</span><br><span class="line">exit 0</span><br></pre></td></tr></table></figure></p>
<p>另外，客户发现memory用到超过vm_memory_high_watermark_paging_ratio (<a href="https://www.rabbitmq.com/memory.html#paging" target="_blank" rel="external">https://www.rabbitmq.com/memory.html#paging</a> )时似乎memory也没有被page到硬盘造成memory一直在增大，这个网页（<a href="https://stackoverflow.com/questions/21666537/rabbitmq-memory-control-queue-is-full-and-is-not-paging-connection-hangs）说对“durable=true”queue才生效。" target="_blank" rel="external">https://stackoverflow.com/questions/21666537/rabbitmq-memory-control-queue-is-full-and-is-not-paging-connection-hangs）说对“durable=true”queue才生效。</a><br>“rabbitmqctl list_queues name  durable -p openstack”看到的durable都是false, 难道是在设置各组件的olso设置为(charm似乎不支持）amqp_durable_queues=True吗?<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[oslo_messaging_rabbit]</span><br><span class="line">amqp_durable_queues = True</span><br></pre></td></tr></table></figure></p>
<p>但这个patch说上面配置被废弃了 - <a href="https://bugs.launchpad.net/oslo.messaging/+bug/1433956" target="_blank" rel="external">https://bugs.launchpad.net/oslo.messaging/+bug/1433956</a>, 但master code中有这个配置，还没细查。另外，这个网页（<a href="https://elkano.org/blog/high-availability-rabbitmq-cluster-openstack/）说用这个配置：" target="_blank" rel="external">https://elkano.org/blog/high-availability-rabbitmq-cluster-openstack/）说用这个配置：</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[oslo_messaging_rabbit]</span><br><span class="line">rabbit_hosts=node1:5672,node2:5672,node3:5672</span><br><span class="line">rabbit_retry_interval=1</span><br><span class="line">rabbit_retry_backoff=2</span><br><span class="line">rabbit_max_retries=0</span><br><span class="line">rabbit_ha_queues=true</span><br><span class="line">rabbit_userid = openstack</span><br><span class="line">rabbit_password = openstack_pass</span><br><span class="line">amqp_auto_delete = true</span><br><span class="line">amqp_durable_queues=True</span><br></pre></td></tr></table></figure></p>
<p>charm ./charmhelpers/contrib/openstack/templates/section-rabbitmq-oslo 中有下面代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> 8 &#123;% if rabbitmq_ha_queues -%&#125;</span><br><span class="line"> 9 rabbit_ha_queues = True</span><br><span class="line">10 rabbit_durable_queues = False</span><br><span class="line">11 &#123;% endif -%&#125;</span><br></pre></td></tr></table></figure></p>
<p>其他debug手段:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo rabbitmqctl -p openstack list_queues|grep -vw 0</span><br><span class="line"></span><br><span class="line"># returns a list with info about memory dynamically allocated by the Erlang emulator</span><br><span class="line">rabbitmqctl eval &apos;erlang:memory().&apos;</span><br><span class="line">rabbitmq-diagnostics memory_breakdown  #for new version</span><br></pre></td></tr></table></figure></p>
<p>内存使用量一直突然增长可能是这个错误造成的， 详见：<a href="https://bugs.launchpad.net/oslo.messaging/+bug/1789177" target="_blank" rel="external">https://bugs.launchpad.net/oslo.messaging/+bug/1789177</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ grep -r &apos;no exchange&apos; var/log/rabbitmq/rabbit@juju-0aa49a-10-lxd-8.log |wc -l</span><br><span class="line">3511</span><br><span class="line">$ grep -r &apos;no exchange&apos; var/log/rabbitmq/rabbit@juju-0aa49a-10-lxd-8.log |head -n1</span><br><span class="line">operation basic.publish caused a channel exception not_found: no exchange &apos;reply_bee2ebfe01854e0595ddaa7462dc4054&apos; in vhost &apos;openstack&apos;</span><br></pre></td></tr></table></figure></p>
<p>这是关于rabbitm HA一个非常好的贴子（<a href="https://blog.csdn.net/zyz511919766/article/details/41896823" target="_blank" rel="external">https://blog.csdn.net/zyz511919766/article/details/41896823</a>), 也就是说，在rabbitmq non-ha中，exchange与 bindings也是在所在节点上的，而queue只处于一个节点。在rabbitmq ha环境中，只是将queue也根据policy放置在所有或某些节点上。当consumer从rabbitmq的某个节点消费queue时，consumer死掉了，consumer创建的reply-xxx queues也会被删掉但此时可能consumer没有收到basic cancel signal（eg：当reply-xxx被删后刚好consumer在重启）, 这样，consumer没ack消息这样server还会继续给consumer发, 但此时就会报no exchange (exchange与reply-xxx queue都是consumer创建的，reply-xxx queue为0时exchange也会被删除）。<br>但是，有一点没闹明白，consumer重启之后不是会调用 reconnect继续重建exchange与reploy-xxx queue么？不过在重建之前，server会继续发， 这段期间就会有no exchange错误吧，这样会耗费大量的cpu，这样或许也是cpu升级的原因 （Lots of exchanges create problems during failover under high load）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ grep -r &apos;^reply_&apos; sos_commands/rabbitmq/rabbitmqctl_report |grep exchange |head -n1</span><br><span class="line">reply_0003bee270e54c8cb9b78ba3b51ba4e2  exchange        reply_0003bee270e54c8cb9b78ba3b51ba4e2  queue   reply_0003bee270e54c8cb9b78ba3b51ba4e2  []      openstack</span><br><span class="line">$ grep -r &apos;^reply_&apos; sos_commands/rabbitmq/rabbitmqctl_report |grep exchange |wc -l</span><br><span class="line">24991</span><br></pre></td></tr></table></figure></p>
<p>24991肯定会造成memory一直升高，所以一个最好办法就是设置x-cancel-on-ha-failover让queue没了时也取消consumer。另一个办法是也可以哪出哪些consumer在作怪.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cat sos_commands/networking/netstat_-W_-neopa| awk &apos;/:5672/ &#123; print $5 &#125;&apos; | awk -F: &apos;&#123; a[$1]++ &#125;; END &#123; for (i in a) print i, a[i] &#125;&apos; |sort -n -k 2 -r |more |head -n 3</span><br><span class="line">10.160.0.106 96</span><br><span class="line">10.160.0.208 86</span><br><span class="line">10.160.0.75 83</span><br></pre></td></tr></table></figure></p>
<p>也可以用下面的policy缓解:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#rabbitmqctl set_policy min-masters-queue -p openstack &apos;.*&apos; &apos;&#123;&quot;queue-master-locator&quot;:&quot;min-masters&quot;&#125;&apos; --apply-to queues --priority 10</span><br><span class="line">rabbitmqctl set_policy HA -p openstack &apos;^(?!amq\\.).*&apos; &apos;&#123;&quot;queue-master-locator&quot;:&quot;min-masters&quot;, &quot;ha-mode&quot;:&quot;all&quot;, &quot;ha-sync-mode&quot;:&quot;automatic&quot;&#125;&apos;</span><br></pre></td></tr></table></figure></p>
<p>但是exchange只是记录的一个名称，是不占用内存的，queues可能会占用内存。但30481个queue也就1.2G左右，也不大啊。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat rabbitmq_report.txt |gawk -F&apos;\t&apos; &apos;/^Queues on openstack/ &#123;a=1;next&#125;; a &amp;&amp; NF &#123;mem+=$17; n+=1&#125;; !NF &#123;a=0&#125; END &#123;print n, mem&#125;&apos;</span><br><span class="line">30481 1248040000</span><br></pre></td></tr></table></figure></p>
<p>相较，rabbitmqctl status显示queue_procs与queue_slave_procs占用的内存更大, queue_procs占了约199G. 队列占用的内存指的是队列进程消耗的，并不包含消息体（在二进制中）。当内存不足时，这部分的内存将交换到磁盘上。</p>
<ul>
<li>queue_procs：主队列，索引和消息保存在内存中。排队的消息数量越多，通常会将此内容归因于此部分。但是，这在很大程度上取决于队列属性以及消息是否作为瞬态发布</li>
<li>queue_slave_procs：队列镜像，索引和消息保存在内存中。减少镜像（副本）的数量或不使用固有的瞬态数据镜像队列可以减少镜像使用的RAM量。排队的消息数量越多，通常会将此内容归因于此部分。但是，这在很大程度上取决于队列属性以及消息是否作为瞬态发布。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;queue_procs,199051279736&#125;,</span><br><span class="line">&#123;queue_slave_procs,1243762680&#125;,</span><br></pre></td></tr></table></figure>
<p>这个网页（<a href="https://www.rabbitmq.com/monitoring.html#diagnostics-observer）提到&#39;rabbitmq-diagnostics" target="_blank" rel="external">https://www.rabbitmq.com/monitoring.html#diagnostics-observer）提到&#39;rabbitmq-diagnostics</a> observer’命令可以像top一样查看erlang虚拟机内进程的内存使用量，但rabbitmq-server 3.8版本才支持啊。那perf是否可以查看erlang虚机机内进程的call trace呢？但这篇文章说了怎么查erlang下的进程所用内存：<br>RabbitMQ及Erlang内存使用分析 - <a href="https://blog.csdn.net/jaredcoding/article/details/78115235" target="_blank" rel="external">https://blog.csdn.net/jaredcoding/article/details/78115235</a><br>RabbitMQ运维经验分享 - <a href="https://my.oschina.net/hackandhike/blog/801052" target="_blank" rel="external">https://my.oschina.net/hackandhike/blog/801052</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl status #single node status</span><br><span class="line">rabbitmqctl report #cluster status</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/27/Shell编程注意点/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/27/Shell编程注意点/" itemprop="url">Shell编程注意点</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-27T09:53:02+08:00">
                2020-11-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华 写于：发表于：2011-04-06<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )</p>
<p>1在shell脚本中调用另一个脚本的三种不同方式(fork,exec, source)<br>       我们先谈谈在shell脚本中调用另一个脚本的三种不同方式的区别（fork,exec, source )</p>
<p>fork ( /directory/script.sh ), fork是最普通的,就是直接在脚本里面用/directory/script.sh来调用script.sh这个脚本.运行的时候开一个sub-shell执行调用的脚本，sub-shell执行的时候,parent-shell还在。sub-shell执行完毕后返回parent-shell.sub-shell从parent-shell继承环境变量.但是sub-shell中的环境变量不会带回parent-shell</p>
<p>exec(exec /directory/script.sh) exec与fork不同，不需要新开一个sub-shell来执行被调用的脚本. 被调用的脚本与父脚本在同一个shell内执行。但是使用exec调用一个新脚本以后,父脚本中exec行之后的内容就不会再执行了。这是exec和source的区别</p>
<p>source(source/directory/script.sh)与fork的区别是不新开一个sub-shell来执行被调用的脚本，而是在同一个shell中执行.所以被调用的脚本中声明的变量和环境变量,都可以在主脚本中得到和使用.</p>
<p>可以通过下面这两个脚本来体会三种调用方式的不同:</p>
<p>1.sh </p>
<p>#!/bin/bash<br>A=B<br>echo “PID for 1.sh before exec/source/fork:$$”<br>exportA<br>echo “1.sh: /$A is $A”<br>case $1 in<br>       exec)<br>               echo “using exec…”<br>               exec ./2.sh ;;<br>       source)<br>               echo “using source…”<br>               ../2.sh ;;<br>        *)<br>               echo”using fork by default…”<br>               ./2.sh ;;<br>esac<br>echo”PID for 1.sh after exec/source/fork:$$”<br>echo “1.sh:/$A is $A”<br>2.sh </p>
<p>#!/bin/bash<br>echo”PID for 2.sh: $$”<br>echo “2.sh get /$A=$A from1.sh”<br>A=C<br>export A<br>echo “2.sh: /$A is $A”</p>
<p>执行情况：<br>$./1.sh<br>PID for 1.sh beforeexec/source/fork:5845364<br>1.sh: $A is B<br>using fork bydefault…<br>PID for 2.sh: 5242940<br>2.sh get $A=B from 1.sh<br>2.sh:$A is C<br>PID for 1.sh after exec/source/fork:5845364<br>1.sh: $A isB<br>$ ./1.sh exec<br>PID for 1.sh beforeexec/source/fork:5562668<br>1.sh: $A is B<br>using exec…<br>PID for2.sh: 5562668<br>2.sh get $A=B from 1.sh<br>2.sh: $A is C<br>$./1.sh source<br>PID for 1.sh beforeexec/source/fork:5156894<br>1.sh: $A is B<br>using source…<br>PIDfor 2.sh: 5156894<br>2.sh get $A=B from 1.sh<br>2.sh: $A is C<br>PIDfor 1.sh after exec/source/fork:5156894<br>1.sh: $A is C<br>$<br>2函数调用<br>先看一个例子，执行mysql的函数mysqlExec,如下：</p>
<p>source“mysql.conf”</p>
<p>mysqlExec(){<br>sql=$1<br>sqlOp=<code>echo${sql:0:6}| tr A-Z a-z</code><br>if[ “$sqlOp” != “select” ]; then<br>  sql=$sql”;select row_count();”<br>fi</p>
<p>#use different mysql command depends on the password</p>
<p>if[ -z $MYSQL_PASSWORD ]<br>then<br>  $mysql $MYSQL_DATABASE -h$MYSQL_HOSTNAME -u$MYSQL_USERNAME -se “${sql};”<br>else<br>  $mysql $MYSQL_DATABASE -h$MYSQL_HOSTNAME -u$MYSQL_USERNAME -p$MYSQL_PASSWORD-se “${sql};”<br>fi<br>status=$?<br>if[ $status -eq 0 ]; then<br>  if[ “$sqlOp” != “select” ]; then<br>    log “OK $sql”<br>  fi<br>else<br>  log “Occur DB Error, can retry in 3 seconds later -&gt; $sql”<br>  sleep3<br>  echo “DB_ERROR”<br>fi<br>return $status<br>exit<br>}</p>
<p>函数调用要注意两点：<br>1）函数中可以用echo,如上面的echo “DB_ERROR”，在调用时要获取echo的值，应该这样:<br>campaign=<code>mysqlExec&quot;$sql&quot;</code><br>if[ “x$campaign” == “x” -o “$campaign” ==”DB_ERROR” ]; then<br>continue<br>fi<br>2)函数中也可以有返回值，如上面的return $status，在调用时应该通过$?获得，如：<br>if[ $? -eq 0 ]; then<br>echo“zhanghua”<br>fi</p>
<p>3) 如果想从被调用的函数处返回一个值，可以这样</p>
<p>   调用gen_conf函数，传一个引用（注意不是变量）config_file进去， gen_conf $host config_file</p>
<p>   在gen_conf函数中通过__resultvar变量返回值:</p>
<p>   gen_config{</p>
<pre><code> local  __resultvar=$2
eval $__resultvar=&quot;&apos;$config&apos;&quot;
</code></pre><p>   }</p>
<p>3shell中的要用局部变量很纠结<br>你会看到shell有一个非常大的缺点，就是它在函数调用时，没有局部变量与全局变量之分，如A脚本调用B脚本中的一个函数，在B脚本内部有一个变量vari（你可能受JAVA影响认为它是局部变量那就大错特错了），如果A脚本中也有这个名为vari的变量，那么在函数返回时，B脚本的那个vari变量会将A脚本的vari变量覆盖，举个例子：<br>updateWithOptimisticLock(){<br>rand=$1<br>campaignId=$2<br>seq=1<br>updateVal=-1<br>status=1<br>while[ true ]; do<br>if[ $seq -gt 3 ]; then<br>log”FATAL ERROR, Update num error, $updatesql”<br>break<br>fi<br>cur=<code>queryCur&quot;$campaignId&quot;</code><br>if[ “$cur” == “DB_ERROR” ]; then<br>continue<br>fi<br>updateVal=$(($cur+$rand))<br>if[ “$rand” == “0” ]; then<br>break<br>fi<br>updatesql=”updatet<em>campaign</em> set num=$updateVal where campaign<em>id=$campaignId andnum=$cur”<br>affectRows=<code>mysqlExec&quot;$updatesql&quot;</code><br>if[ “$affectRows” == “1” ]; then<br>status=0<br>break<br>fi<br>seq=$(($seq+1))<br>done<br>echo$updateVal<br>return$status<br>}<br>调用的伪码如下，这时里面的seq变量会被上述updateWithOptimisticLock函数中的变量seq给覆盖，所以在shell中没有局部变量一说<br>seq=1<br>while[ $seq -le $cycleNum ]; do<br>updateFakeNumWithOptimisticLock$rand $campaignId<br>done<br>4使用xargs来传参数<br>在shell中的管道符|很强大，可以将前一条命令的标准输出作为下一条命令的标准输入，但是如果下一条命令不是标准输入而是需要传参的话，那怎么办呢，用xargs即可，例如下列shell中xargs命令的-i选项告诉xargs用前一条命令的标准输出去替换{}。<br>find. | xargs zgrep “<url>/Search?” | sed’s/.*q=/([-</url></em><em>()~.%+0-9A-Za-z]</em>/).*//1/‘ | sort -nr | uniq -c | sort-nr | head -1000 | xargs -i php -r “echorawurldecode(‘{}’)./“/n/“;” &gt; result.out &amp;</p>
<p>另， 修改/etc/hosts, 能处理127或localhost打头的多个hostname项如(ubuntu.me.com  ubuntu)， 用‘/usr/bin/awk ‘$1 ~ /^127|localhost/ {print $0}’ /etc/hosts’是为了避免IPv6 中的node如ff02::1 ip6-allnodes</p>
<p>/usr/bin/awk ‘$1 ~ /^127|localhost/ {print $0}’ /etc/hosts |awk ‘$1 ~ /^127|localhost/ {print $0}’ | /bin/sed “s/\s<em>(${CURRENT_HOSTNAME})(\s</em>)/\t${NETCFG_HOSTNAME}/g”</p>
<p>当然exec也可以实现上述功能，只是exec都是一次性读入内存容易内存溢出，如：<br>find. -name “<em>.m4” -exec grep –color -H “catalina”{} /;<br>5shell中的sed命令使用的正则是缩水版<br>shell中的sed命令使用的正则引擎和我们java中平常所用正则引擎并不一样，功能比较弱。<br>如上节中的|sed ‘s/.</em>q=/([-_<em>()~.%+0-9A-Za-z]</em>/)，就是因为shell的很多正则不支持，才在使用sed命令时用了那么多枚举。</p>
<p>20191115更新</p>
<p>上面说法不正确, sed可以加-r参数不用枚举, 例:</p>
<p>grep -r ‘get_or_set_cached_cell_and_set_connections’ var/log/nova/ |sed -r ‘s/.+(waited|held) (.+) inner.+/\2/g;t;d’ |sort -nr |head -n 5</p>
<p>nova service-list –bi nova-compute | grep nova-compute | cut -d ‘ ‘ -f 4 | xargs -n 1 -I {} ssh -o StrictHostKeyChecking=no  ubuntu@{} “date; hostname; zgrep MessagingTimeout /var/log/nova/nova-compute.log*; echo -e ‘—————————–\n’”</p>
<p>数组</p>
<p>readarray -t cookies&lt;&lt;&lt;”<code>ls -1 /var/snap/ovs-stat/common/tmp/tmp.662kJWxfEg/juju-4f585d-sf00272961-cisco-7/ovs/bridges/br-int/flowinfo/cookies/| grep -v 6cc04af0837d3b42</code>“<br>for c in ${cookies[@]}; do grep -rl $c /var/snap/ovs-stat/common/tmp/tmp.662kJWxfEg/juju-4f585d-sf00272961-cisco-7/ovs/bridges/br-int/flowinfo/tables; done| sort| uniq -c</p>
<p>svcs=(<br>nova-compute-kvm<br>neutron-openvswitch<br>)<br>for svc in ${svcs[@]}; do<br>juju config $svc worker-multiplier<br>done</p>
<p>排序相加</p>
<p>cat ps.txt | sed ‘s/.<em>](.</em>)/\1/g’ | column -t | body sort -n -k4 -r<br>uid     tgid     total_vm  rss      pgtables_bytes  swapents  oom_score_adj  name<br>100112  832785   5760963   1650592  16789504        76362     0              beam.smp</p>
<p>cat ps.txt | sed ‘s/.<em>](.</em>)/\1/g’ | column -t | body sort -n -k4 -r | awk ‘{sum+=$4;} END{print sum;}’</p>
<p>rabbitmq相序相加举例</p>
<p>$ cat sos<em>commands/networking/netstat</em>-W_-neopa |head -n1<br>Proto Recv-Q Send-Q Local Address           Foreign Address         State       User       Inode      PID/Program name     Timer<br>tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      101        1671174047 178/systemd-resolve  off (0.00/0/0)</p>
<p>$ cat sos<em>commands/networking/netstat</em>-W_-neopa| awk ‘/:5672/ { print $5 }’ | awk -F: ‘{ a[$1]++ }; END { for (i in a) print i, a[i] }’ |sort -n -k 2 -r |more |head -n 10<br>10.164.0.107 69</p>
<p>实例 - 抓取</p>
<p>#!/bin/bash</p>
<h1 id="Usage-nohup-proxychains4-crawl-sh-gt-log-txt-2-gt-1-amp"><a href="#Usage-nohup-proxychains4-crawl-sh-gt-log-txt-2-gt-1-amp" class="headerlink" title="Usage: nohup proxychains4 ./crawl.sh &gt; log.txt 2&gt;1&amp;"></a>Usage: nohup proxychains4 ./crawl.sh &gt; log.txt 2&gt;1&amp;</h1><p>#set -x<br>[[ -f page.txt ]] &amp;&amp; echo ‘skip lynx’ || lynx -dump ftp://ftp.hycom.org/datasets/force/ncep_cfsr/netcdf/ &gt;page.txt<br>grep -r “ftp://“ page.txt |awk ‘{print $2}’ &gt; urls.txt<br>readarray -t exist_files&lt;&lt;&lt;”<code>ls .</code>“<br>for url in $(cat urls.txt)<br>do<br>  skip=”0”<br>  name=$(echo $url |awk -F ‘/‘ ‘{print $NF}’)<br>  readarray -t exist_files&lt;&lt;&lt;”<code>ls .</code>“<br>  for f in ${exist_files[@]}; do<br>    [[ “$name” == “$f” ]] &amp;&amp; skip=”1”; break;<br>  done<br>  if [ “$skip” == “1” ]; then<br>    echo “skipping ${f}”;<br>  else<br>    echo “download ${url}”;<br>    wget -c ${url}<br>  fi<br>done<br>改进版</p>
<p>#!/bin/bash</p>
<h1 id="Usage-nohup-proxychains4-crawl-sh-gt-log-txt-2-gt-1-amp-1"><a href="#Usage-nohup-proxychains4-crawl-sh-gt-log-txt-2-gt-1-amp-1" class="headerlink" title="Usage: nohup proxychains4 ./crawl.sh &gt; log.txt 2&gt;1&amp;"></a>Usage: nohup proxychains4 ./crawl.sh &gt; log.txt 2&gt;1&amp;</h1><p>#set -x<br>[[ -f page.txt ]] &amp;&amp; echo ‘skip creating lynx’ || lynx -dump <a href="http://tds.hycom.org/thredds/catalog/datasets/force/ncep_cfsr/netcdf/catalog.html" target="_blank" rel="external">http://tds.hycom.org/thredds/catalog/datasets/force/ncep_cfsr/netcdf/catalog.html</a> &gt;page.txt<br>[[ -f urls.txt ]] &amp;&amp; echo ‘skip creating urls.txt’ || grep -r “http://“ page.txt |awk ‘{print $2}’ |grep -E ‘.nc$’ &gt; urls.txt<br>sed -i ‘/dswsfc/d’ urls.txt<br>sed -i ‘/dlwsfc/d’ urls.txt<br>readarray -t exist_files&lt;&lt;&lt;”<code>ls . |grep -E &#39;\.nc$&#39;</code>“;  #can’t use multiple commands in [[ ]]<br>[[ -f skip.txt ]] &amp;&amp; echo ‘will use skip.txt’ || printf “%s\n” “${exist_files[@]}” &gt; skip.txt<br>readarray -t skip_files&lt;&lt;&lt;”<code>cat skip.txt</code>“<br>for url in $(cat urls.txt)<br>do<br>  name=$(echo $url |awk -F ‘/‘ ‘{print $NF}’)<br>  realurl=’<a href="http://tds.hycom.org/thredds/fileServer/datasets/force/ncep_cfsr/netcdf/&#39;$name" target="_blank" rel="external">http://tds.hycom.org/thredds/fileServer/datasets/force/ncep_cfsr/netcdf/&#39;$name</a><br>  if <code>echo ${skip_files[@]} |grep -q &quot;$name&quot;</code>; then<br>    echo “skipping ${f}”;<br>  else<br>    echo “download ${realurl}”;</p>
<pre><code>#wget -c --limit-rate=3m ${realurl}
wget -c ${realurl}
#echo ${realurl} &gt;&gt; skip.txt
</code></pre><p>  fi<br>done<br>获取绝对目录</p>
<p>export OS_CACERT=$(dirname “$(realpath -s “${BASH_SOURCE[0]}”)”)/ssl/openstack-ssl/results/cacert.pem<br>从国外下载气象数据</p>
<p>#gdisk /dev/sdd1   #t, 0700</p>
<p>#sudo mkfs.ntfs -f -L win /dev/sdd1</p>
<p>#sudo ntfsfix /dev/sdd1<br>sudo mkfs.ext4 /dev/xvdb<br>sudo parted /dev/xvdb  #print<br>sudo mount /dev/xvdb /mnt/<br>sudo mkdir -p /mnt/ftp<br>sudo chown -R $USER /mnt/<br>sudo apt install curlftpfs -y</p>
<p>#sudo fusermount -zu /mnt/ftp<br>sudo curlftpfs ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/ /mnt/ftp<br>sudo curlftpfs -o rw,allow_other,uid=1000,gid=1000 ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/ /mnt/ftp<br>$ scp -i ~/.aws/zhhuabj.pem ubuntu@13.114.59.98:/mnt/ftp/uvel/rarchv.2012_205_00_3zu.nc4 /tmp/<br>rarchv.2012_205_00_3zu.nc4                                                                                                94%  236MB   7.6MB/s   00:01 ETA</p>
<h1 id="using-direct-io-and-cache-no-to-avoid-using-disk"><a href="#using-direct-io-and-cache-no-to-avoid-using-disk" class="headerlink" title="using direct_io and cache=no to avoid using disk"></a>using direct_io and cache=no to avoid using disk</h1><p>sudo bash -c ‘cat &gt;&gt;/etc/fstab’ &lt;&lt;EOF<br>curlftpfs#@ftp.hycom.org/datasets/global/GLBa0.08_rect/data/ /mnt/ftp fuse defaults,direct_io,cache=no,rw,allow_other,uid=1000,gid=1000,_netdev 0 0<br>EOF<br>sudo mount -a</p>
<p>rsync -avztur -e “ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ~/.aws/lxj.pem” –progress ubuntu@3.18.107.xxx:/mnt/ftp .</p>
<p>注意：　上面加了direct_io,cache=no后总是hang，得去掉．</p>
<p>#debug curlftpfs<br>sudo fusermount -zu /mnt/ftp/2d<br>sudo curlftpfs -f -v -o debug,ftpfs_debug=3,rw,allow_other,uid=1000,gid=1000 ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/2d /mnt/ftp/2d<br>1, use lftp to mirror</p>
<h1 id="lftp-e-“mirror-–delete-–only-newer-–verbose-–parallel-2”-ftp-ftp-hycom-org-datasets-global-GLBa0-08-rect-data"><a href="#lftp-e-“mirror-–delete-–only-newer-–verbose-–parallel-2”-ftp-ftp-hycom-org-datasets-global-GLBa0-08-rect-data" class="headerlink" title="lftp -e “mirror –delete –only-newer –verbose –parallel=2” ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/"></a>lftp -e “mirror –delete –only-newer –verbose –parallel=2” ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/</h1><p>lftp -c “set ftp:list-options -a;<br>open ‘ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/‘;<br>lcd /home/ubuntu/test;<br>cd 2d<br>mirror -c –use-cache –verbose –allow-chown –allow-suid –no-umask –verbose –parallel=2”</p>
<p>2, use wget to mirror</p>
<p>wget -c -m ftp://ftp.hycom.org/datasets/global/GLBa0.08_rect/data/<br>lynx -dump <a href="http://13.59.199.151/new_01hr/" target="_blank" rel="external">http://13.59.199.151/new_01hr/</a> &gt; page.txt<br>grep -r “http://“ page.txt |awk ‘{print $2}’ |grep -E ‘.nc$’ &gt; urls.txt<br>aria2c -x 10 -i urls.txt &gt;/dev/null 2&gt;/dev/null &amp;</p>
<p>并行运行</p>
<p>token=’111’<br>seq $PARALLEL_REQS | xargs -I {} -n1 -P$PARALLEL_REQS echo “$token”</p>
<p>#!/bin/bash -eux<br>PARALLEL_REQS=5<br>SLEEP_SECS=0.1<br>TEMPLOG=$(mktemp).log<br>K8S_ENDPOINT=’/api’<br>export CLUSTER_NAME=”juju-cluster”<br>APISERVER=$(kubectl config view -o jsonpath=”{.clusters[?(@.name==\”$CLUSTER_NAME\”)].cluster.server}”)<br>TOKEN=$(kubectl get secrets -o jsonpath=”{.items[?(@.metadata.annotations[‘kubernetes.io/service-account.name’]==’default’)].data.token}”|base64 –decode)</p>
<h1 id="trap-ctrl-c-and-call-ctrl-c"><a href="#trap-ctrl-c-and-call-ctrl-c" class="headerlink" title="trap ctrl-c and call ctrl_c()"></a>trap ctrl-c and call ctrl_c()</h1><p>trap ctrl_c INT<br>function ctrl<em>c() {<br>  local DEST=query-kubeapiserver.$(date ‘+%Y%m%d</em>%H%M’).log<br>  mv $TEMPLOG $DEST<br>  echo “output at $DEST”<br>  exit 0<br>}</p>
<p>set +x<br>while true; do<br>  echo “in while loop”<br>  seq $PARALLEL_REQS | xargs -I {} -n1 -P$PARALLEL_REQS curl -s -X GET $APISERVER$K8S_ENDPOINT –header “Authorization: Bearer $TOKEN” –insecure 2&gt;&amp;1 &gt;&gt; $TEMPLOG</p>
<h1 id="seq-PARALLEL-REQS-xargs-I-P-PARALLEL-REQS-sudo-etcd-etcdctl-–cacert-root-cdk-etcd-client-ca-pem-–cert-root-cdk-etcd-client-cert-pem-–key-root-cdk-etcd-client-key-pem-–endpoints-ETCD-ENDPOINTS-get-w-json-KEY-2-gt-amp-1-gt-gt-TEMPLOG"><a href="#seq-PARALLEL-REQS-xargs-I-P-PARALLEL-REQS-sudo-etcd-etcdctl-–cacert-root-cdk-etcd-client-ca-pem-–cert-root-cdk-etcd-client-cert-pem-–key-root-cdk-etcd-client-key-pem-–endpoints-ETCD-ENDPOINTS-get-w-json-KEY-2-gt-amp-1-gt-gt-TEMPLOG" class="headerlink" title="seq $PARALLEL_REQS | xargs -I {} -P$PARALLEL_REQS sudo etcd.etcdctl –cacert /root/cdk/etcd/client-ca.pem –cert /root/cdk/etcd/client-cert.pem –key /root/cdk/etcd/client-key.pem –endpoints=$ETCD_ENDPOINTS get -w json $KEY 2&gt;&amp;1 &gt;&gt; $TEMPLOG"></a>seq $PARALLEL_REQS | xargs -I {} -P$PARALLEL_REQS sudo etcd.etcdctl –cacert /root/cdk/etcd/client-ca.pem –cert /root/cdk/etcd/client-cert.pem –key /root/cdk/etcd/client-key.pem –endpoints=$ETCD_ENDPOINTS get -w json $KEY 2&gt;&amp;1 &gt;&gt; $TEMPLOG</h1><p>  echo “sleeping..”<br>  /bin/sleep $SLEEP_SECS<br>  echo “sleep done”<br>done</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/13/恢复系统记录/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/13/恢复系统记录/" itemprop="url">恢复系统记录</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-13T09:26:11+08:00">
                2020-11-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>作者：张华 发表于：2017-02-09<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明<br>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )</strong></p>
<p>今天系统又无故crash并无法启动了，折腾了一下午，记录一下。</p>
<p>突然运行“sudo apt-get update”时发生错误，一看是写保护，所以运行”sudo mount -o rw,remount /“, 但是系统报”unknown filesystem”，接着就crash了。</p>
<p>重启后出现了grub resue界面，试图通过下列命令恢复grub时仍然报”unknown filesystem”错误。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ls (hd1,msdos5)</span><br><span class="line">set root=(hd1,msdos5)</span><br><span class="line">set prefix=(hd1,5)/boot/grub</span><br><span class="line">lsmod normal</span><br><span class="line">normal</span><br><span class="line"></span><br><span class="line">sudo update-grub</span><br><span class="line">sudo grub-install /dev/sdb</span><br></pre></td></tr></table></figure></p>
<p>通过usb启动盘启动后运行“sudo fsck.ext4 -y /dev/sda9”之后上面磁盘的问题是解决了，也出现了登录界面，但是却无法登录，原本想通过下列命令重置:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -fr ~/.cache/compizconfig-1</span><br><span class="line">sudo rm -fr ~/.Xauthority</span><br><span class="line">sudo apt-get install --reinstall ubuntu-desktop unity compizconfig-settings-manager</span><br><span class="line">sudo dconf reset -f /org/compiz/</span><br><span class="line">setsid unity</span><br><span class="line">sudo rm -fr .cache/</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">http://kuring.me/ref/x_window/X_client_server_example.svg.png</span><br><span class="line">linux itself –&gt; X Server(xorg|wayland) &lt;- [X Protocol] -&gt; unity-greeter -&gt; Display Manager(GDM|lightdm, login GUI, /etc/X11/default-display-manager, or sudo dpkg-reconfigure lightdm) -&gt; Desktop Manager(GNOME, KDE) -&gt; X Application(eg: xclock)</span><br><span class="line">1, ctrl+alt+fn [n: 1-12]: used to switch virtual control console, 1-6 is used for linux (eg: echo $DISPLAY &amp;&amp; xclock -display :0 &amp;&amp; DISPLAY=:0 xclock)</span><br><span class="line">2, startx process (login -&gt; bash -&gt; startx -&gt; xinit -&gt; X -&gt; ck-xinit-session -&gt; gnome-session), xinit is used to start X Server software.</span><br><span class="line">3, &apos;init 5&apos; is used to restart X window</span><br></pre></td></tr></table></figure>
<p>但是发现/var/lib/dpkg目录不存在了，另外也有其他很多程序出现少文件的问题，不是我删除的，应该是fsck命令没有全部正确恢复inode吧。这种情况只能重装操作系统了，将所有工作需要的应用都安装好后也做了一个备份（dd if=/dev/sdb conv=sync,noerror bs=64K | gzip -c  &gt; /images/working_os_bak.img.gz）， 今后再出问题时希望通过命令（gunzip -c /images/working_os_bak.img.gz | dd of=/dev/sdb conv=sync,noerror bs=64K）能快速恢复操作系统和所需要的应用吧。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># dd if=/dev/sdb conv=sync,noerror bs=64K | gzip -c  &gt; /images/working_os_bak.img.gz</span><br><span class="line">1831575+1 records in</span><br><span class="line">1831576+0 records out</span><br><span class="line">120034164736 bytes (120 GB, 112 GiB) copied, 914.123 s, 131 MB/s</span><br><span class="line"></span><br><span class="line">$ ll /images/working_os_bak.img.gz -h</span><br><span class="line">-rw-r--r-- 1 root root 4.7G Feb  9 17:36 /images/working_os_bak.img.gz</span><br></pre></td></tr></table></figure>
<p>硬盘损坏看起来像：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;- - -&quot; &gt; /sys/class/scsi_host/host0/scan</span><br><span class="line"></span><br><span class="line">#Unused HDD</span><br><span class="line">Mar  1 10:24:28 node1 kernel: [ 2697.930058] ata3: hard resetting link</span><br><span class="line">Mar  1 10:24:28 node1 kernel: [ 2698.245478] ata3: SATA link down (SStatus 0 SControl 300)</span><br><span class="line">Mar  1 10:24:28 node1 kernel: [ 2698.245490] ata3: EH complete</span><br><span class="line"></span><br><span class="line">#Good HDD</span><br><span class="line">Mar  1 10:23:43 node1 kernel: [ 2652.763243] ata1: hard resetting link</span><br><span class="line">Mar  1 10:23:43 node1 kernel: [ 2653.077298] ata1: SATA link up 6.0 Gbps (SStatus 133 SControl 300)</span><br><span class="line">Mar  1 10:23:43 node1 kernel: [ 2653.097249] ata1.00: configured for UDMA/133</span><br><span class="line">Mar  1 10:23:43 node1 kernel: [ 2653.097251] ata1: EH complete</span><br><span class="line"></span><br><span class="line">#Bad HDD</span><br><span class="line">Feb 28 16:00:30 node1 kernel: [  231.696088] ata2.00: exception Emask 0x0 SAct 0x80000 SErr 0x0 action 0x6 frozen</span><br><span class="line">Feb 28 16:00:30 node1 kernel: [  231.696098] ata2.00: failed command: READ FPDMA QUEUED</span><br><span class="line">Feb 28 16:00:30 node1 kernel: [  231.696108] ata2.00: cmd 60/10:98:b0:bd:3c/00:00:a6:00:00/40 tag 19 ncq 8192 in</span><br><span class="line">Feb 28 16:00:30 node1 kernel: [  231.696108]          res 40/00:00:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)</span><br><span class="line">Feb 28 16:00:30 node1 kernel: [  231.696113] ata2.00: status: &#123; DRDY &#125;</span><br><span class="line">Feb 28 16:00:30 node1 kernel: [  231.696119] ata2: hard resetting link</span><br><span class="line">Feb 28 16:00:36 node1 kernel: [  237.059963] ata2: link is slow to respond, please be patient (ready=0)</span><br><span class="line">Feb 28 16:00:40 node1 kernel: [  241.707975] ata2: COMRESET failed (errno=-16)</span><br><span class="line">Feb 28 16:00:40 node1 kernel: [  241.707980] ata2: hard resetting link</span><br><span class="line">Feb 28 16:00:46 node1 kernel: [  247.067970] ata2: link is slow to respond, please be patient (ready=0)</span><br></pre></td></tr></table></figure></p>
<p>换硬盘不需要重装系统，将fstab文件修改一下即可:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LABEL=&quot;images&quot; /images         ext4    defaults        0       2</span><br><span class="line">LABEL=&quot;bak&quot; /bak         ext4    defaults        0       2</span><br><span class="line">192.168.99.122:/Public /nas nfs noauto,x-systemd.automount,x-systemd.device-timeout=10,timeo=14,x-systemd.idle-timeout=1min 0 0</span><br></pre></td></tr></table></figure></p>
<h2 id="201923更新-解决login-loop问题"><a href="#201923更新-解决login-loop问题" class="headerlink" title="201923更新 - 解决login loop问题"></a>201923更新 - 解决login loop问题</h2><p>反复出现登录界面, 根据下列流程, 似乎问题只可能出现在gnome这块出问题了才会回到lightdm的登录界面处.<br>linux itself –&gt; X Server(xorg|wayland) &lt;- [X Protocol] -&gt; unity-greeter -&gt; Display Manager(GDM|lightdm, login GUI, /etc/X11/default-display-manager, or sudo dpkg-reconfigure lightdm) -&gt; Desktop Manager(GNOME, KDE) -&gt; X Application(eg: xclock)<br>搜索kern.log的Call strace确认问题与nvidia驱动相关:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"> 733 Nov 16 22:47:03 localhost kernel: [    1.896228] Could not determine valid watermarks for inherited state</span><br><span class="line"> 734 Nov 16 22:47:03 localhost kernel: [    1.896290] WARNING: CPU: 2 PID: 183 at /build/linux-O4X9pa/linux-4.15.0/drivers/gpu/d</span><br><span class="line"> 735 Nov 16 22:47:03 localhost kernel: [    1.896293] Modules linked in: nvidia_drm(POE) nvidia_modeset(POE) nvidia(POE) rtsx_pc</span><br><span class="line"> 736 Nov 16 22:47:03 localhost kernel: [    1.896309] CPU: 2 PID: 183 Comm: systemd-udevd Tainted: P           OE    4.15.0-68-g</span><br><span class="line"> 737 Nov 16 22:47:03 localhost kernel: [    1.896311] Hardware name: LENOVO 20AN002LCD/20AN002LCD, BIOS GLET64WW (2.18 ) 12/18/2</span><br><span class="line"> 738 Nov 16 22:47:03 localhost kernel: [    1.896342] RIP: 0010:intel_modeset_init+0xfcf/0x1010 [i915]</span><br><span class="line"></span><br><span class="line">748 Nov 16 22:47:03 localhost kernel: [    1.896362] Call Trace:</span><br><span class="line"> 749 Nov 16 22:47:03 localhost kernel: [    1.896389]  i915_driver_load+0xa73/0xe60 [i915]</span><br><span class="line"> 750 Nov 16 22:47:03 localhost kernel: [    1.896414]  i915_pci_probe+0x42/0x70 [i915]</span><br><span class="line"> 751 Nov 16 22:47:03 localhost kernel: [    1.896418]  local_pci_probe+0x47/0xa0</span><br><span class="line"> 752 Nov 16 22:47:03 localhost kernel: [    1.896421]  pci_device_probe+0x10e/0x1c0</span><br><span class="line"> 753 Nov 16 22:47:03 localhost kernel: [    1.896424]  driver_probe_device+0x30c/0x490</span><br><span class="line"> 754 Nov 16 22:47:03 localhost kernel: [    1.896426]  __driver_attach+0xcc/0xf0</span><br><span class="line"> 755 Nov 16 22:47:03 localhost kernel: [    1.896429]  ? driver_probe_device+0x490/0x490</span><br><span class="line"> 756 Nov 16 22:47:03 localhost kernel: [    1.896431]  bus_for_each_dev+0x70/0xc0</span><br><span class="line"> 757 Nov 16 22:47:03 localhost kernel: [    1.896433]  driver_attach+0x1e/0x20</span><br><span class="line"> 758 Nov 16 22:47:03 localhost kernel: [    1.896436]  bus_add_driver+0x1c7/0x270</span><br><span class="line"> 759 Nov 16 22:47:03 localhost kernel: [    1.896438]  ? 0xffffffffc041b000</span><br><span class="line"> 760 Nov 16 22:47:03 localhost kernel: [    1.896440]  driver_register+0x60/0xe0</span><br><span class="line"> 761 Nov 16 22:47:03 localhost kernel: [    1.896442]  ? 0xffffffffc041b000</span><br><span class="line"> 762 Nov 16 22:47:03 localhost kernel: [    1.896444]  __pci_register_driver+0x5a/0x60</span><br><span class="line"> 763 Nov 16 22:47:03 localhost kernel: [    1.896465]  i915_init+0x5c/0x5f [i915]</span><br></pre></td></tr></table></figure>
<p>升级nvidia驱动从nvidia-driver-390到435看是否能解决问题<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">deb http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic main</span><br><span class="line">sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys FCAE110B1118213C</span><br><span class="line">sudo apt purge nvidia-driver-390 &amp;&amp; sudo apt autoremove &amp;&amp; sudo apt install nvidia-driver-435</span><br></pre></td></tr></table></figure></p>
<p>也可以试试:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#https://launchpad.net/~canonical-kernel-team/+archive/ubuntu/ppa</span><br><span class="line">sudo apt upgrade-dist</span><br><span class="line">sudo add-apt-repository ppa:canonical-kernel-team/ppa</span><br><span class="line">#can install the latest hwe kernel by using above ppa</span><br><span class="line">sudo apt install --install-recommends linux-generic-hwe-18.04 xserver-xorg-hwe-18.04</span><br></pre></td></tr></table></figure></p>
<p>20200203更 , 还是不行, 用下列方法重装了desktop再观察:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">sudo rm /var/lib/apt/lists/lock</span><br><span class="line">sudo rm /var/lib/dpkg/lock</span><br><span class="line">sudo rm /var/lib/dpkg/lock-frontend</span><br><span class="line">sudo dpkg --configure -a</span><br><span class="line">sudo apt clean</span><br><span class="line">sudo apt update --fix-missing</span><br><span class="line">sudo apt install -f</span><br><span class="line">sudo dpkg --configure -a</span><br><span class="line">sudo apt upgrade</span><br><span class="line">sudo apt dist-upgrade</span><br><span class="line"></span><br><span class="line">sudo apt purge *unity*</span><br><span class="line">sudo apt purge *gnome*</span><br><span class="line">sudo apt autoremove</span><br><span class="line">sudo apt-get install --reinstall ubuntu-desktop</span><br><span class="line">sudo update-grub</span><br><span class="line">sudo apt install gdm3</span><br><span class="line">sudo dpkg-reconfigure gdm3</span><br><span class="line"></span><br><span class="line">#Install Gnome and set it as default</span><br><span class="line">sudo apt install gnome-session</span><br><span class="line">sudo update-alternatives --config gdm3.css</span><br><span class="line">sudo reboot</span><br><span class="line">#Switch to a Windows-like Taskbar - https://vitux.com/how-to-get-the-windows-look-feel-on-ubuntu/</span><br><span class="line">sudo apt install gnome-shell-extensions gnome-shell-extension-dash-to-panel gnome-tweaks adwaita-icon-theme-full</span><br><span class="line"></span><br><span class="line">#Install windows-style desktop Cinnamon, but it&apos;s bluetooth doesn&apos;t work</span><br><span class="line">sudo add-apt-repository ppa:embrosyn/cinnamon</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install cinnamon</span><br><span class="line">#sudo apt remove --autoremove cinnamon cinnamon-desktop-data</span><br><span class="line">#sudo add-apt-repository --remove ppa:embrosyn/cinnamon</span><br></pre></td></tr></table></figure>
<p>20201113更新，切换到使用wayland再试试:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hua@t440p:~$ grep -r &apos;Wayland&apos; /etc/gdm3/custom.conf</span><br><span class="line">WaylandEnable=true</span><br><span class="line">sudo systemctl restart gdm3</span><br><span class="line">hua@t440p:~$ echo $XDG_SESSION_TYPE</span><br><span class="line">wayland</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/12/解决ssh-server偶尔连不上或者发现网络连接初始很慢的状况/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/12/解决ssh-server偶尔连不上或者发现网络连接初始很慢的状况/" itemprop="url">解决ssh server偶尔连不上或者发现网络连接初始很慢的状况</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-12T11:35:06+08:00">
                2020-11-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明 (<a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a>)</strong></p>
<p>今天花了近一天时间解决了一个恶心的问题，回过头看来当然很简单了，但要记录一下。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>我将办公用的台式机搬到了客厅，今后打算每天在笔记本上通过ssh连接到台式机办公。但是发生ssh连接老断，刚开始以为是ssh keepalive的问题，照着这个博客（<a href="http://blog.csdn.net/quqi99/article/details/51434248）配置后仍然无效。ssh连接断了之后，到台式机上查看发现网络是好的，重启台式机上的网络和ssh服务仍然无效，但重启笔记本上的网络服务就好了。大概五六分钟发生一次吧。" target="_blank" rel="external">http://blog.csdn.net/quqi99/article/details/51434248）配置后仍然无效。ssh连接断了之后，到台式机上查看发现网络是好的，重启台式机上的网络和ssh服务仍然无效，但重启笔记本上的网络服务就好了。大概五六分钟发生一次吧。</a></p>
<h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>家里的tplink摄像头指定了固定IP 192.168.99.123，不知道为什么openwrt路由器将这个已占用的IP分配给了台式机。因为摄像头长期没用了，这个IP也ping不通，并且dnsmasq的/tmp/dhcp.leases配置文件中显示的这个IP都是和台式机的hostname在一起的，所以刚开始就没有想到这个重要的问题。</p>
<p>1, 当在台式机上ping网关时，居然第一次要停顿11秒，但第二次之后就正常了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hua@node1:~$ time ping -c 1 192.168.99.1</span><br><span class="line">PING 192.168.99.1 (192.168.99.1): 56 data bytes</span><br><span class="line">--- 192.168.99.1 ping statistics ---</span><br><span class="line">1 packets transmitted, 0 packets received, 100% packet loss</span><br><span class="line">real	0m11.012s</span><br><span class="line">user	0m0.000s</span><br><span class="line">sys	0m0.000s</span><br></pre></td></tr></table></figure></p>
<p>2, 台式机给网关发消息，台式机通过arp广播要网关的mac地址，网关向台式机返回arp响应，但这个响应被错误的发到摄像头的mac f0:7d:68:0c:90:40之上，所以当问题发生时，路由器抓包到了大量的下列消息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@OpenWrt:~# tcpdump -i br-lan  -e -n &apos;icmp&apos; and src host 192.168.99.123</span><br><span class="line">14:10:30.449983 f8:32:e4:be:87:cd &gt; 04:a1:51:8a:2c:a7, ethertype IPv4 (0x0800), length 98: 192.168.99.123 &gt; 192.168.99.1: ICMP echo request, id 9604, seq 0, length 64</span><br><span class="line">14:10:33.403550 f0:7d:68:0c:90:40 &gt; 04:a1:51:8a:2c:a7, ethertype IPv4 (0x0800), length 118: 192.168.99.123 &gt; 85.199.214.101: ICMP 192.168.99.123 udp port 123 unreachable, length 84</span><br></pre></td></tr></table></figure></p>
<p>3, 台式机因为等不到arp请求，从而停顿。</p>
<p>使用“ip -s -s neigh flush all”命令清台式机与路由器arp cache均无效，在台式机上运行arp请求会很慢。</p>
<p>后来给台式机更换其他IP就好了.</p>
<p>我看到的这个问题的影响如下：<br>1, 当笔记本采用ssh连接到台式机上，也就会时不时发生停顿现象<br>2, 使用apt update时会第一次连接时像断了一样，但后面又好了</p>
<h2 id="附录：Ubuntu下的远程桌面与X转发"><a href="#附录：Ubuntu下的远程桌面与X转发" class="headerlink" title="附录：Ubuntu下的远程桌面与X转发"></a>附录：Ubuntu下的远程桌面与X转发</h2><p>上面解决了ssh连接办公的问题，下面将远程桌面一并解决。</p>
<p>Ubuntu下有一个默认的vino-server, 可在”Desktop Sharing”中开启，另外，需要运行dconf-editor将org-&gt;gnome-&gt;desktop-&gt;remote-access中的requre-encryption去掉，最后使用Remmina Remote Desktop Client选择vnc协议输入“IP :0”登录。不过，这种自带的Vino-Server方式有一个最显著的缺点：那就是当你重启机器之后，必须首先到远程服务器那边登录机器，进入系统（相当于创建了一个Session）之后，才能在本地使用远程桌面连接这个远程服务器。所以我们改使用vnc4server。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo apt -y install vnc4server xvnc4viewer</span><br><span class="line">vncpasswd</span><br><span class="line"># First time to run it to generate ~/.vnc</span><br><span class="line">vncserver :2</span><br><span class="line">ll ~/.vnc/</span><br><span class="line">vncserver -kill :2</span><br><span class="line"># Second time to run it after modifying ~/.vnc</span><br><span class="line">sudo apt install mate-session-manager</span><br><span class="line">echo &apos;exec /usr/bin/mate-session &amp;&apos; &gt;&gt; ~/.vnc/xstartup</span><br><span class="line">vncserver :2 -geometry 1280x680 -alwaysshared</span><br><span class="line">Client Side: vncviewer node1:2</span><br></pre></td></tr></table></figure>
<p>运行”ssh -vvv -X -o ForwardX11=yes node1“时报错”X11 forwarding request failed on channel 0“， 这是因为出于安全原因，OpenSSH默认将X11转发请求绑定到本地回环地址上，并且在DISPLAY环境变量中将主机名设置为“localhost”。在这样的设定下，一些 X11客户端不能正确处理X11转发，这会导致报告中的错误。要解决这个问题，在/etc/ssh/sshd配置文件中加入下面这几行，它可以将X11转发请求绑定到外网卡地址上。<br>X11Forwarding yes<br>X11UseLocalhost no<br>如果远程主机的SSH服务禁止了IPv6，那么X11转发失败的错误也有可能发生。故还需要添加： AddressFamily inet</p>
<h2 id="20191220更新-另一个例子"><a href="#20191220更新-另一个例子" class="headerlink" title="20191220更新 - 另一个例子"></a>20191220更新 - 另一个例子</h2><p><img src="https://img-blog.csdnimg.cn/20191220175958218.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly96aGh1YWJqLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">客户有两个external的dmz网络172.21.8.0/24与193.169.205.0/24</span><br><span class="line">两个neutron vrouter, dmzrouterinternal接172.21.8.0/24(qg-xxx=172.21.8.87), dmzroutereexternal接193.169.205.0/24</span><br><span class="line">dmzrouterinternal接一个subnet(192.168.4.1), 这样创建一个测试VM=192.168.4.199 FIP=172.21.8.85</span><br><span class="line">一个外部物理路由器有172.21.8.1与193.169.205.1, 外部路由器上接一个外部dns服务器(193.169.205.141)</span><br><span class="line">问题是VM无法ping 193.169.205.141, 抓包看到ARP reply到达了dmzrouterinternal却被drop掉了</span><br><span class="line">15:26:20.634541 ARP, Request who-has 193.169.205.141 tell 172.21.8.87, length 28</span><br><span class="line">15:26:20.634810 ARP, Reply 193.169.205.141 is-at 00:15:5d:2d:d9:2c (oui Unknown), length 46</span><br><span class="line"></span><br><span class="line">发现它有这些路由:</span><br><span class="line"># ip netns exec qrouter-ffd74122-45b4-4155-b585-fe62b2d3edd4 ip route</span><br><span class="line">default via 172.21.8.1 dev qg-d84aed2a-34</span><br><span class="line">172.21.8.0/24 dev qg-d84aed2a-34 proto kernel scope link src 172.21.8.87</span><br><span class="line">192.168.4.0/24 dev qr-3c248a21-06 proto kernel scope link src 192.168.4.1</span><br><span class="line">193.169.205.0/24 dev qg-d84aed2a-34 scope link</span><br><span class="line"></span><br><span class="line">显然问题是这条路由&apos;193.169.205.0/24 dev qg-d84aed2a-34 scope link&apos;造成dmzrouterinternal与externaldns直连:</span><br><span class="line">ping之前, 看到193.169.205.141 at 00:00:0c:9f:f0:2c, 00:00:0c:9f:f0:2c是172.21.8.1的mac</span><br><span class="line"># arp -a</span><br><span class="line">? (192.168.4.42) at fa:16:3e:79:07:f0 [ether] on qr-3c248a21-06</span><br><span class="line">? (193.169.205.141) at 00:00:0c:9f:f0:2c [ether] on qg-d84aed2a-34</span><br><span class="line">当ping时, tcpdump收到ARP reply里的mac是00:15:5d:2d:d9:2c, 而不是00:00:0c:9f:f0:2c, 所以drop掉.</span><br><span class="line"></span><br><span class="line">如果临时运行下列命令, 也能ping通, 但一会儿这个arp就会过期, 问题就会再次重现, 所以解决的办法是删除这条路由(193.169.205.0/24 dev qg-d84aed2a-34 scope link)</span><br><span class="line">arp -d 193.169.205.141</span><br><span class="line">arp -i qg-d84aed2a-34 -s 193.169.205.141 02:00:5e:10:de:14</span><br><span class="line"></span><br><span class="line">下面做一个实验, 用一个neutron vrouter externalrouter来模拟外部物理路由器.</span><br><span class="line">./generate-bundle.sh -s bionic -r stein --num-compute 2</span><br><span class="line"></span><br><span class="line">openstack router create externalrouter</span><br><span class="line">openstack network create dmzexternalnet --external</span><br><span class="line">openstack network create dmzinternalnet --external</span><br><span class="line">openstack subnet create --subnet-range 172.21.8.0/24 --network dmzinternalnet --allocation-pool start=172.21.8.86,end=172.21.8.100 --gateway 172.21.8.1 dmzsubnetinternal</span><br><span class="line">openstack subnet create --subnet-range 193.169.205.0/24 --network dmzexternalnet --allocation-pool start=193.169.205.87,end=193.169.205.100 --gateway 193.169.205.1 dmzsubnetexternal</span><br><span class="line">openstack router add subnet externalrouter dmzsubnetinternal</span><br><span class="line">openstack router add subnet externalrouter dmzsubnetexternal</span><br><span class="line"></span><br><span class="line">#注:也可以不创建两个network, 只创建一个network, 然后将两个subnet都加到这个network, 在和router关联时特殊处理, 如:</span><br><span class="line">openstack router create --disable --centralized --ha dmzrouterinternal</span><br><span class="line">openstack router add subnet dmzrouterinternal dmztenantinternalsubnet</span><br><span class="line">openstack router set --external-gateway dmznet --fixed-ip subnet=dmzsubnetinternal,ip-address=172.21.8.87 dmzrouterinternal</span><br><span class="line">openstack router set --enable dmzrouterinternal</span><br><span class="line"></span><br><span class="line">openstack router create dmzrouterinternal</span><br><span class="line">openstack router set --external-gateway dmzinternalnet dmzrouterinternal</span><br><span class="line">openstack network create tenantnet</span><br><span class="line">openstack subnet create --subnet-range 192.168.4.0/24 --network tenantnet --allocation-pool start=192.168.4.87,end=192.168.4.100 --gateway 192.168.4.1 tenantsubnet</span><br><span class="line">openstack router add subnet dmzrouterinternal tenantsubnet</span><br><span class="line"></span><br><span class="line">openstack router create dmzrouterexternal</span><br><span class="line">openstack router set --external-gateway dmzexternalnet dmzrouterexternal</span><br><span class="line"></span><br><span class="line">openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey</span><br><span class="line">openstack server create --wait --image cirros2 --flavor m1.small --key-name mykey --network=$(openstack network show dmzexternalnet -c id -f value) externaldns</span><br><span class="line">openstack server create --wait --image cirros2 --flavor m1.small --key-name mykey --network=$(openstack network show tenantnet -c id -f value) i1</span><br><span class="line">dmzinternalnet=$(openstack network show dmzinternalnet -f value -c id)</span><br><span class="line">fip=$(openstack floating ip create $dmzinternalnet -f value -c floating_ip_address)</span><br><span class="line">openstack floating ip set $fip --fixed-ip-address 192.168.4.100 --port $(openstack port list --fixed-ip ip-address=192.168.4.100 -c id -f value)</span><br><span class="line"></span><br><span class="line">$ openstack server list</span><br><span class="line">+--------------------------------------+-------------+--------+--------------------------------------+---------+----------+</span><br><span class="line">| ID                                   | Name        | Status | Networks                             | Image   | Flavor   |</span><br><span class="line">+--------------------------------------+-------------+--------+--------------------------------------+---------+----------+</span><br><span class="line">| 3a1d1c70-6165-4060-ad78-9c67717e61e8 | externaldns | ACTIVE | dmzexternalnet=193.169.205.99        | cirros2 | m1.small |</span><br><span class="line">| 89b497c0-f236-4df4-8293-18f44547e239 | i1          | ACTIVE | tenantnet=192.168.4.100, 172.21.8.95 | cirros2 | m1.small |</span><br><span class="line">+--------------------------------------+-------------+--------+--------------------------------------+---------+----------+</span><br><span class="line"></span><br><span class="line">ubuntu@zhhuabj-bastion:~/stsstack-bundles/openstack$ openstack router list</span><br><span class="line">+--------------------------------------+-------------------+--------+-------+----------------------------------+-------------+-------+</span><br><span class="line">| ID                                   | Name              | Status | State | Project                          | Distributed | HA    |</span><br><span class="line">+--------------------------------------+-------------------+--------+-------+----------------------------------+-------------+-------+</span><br><span class="line">| 09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 | dmzrouterinternal | ACTIVE | UP    | e2402e56ee0a4c31904e46b5c01c80f8 | False       | False |</span><br><span class="line">| 8320db06-5a9d-46fb-8af6-5c7b974ff7a9 | externalrouter    | ACTIVE | UP    | e2402e56ee0a4c31904e46b5c01c80f8 | False       | False |</span><br><span class="line">| ef19ea38-13e0-416a-9ce1-ac25c8aba9c0 | dmzrouterexternal | ACTIVE | UP    | e2402e56ee0a4c31904e46b5c01c80f8 | False       | False |</span><br><span class="line">+--------------------------------------+-------------------+--------+-------+----------------------------------+-------------+-------+</span><br><span class="line"></span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 ip route add 193.169.205.0/24 dev qg-e6cd9397-b8 scope link</span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 ip route</span><br><span class="line">default via 172.21.8.1 dev qg-e6cd9397-b8</span><br><span class="line">172.21.8.0/24 dev qg-e6cd9397-b8 proto kernel scope link src 172.21.8.92</span><br><span class="line">192.168.4.0/24 dev qr-e4d7b70a-14 proto kernel scope link src 192.168.4.1</span><br><span class="line">193.169.205.0/24 dev qg-e6cd9397-b8 scope link</span><br><span class="line">$ ping 193.169.205.99 -c 1</span><br><span class="line">...</span><br><span class="line">1 packets transmitted, 0 packets received, 100% packet loss</span><br><span class="line"></span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 arp -d 193.169.205.99</span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 arp -i qg-e6cd9397-b8 -s 193.169.205.99 fa:16:3e:7b:af:d6</span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 arp -a</span><br><span class="line">? (172.21.8.1) at fa:16:3e:0f:87:da [ether] on qg-e6cd9397-b8</span><br><span class="line">? (193.169.205.99) at fa:16:3e:7b:af:d6 [ether] PERM on qg-e6cd9397-b8</span><br><span class="line">? (192.168.4.100) at fa:16:3e:5d:2f:5b [ether] on qr-e4d7b70a-14</span><br><span class="line">? (172.21.8.87) at fa:16:3e:a1:c2:e4 [ether] on qg-e6cd9397-b8</span><br><span class="line">$ ping 193.169.205.99 -c 1</span><br><span class="line">...</span><br><span class="line">1 packets transmitted, 0 packets received, 100% packet loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 ip route del 193.169.205.0/24 dev qg-e6cd9397-b8 scope link</span><br><span class="line">$ ping 193.169.205.99 -c 1</span><br><span class="line">PING 193.169.205.99 (193.169.205.99): 56 data bytes</span><br><span class="line">64 bytes from 193.169.205.99: seq=0 ttl=62 time=11.998 ms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 arp -a</span><br><span class="line">? (172.21.8.1) at fa:16:3e:0f:87:da [ether] on qg-e6cd9397-b8</span><br><span class="line">? (192.168.4.100) at fa:16:3e:5d:2f:5b [ether] on qr-e4d7b70a-14</span><br><span class="line">? (172.21.8.87) at fa:16:3e:a1:c2:e4 [ether] on qg-e6cd9397-b8</span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-09ac17b3-c1ec-4d47-b70a-9f6e3ae98004 ip route</span><br><span class="line">default via 172.21.8.1 dev qg-e6cd9397-b8</span><br><span class="line">172.21.8.0/24 dev qg-e6cd9397-b8 proto kernel scope link src 172.21.8.92</span><br><span class="line">192.168.4.0/24 dev qr-e4d7b70a-14 proto kernel scope link src 192.168.4.1</span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-8320db06-5a9d-46fb-8af6-5c7b974ff7a9 arp -a</span><br><span class="line">? (172.21.8.92) at fa:16:3e:d5:36:a6 [ether] on qr-0912ab72-90</span><br><span class="line">? (193.169.205.87) at fa:16:3e:09:83:10 [ether] on qr-6fa183e9-a6</span><br><span class="line">? (172.21.8.86) at fa:16:3e:89:ce:85 [ether] on qr-0912ab72-90</span><br><span class="line">? (193.169.205.99) at fa:16:3e:7b:af:d6 [ether] on qr-6fa183e9-a6</span><br><span class="line">? (172.21.8.95) at fa:16:3e:d5:36:a6 [ether] on qr-0912ab72-90</span><br><span class="line">? (172.21.8.87) at fa:16:3e:a1:c2:e4 [ether] on qr-0912ab72-90</span><br><span class="line">root@juju-a7ddde-stein-5:~# ip netns exec qrouter-8320db06-5a9d-46fb-8af6-5c7b974ff7a9 ip route</span><br><span class="line">172.21.8.0/24 dev qr-0912ab72-90 proto kernel scope link src 172.21.8.1</span><br><span class="line">193.169.205.0/24 dev qr-6fa183e9-a6 proto kernel scope link src 193.169.205.1</span><br></pre></td></tr></table></figure></p>
<h2 id="20201112更新"><a href="#20201112更新" class="headerlink" title="20201112更新"></a>20201112更新</h2><p>ssh这次失去连接，是因为台式机上的虚机使用mavtap时不小心将bridge改成了passthough导致虚机启动时失去网络.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/12/SSH连接总是定期断掉的解决办法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张华">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="技术并艺术着">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/12/SSH连接总是定期断掉的解决办法/" itemprop="url">SSH连接总是定期断掉的解决办法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-12T10:04:27+08:00">
                2020-11-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：张华  发表于：2016-05-17<br>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</p>
<p>( <a href="http://blog.csdn.net/quqi99" target="_blank" rel="external">http://blog.csdn.net/quqi99</a> )</p>
<p>SSH连接总是隔一段时间没有输入时就断开，解决办法如下：</p>
<p>服务端配置<br>sudo vi /etc/ssh/sshd_config<br>ClientAliveInterval 60     #服务端主动向客户端请求响应的间隔<br>ClientAliveCountMax 10    #服务器发出请求后客户端没有响应的次数达到一定值就自动断开<br>sudo restart ssh</p>
<p>客户端配置<br>sudo vi /etc/ssh/ssh_config  #或~/.ssh/config</p>
<pre><code>TCPKeepAlive yes
ServerAliveInterval 15
ServerAliveCountMax 6
StrictHostKeyChecking no
ForwardAgent yes
Compression yes

 IPQoS throughput
</code></pre><p>或</p>
<p>ssh -i <key-file> IPQoS=throughput -o StrictHostKeyChecking=no -o TCPKeepAlive=yes -o ServerAliveInterval=30 ubuntu@<ip></ip></key-file></p>
<p>上面方式任选一种，我选客户端配置方式。</p>
<p>20200316更新, 如果ssh总是断时也得考虑使用白名单模式时是否将某些IP排除了.</p>
<p>20200902更新, 如果使用Compression yes会被墙看上, 会时不时ssh断掉, 去掉Compression即可.</p>
<p>20201016更新, 到国外的ssh连接总断, 除了ISP QoS外, 还试试在sshd.conf中添加:</p>
<p>UseDNS no</p>
<p>GSSAPIAuthentication no</p>
<p>20201028更新，</p>
<p>这次应该是找到了ssh总断的根源，因为bastion上安装了devstack所以在并行运行’opestack image set”时ssh会中断．所以习惯很重要，不能乱安装东西．</p>
<p>#openstack image set –property $p=${props[$p]} $img_name</p>
<p>openstack image set –property $p=${props[$p]} $img_name &amp;</p>
<p>另外，使用压缩传输数据：</p>
<p>#strings /usr/sbin/sshd |egrep “chacha|aes”<br>scp -p -C -o ‘IPQoS throughput’  -c chacha20-poly1305@openssh.com win10.img hua@node1:/images/</p>
<p>20201112更新</p>
<p>ssh总断的原因是因为机器上为反复重启virtualbox导致chrome会通过fast.com测速变得只有十几K, virutalbox改成了kvm现在观察了几天似乎这个问题消失了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">张华</p>
              <p class="site-description motion-element" itemprop="description">blog.csdn.net/quqi99</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">84</span>
                  <span class="site-state-item-name">Artikel</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张华</span>

  
</div>


  <div class="powered-by">Erstellt mit  <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
